{"cells":[{"cell_type":"code","source":["## Filtering operations\n\nfrom pyspark.sql import Row\nimport pandas as pd\nspark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)\n\nimport datetime\nusers = [\n    {\n        \"id\": 1,\n        \"first_name\": \"Corrie\",\n        \"last_name\": \"Van den Oord\",\n        \"email\": \"cvandenoord0@etsy.com\",\n        \"gender\": \"male\",\n        \"current_city\": \"Dallas\",\n        \"phone_numbers\": Row(mobile=\"+1 234 567 8901\", home=\"+1 234 567 8911\"),\n        \"courses\": [1, 2],\n        \"is_customer\": True,\n        \"amount_paid\": 1000.55,\n        \"customer_from\": datetime.date(2021, 1, 15),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n    },\n    {\n        \"id\": 2,\n        \"first_name\": \"Nikolaus\",\n        \"last_name\": \"Brewitt\",\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"gender\": \"male\",\n        \"current_city\": \"Houston\",\n        \"phone_numbers\":  Row(mobile=\"+1 234 567 8923\", home=\"1 234 567 8934\"),\n        \"courses\": [3],\n        \"is_customer\": True,\n        \"amount_paid\": 900.0,\n        \"customer_from\": datetime.date(2021, 2, 14),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n    },\n    {\n        \"id\": 3,\n        \"first_name\": \"Orelie\",\n        \"last_name\": \"Penney\",\n        \"email\": \"openney2@vistaprint.com\",\n        \"gender\": \"female\",\n        \"current_city\": \"\",\n        \"phone_numbers\": Row(mobile=\"+1 714 512 9752\", home=\"+1 714 512 6601\"),\n        \"courses\": [2, 4],\n        \"is_customer\": True,\n        \"amount_paid\": 850.55,\n        \"customer_from\": datetime.date(2021, 1, 21),\n        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n    },\n    {\n        \"id\": 4,\n        \"first_name\": \"Ashby\",\n        \"last_name\": \"Maddocks\",\n        \"email\": \"amaddocks3@home.pl\",\n        \"gender\": \"male\",\n        \"current_city\": \"San Fransisco\",\n        \"phone_numbers\": Row(mobile=None, home=None),\n        \"courses\": [],\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n    },\n    {\n        \"id\": 5,\n        \"first_name\": \"Kurt\",\n        \"last_name\": \"Rome\",\n        \"email\": \"krome4@shutterfly.com\",\n        \"gender\": \"female\",\n        \"current_city\": None,\n        \"phone_numbers\": Row(mobile=\"+1 817 934 7142\", home=None),\n        \"courses\": [],\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n    }\n]\n\n\nusers_df = spark.createDataFrame(pd.DataFrame(users))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48462ad2-1bf1-485e-9a3e-52b1c725600d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["users_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00d72894-a596-4dd0-b36c-b307c3cbe305","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender| current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|       Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|  male|      Houston|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|female|             |{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|  male|San Fransisco|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|female|         null|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender| current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|       Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|  male|      Houston|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|female|             |{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|  male|San Fransisco|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|female|         null|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n# * `where` and `filter` are synonyms\n# * We can pass conditions either by using SQL Style or Non SQL Style.\n# * For Non SQL Style we can pass columns using `col` function on column name as string or using the notation of`df['column_name']`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa7ffed9-8415-4b92-8f70-2fce15d4189c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nusers_df.filter(col('id') == 1).show()\nusers_df.where(col('id') == 1).show()\nusers_df.filter(users_df['id'] == 1).show()\n\n# 'id == 1' also works\nusers_df.filter('id = 1').show()\n## here filter and where conditions are doing the same\nusers_df.createOrReplaceTempView('users')\nspark.sql(\"\"\"\n    SELECT *\n    FROM users\n    WHERE id = 1\n\"\"\"). \\\n    show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4fe0f030-c68b-44a6-b1bb-6cc3b53ee1ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n#     Equal -> = or ==\n#     Not Equal -> !=\n#     Greater Than -> >\n#     Less Than -> <\n#     Greater Than or Equal To -> >=\n#     Less Than or Equal To -> <=\n#     IN Operator -> isin function or IN or contains function\n#     Between Operator -> between function or BETWEEN with AND\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5754908-89cf-427e-b7c4-37c4e8dd2867","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nusers_df.filter(col('current_city') == 'Dallas').show()\nusers_df.filter('amount_paid == \"900.0\"').show()\n\n\nfrom pyspark.sql.functions import isnan\n## note isnan == is not a nan   returns true or False values \nusers_df.select('amount_paid', isnan('amount_paid')).show()\n\n###\nusers_df.filter('isnan(amount_paid) = True').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4aa17c47-b215-4d1f-b79f-f16f47a1061f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+---------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+---------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  2|  Nikolaus|  Brewitt|nbrewitt1@dailyma...|  male|     Houston|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n+---+----------+---------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+-----------+------------------+\n|amount_paid|isnan(amount_paid)|\n+-----------+------------------+\n|    1000.55|             false|\n|      900.0|             false|\n|     850.55|             false|\n|        NaN|              true|\n|        NaN|              true|\n+-----------+------------------+\n\n+---+----------+---------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|last_name|               email|gender| current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+---------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  4|     Ashby| Maddocks|  amaddocks3@home.pl|  male|San Fransisco|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|     Rome|krome4@shutterfly...|female|         null|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+---------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|  male|      Dallas|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n+---+----------+------------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+---------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|last_name|               email|gender|current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+---------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  2|  Nikolaus|  Brewitt|nbrewitt1@dailyma...|  male|     Houston|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n+---+----------+---------+--------------------+------+------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+-----------+------------------+\n|amount_paid|isnan(amount_paid)|\n+-----------+------------------+\n|    1000.55|             false|\n|      900.0|             false|\n|     850.55|             false|\n|        NaN|              true|\n|        NaN|              true|\n+-----------+------------------+\n\n+---+----------+---------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|last_name|               email|gender| current_city|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+---------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  4|     Ashby| Maddocks|  amaddocks3@home.pl|  male|San Fransisco|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|     Rome|krome4@shutterfly...|female|         null|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+---------+--------------------+------+-------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## filter with != , filter with OR\nusers_df.select('id', 'current_city').filter(col('current_city') != 'Dallas').show()\n\n## filter with and ie |\nusers_df.select('id', 'current_city').filter((col('current_city') != 'Dallas') | (col('current_city').isNull())).show()\n\n## filter with OR \nusers_df.select('id', 'current_city').filter(\"current_city != 'Dallas' OR current_city IS NULL\").show()\n\n## \nusers_df.select('id', 'current_city').filter(col('current_city') != '').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4fa616b5-f9ff-48f2-8ba1-3b408bb4d17b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-------------+\n| id| current_city|\n+---+-------------+\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  1|       Dallas|\n|  2|      Houston|\n|  4|San Fransisco|\n+---+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------------+\n| id| current_city|\n+---+-------------+\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  1|       Dallas|\n|  2|      Houston|\n|  4|San Fransisco|\n+---+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["### between operator \n### between(lowerBound, upperBound)\n\nusers_df.select('id', 'email', 'last_updated_ts').filter(col('last_updated_ts').between('2021-02-15 00:00:00', '2021-03-15 23:59:59')).show()\n\n#########\nusers_df.select('id', 'amount_paid').filter('amount_paid BETWEEN \"850\" AND \"900\"').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2baec1ac-fef5-4a98-b6d0-5713d26869ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------------------+-------------------+\n| id|               email|    last_updated_ts|\n+---+--------------------+-------------------+\n|  2|nbrewitt1@dailyma...|2021-02-18 03:33:00|\n|  3|openney2@vistapri...|2021-03-15 15:16:55|\n+---+--------------------+-------------------+\n\n+---+-----------+\n| id|amount_paid|\n+---+-----------+\n|  2|      900.0|\n|  3|     850.55|\n+---+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------------------+-------------------+\n| id|               email|    last_updated_ts|\n+---+--------------------+-------------------+\n|  2|nbrewitt1@dailyma...|2021-02-18 03:33:00|\n|  3|openney2@vistapri...|2021-03-15 15:16:55|\n+---+--------------------+-------------------+\n\n+---+-----------+\n| id|amount_paid|\n+---+-----------+\n|  2|      900.0|\n|  3|     850.55|\n+---+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## is null operations\nusers_df.select('id', 'current_city').filter(col('current_city').isNull()).show()\n## same with string \nusers_df.select('id', 'customer_from').filter('customer_from IS NULL').show()\n## is not null function\nusers_df.select('id', 'current_city').filter(col('current_city').isNotNull()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b72d8a6f-fab3-4638-a660-a0c19f5f0187","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------+\n| id|current_city|\n+---+------------+\n|  5|        null|\n+---+------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  4|         null|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  1|       Dallas|\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n+---+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------+\n| id|current_city|\n+---+------------+\n|  5|        null|\n+---+------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  4|         null|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id| current_city|\n+---+-------------+\n|  1|       Dallas|\n|  2|      Houston|\n|  3|             |\n|  4|San Fransisco|\n+---+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Boolean Operations\n\n#     Boolean OR\n#     Boolean AND\n#     Negation\nprint(True or True)\nprint(True or False)\nprint(True and True)\nprint(False or False)\nprint(not True)\nprint(not False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b0a43df-67b3-4f57-8980-da1a1f7e1c28","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"True\nTrue\nTrue\nFalse\nFalse\nTrue\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["True\nTrue\nTrue\nFalse\nFalse\nTrue\n"]}}],"execution_count":0},{"cell_type":"code","source":["#10 Boolean OR on same column of Spark Data Frame and IN Operator\n\nusers_df.select('id', 'current_city').filter((col('current_city') == '') | (col('current_city').isNull())).show()\n## note: ie showing null values and ''"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cfa38c9e-1b3b-4c2d-9fc1-ec1a46e11fe5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------+\n| id|current_city|\n+---+------------+\n|  3|            |\n|  5|        null|\n+---+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------+\n| id|current_city|\n+---+------------+\n|  3|            |\n|  5|        null|\n+---+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n# Not recommended, use in instead\nusers_df.select('id', 'current_city').filter((col('current_city') == 'Houston') | (col('current_city') == 'Dallas')).show()\n## note: above method is not recomadded as filtering is going to happen in single column \n# Not recommended, use in instead\nusers_df.select('id', 'current_city').filter(\"current_city = 'Houston' OR current_city = 'Dallas'\").show()\n#### note: above method is not recomadded as filtering is going to happen in single column \nusers_df.select('id','current_city').filter(col(\"current_city\").isin('Houston','Dallas')).show()\n\n## isin () is used within filter \nusers_df.select('id', 'current_city').filter(\"current_city IN ('Houston', 'Dallas', '','NULL')\").show()\n# IMp Note :Passing null will not be effective\nusers_df.select('id', 'current_city').filter(\"current_city IN ('Houston', 'Dallas', '') OR current_city IS NULL\").show()\n\n\n# Boolean OR including null check\nusers_df.select('id', 'current_city').filter((col('current_city').isin('Houston', 'Dallas', '')) | (col('current_city').isNull())).show()\n## use \"is NULL \" in string expresion filter or use .isNull() in filter conditions "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7893df44-ae94-41f8-952d-b1ada4bb94f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n|  3|            |\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n|  3|            |\n|  5|        null|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n|  3|            |\n|  5|        null|\n+---+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n|  3|            |\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n|  3|            |\n|  5|        null|\n+---+------------+\n\n+---+------------+\n| id|current_city|\n+---+------------+\n|  1|      Dallas|\n|  2|     Houston|\n|  3|            |\n|  5|        null|\n+---+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n#     >\n#     <\n#     >= (equivalent to boolean with col1 > val1 or cal1 = val1)\n#     <= (equivalent to boolean with col1 < val1 or cal1 = val1)\n## filtering with greter than , equal to , <= ,>=\nusers_df.select('id', 'customer_from').filter('customer_from > \"2021-01-21\"').show()\n\nusers_df.select('id', 'customer_from').filter(col('customer_from') > '2021-01-21').show()\n\n## question : find id and amount_paid which are less than 900 and not null \nusers_df.filter('amount_paid <= 900 AND isnan(amount_paid) = false').select('id', 'amount_paid').show()\n\n## find id of cutomer who are customer joined after 2021-02-14\nusers_df.select('id', 'customer_from').filter('customer_from > \"2021-01-21\"').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68b2a557-b64e-4bc0-88bc-3ec8b3d269c1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n+---+-------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n+---+-------------+\n\n+---+-----------+\n| id|amount_paid|\n+---+-----------+\n|  2|      900.0|\n|  3|     850.55|\n+---+-----------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n+---+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n+---+-------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n+---+-------------+\n\n+---+-----------+\n| id|amount_paid|\n+---+-----------+\n|  2|      900.0|\n|  3|     850.55|\n+---+-----------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n+---+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## boolean and and conditioning \nusers_df.filter((col('gender') == 'male') & (col('is_customer') == True)).select('id', 'gender', 'is_customer').show()\nusers_df.filter(\"customer_from >= '2021-01-20' AND customer_from  <= '2021-02-15'\").select('id', 'customer_from').show()\nusers_df.filter((col('customer_from') >= '2021-01-20') & (col('customer_from') <= '2021-02-15')).select('id', 'customer_from').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a87d9b73-29e4-4631-bfc2-1e641b3a8a12","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------+-----------+\n| id|gender|is_customer|\n+---+------+-----------+\n|  1|  male|       true|\n|  2|  male|       true|\n+---+------+-----------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n|  3|   2021-01-21|\n+---+-------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n|  3|   2021-01-21|\n+---+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------+-----------+\n| id|gender|is_customer|\n+---+------+-----------+\n|  1|  male|       true|\n|  2|  male|       true|\n+---+------+-----------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n|  3|   2021-01-21|\n+---+-------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  2|   2021-02-14|\n|  3|   2021-01-21|\n+---+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# dropping the columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7e19840-1934-44e1-bcda-9cb4a5d9dade","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import Row\nimport datetime\nusers = [\n    {\n        \"id\": 1,\n        \"first_name\": \"Corrie\",\n        \"last_name\": \"Van den Oord\",\n        \"email\": \"cvandenoord0@etsy.com\",\n        \"phone_numbers\": Row(mobile=\"+1 234 567 8901\", home=\"+1 234 567 8911\"),\n        \"courses\": [1, 2],\n        \"is_customer\": True,\n        \"amount_paid\": 1000.55,\n        \"customer_from\": datetime.date(2021, 1, 15),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n    },\n    {\n        \"id\": 2,\n        \"first_name\": \"Nikolaus\",\n        \"last_name\": \"Brewitt\",\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"phone_numbers\":  Row(mobile=\"+1 234 567 8923\", home=\"1 234 567 8934\"),\n        \"courses\": [3],\n        \"is_customer\": True,\n        \"amount_paid\": 900.0,\n        \"customer_from\": datetime.date(2021, 2, 14),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n    },\n    {\n        \"id\": 3,\n        \"first_name\": \"Orelie\",\n        \"last_name\": \"Penney\",\n        \"email\": \"openney2@vistaprint.com\",\n        \"phone_numbers\": Row(mobile=\"+1 714 512 9752\", home=\"+1 714 512 6601\"),\n        \"courses\": [2, 4],\n        \"is_customer\": True,\n        \"amount_paid\": 850.55,\n        \"customer_from\": datetime.date(2021, 1, 21),\n        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n    },\n    {\n        \"id\": 4,\n        \"first_name\": \"Ashby\",\n        \"last_name\": \"Maddocks\",\n        \"email\": \"amaddocks3@home.pl\",\n        \"phone_numbers\": Row(mobile=None, home=None),\n        \"courses\": [],\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n    },\n    {\n        \"id\": 5,\n        \"first_name\": \"Kurt\",\n        \"last_name\": \"Rome\",\n        \"email\": \"krome4@shutterfly.com\",\n        \"phone_numbers\": Row(mobile=\"+1 817 934 7142\", home=None),\n        \"courses\": [],\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n    }\n]\nimport pandas as pd\nspark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)\n# note:  using PyArrow if you are using pandas in your spark application, it will speed the data conversion between spark and pandas ie if true \nusers_df = spark.createDataFrame(pd.DataFrame(users))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35c52d77-0fe1-4874-bc10-ba7bb555481e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["users_df.show(3,False)\nusers_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11b7d8a3-d090-44aa-9fa4-49520e415d28","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+-------------------------+----------------------------------+-------+-----------+-----------+-------------+-------------------+\n|id |first_name|last_name   |email                    |phone_numbers                     |courses|is_customer|amount_paid|customer_from|last_updated_ts    |\n+---+----------+------------+-------------------------+----------------------------------+-------+-----------+-----------+-------------+-------------------+\n|1  |Corrie    |Van den Oord|cvandenoord0@etsy.com    |{+1 234 567 8901, +1 234 567 8911}|[1, 2] |true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|\n|2  |Nikolaus  |Brewitt     |nbrewitt1@dailymail.co.uk|{+1 234 567 8923, 1 234 567 8934} |[3]    |true       |900.0      |2021-02-14   |2021-02-18 03:33:00|\n|3  |Orelie    |Penney      |openney2@vistaprint.com  |{+1 714 512 9752, +1 714 512 6601}|[2, 4] |true       |850.55     |2021-01-21   |2021-03-15 15:16:55|\n+---+----------+------------+-------------------------+----------------------------------+-------+-----------+-----------+-------------+-------------------+\nonly showing top 3 rows\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+-------------------------+----------------------------------+-------+-----------+-----------+-------------+-------------------+\n|id |first_name|last_name   |email                    |phone_numbers                     |courses|is_customer|amount_paid|customer_from|last_updated_ts    |\n+---+----------+------------+-------------------------+----------------------------------+-------+-----------+-----------+-------------+-------------------+\n|1  |Corrie    |Van den Oord|cvandenoord0@etsy.com    |{+1 234 567 8901, +1 234 567 8911}|[1, 2] |true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|\n|2  |Nikolaus  |Brewitt     |nbrewitt1@dailymail.co.uk|{+1 234 567 8923, 1 234 567 8934} |[3]    |true       |900.0      |2021-02-14   |2021-02-18 03:33:00|\n|3  |Orelie    |Penney      |openney2@vistaprint.com  |{+1 714 512 9752, +1 714 512 6601}|[2, 4] |true       |850.55     |2021-01-21   |2021-03-15 15:16:55|\n+---+----------+------------+-------------------------+----------------------------------+-------+-----------+-----------+-------------+-------------------+\nonly showing top 3 rows\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## dropping single column \nusers_df.drop('last_updated_ts').printSchema()\nusers_df.drop(users_df['last_updated_ts']).printSchema()\nusers_df.drop(col('last_updated_ts')).printSchema()\n## above all three do the same things \n# If we have column name which does not exist, the column will be ignored\nusers_df.drop(col('user_id')).printSchema()\n\n##note: if we try to drop the column which is not present in dataframe it will not throw error just do nothing \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"635b9b1c-4b74-4a34-aaca-0ef7ca756b94","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## droping multiple columns \n\nusers_df.drop(col('first_name'), col('id')).printSchema()\n##Imp Note: above will fail when we are dropping multiple columns  we need to pass them as strings  \n## ie TypeError: each col in the param list should be a string error \n## # This will fail as we are passing multiple column objects\n# When we want to pass more than one column, we have to pass all column names as strings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1978e3a9-bb73-46c2-87c2-d8c4d8d9554b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-402239863734907>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m## droping multiple columns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0musers_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'first_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;31m##Imp Note: above will fail when we are dropping multiple columns  we need to pass them as strings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m## # This will fail as we are passing multiple column objects\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2738\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2739\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2740\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"each col in the param list should be a string\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2741\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2742\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: each col in the param list should be a string","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: each col in the param list should be a string","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-402239863734907>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m## droping multiple columns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0musers_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'first_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;31m##Imp Note: above will fail when we are dropping multiple columns  we need to pass them as strings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m## # This will fail as we are passing multiple column objects\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2738\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2739\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2740\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"each col in the param list should be a string\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2741\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2742\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: each col in the param list should be a string"]}}],"execution_count":0},{"cell_type":"code","source":["users_df.drop('user_id', 'first_name', 'last_name').printSchema()\nusers_df.drop('user_id', 'first_name', 'last_name').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e25e373d-ee29-45d5-958b-3dc6e78d722b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: long (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\n+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: long (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\n+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## droping list of columns in pyspark \nusers_df.show(3)\npii_columns = ['first_name', 'last_name', 'email', 'phone_numbers']\n\n# This will fail\n# We need to convert list to varrying arguments to get it working\nusers_df_nopii = users_df.drop(pii_columns)\n\n## note : passing list to drop column will fail the thing we need to pass varing argument to that .\n## *args "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3e57c6d-ad9a-4329-b64b-38131099d091","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\nonly showing top 3 rows\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-402239863734909>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# This will fail\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m# We need to convert list to varrying arguments to get it working\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0musers_df_nopii\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0musers_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpii_columns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2734\u001B[0m                 \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2735\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2736\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be a string or a Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2737\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2738\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: col should be a string or a Column","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: col should be a string or a Column","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-402239863734909>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# This will fail\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m# We need to convert list to varrying arguments to get it working\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0musers_df_nopii\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0musers_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpii_columns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2734\u001B[0m                 \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2735\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2736\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be a string or a Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2737\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2738\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: col should be a string or a Column"]}}],"execution_count":0},{"cell_type":"code","source":["users_df_nopii = users_df.drop(*pii_columns).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00ba0b14-4377-4477-b95a-e9f48ad58b5b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-------+-----------+-----------+-------------+-------------------+\n| id|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+-------+-----------+-----------+-------------+-------------------+\n|  1| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+-------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------+-----------+-----------+-------------+-------------------+\n| id|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+-------+-----------+-----------+-------------+-------------------+\n|  1| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+-------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["##Here are the features that are available with respect to dropping duplicate records.\n\n#     Drop duplicates based on all columns. It is also known as distinct.\n#     Drop duplicates based on certain columns.\n#     We can use distinct, drop_duplicates or dropDuplicates for these scenarios.\n\n## drop_duplicates == based on all columns \n## drop duplicates based on certain columns \n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"edc83841-20d0-42b4-9553-f946020b0a03","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\nusers = [\n    {\n        \"id\": 1,\n        \"first_name\": \"Corrie\",\n        \"last_name\": \"Van den Oord\",\n        \"email\": \"cvandenoord0@etsy.com\",\n        \"is_customer\": True,\n        \"amount_paid\": 1000.55,\n        \"customer_from\": datetime.date(2021, 1, 15),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n    },\n    {\n        \"id\": 2,\n        \"first_name\": \"Nikolaus\",\n        \"last_name\": \"Brewitt\",\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"is_customer\": True,\n        \"amount_paid\": 900.0,\n        \"customer_from\": datetime.date(2021, 2, 14),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n    },\n    {\n        \"id\": 3,\n        \"first_name\": \"Orelie\",\n        \"last_name\": \"Penney\",\n        \"email\": \"openney2@vistaprint.com\",\n        \"is_customer\": True,\n        \"amount_paid\": 850.55,\n        \"customer_from\": datetime.date(2021, 1, 21),\n        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n    },\n    {\n        \"id\": 3,\n        \"first_name\": \"Orelie\",\n        \"last_name\": \"Penney\",\n        \"email\": \"openney2@vistaprint.com\",\n        \"is_customer\": True,\n        \"amount_paid\": 850.55,\n        \"customer_from\": datetime.date(2021, 1, 21),\n        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n    },\n    {\n        \"id\": 4,\n        \"first_name\": \"Ashby\",\n        \"last_name\": \"Maddocks\",\n        \"email\": \"amaddocks3@home.pl\",\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n    },\n    {\n        \"id\": 4,\n        \"first_name\": \"Ashby\",\n        \"last_name\": \"Maddocks\",\n        \"email\": \"amaddocks3@home.pl\",\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n    },\n    {\n        \"id\": 5,\n        \"first_name\": \"Kurt\",\n        \"last_name\": \"Rome\",\n        \"email\": \"krome4@shutterfly.com\",\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n    },\n    {\n        \"id\": 2,\n        \"first_name\": \"Nikolaus\",\n        \"last_name\": \"Brewitt\",\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"is_customer\": True,\n        \"amount_paid\": 1050.0,\n        \"customer_from\": datetime.date(2021, 2, 14),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 25, 3, 33, 0)\n    }\n]\n\n\nimport pandas as pd\nusers_df = spark.createDataFrame(pd.DataFrame(users))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec54f18d-aaef-4285-8d23-ceb1477bfaf9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["users_df.show(4,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"870624d9-f029-4fe8-bd1e-0e9539f815e5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+-------------------------+-----------+-----------+-------------+-------------------+\n|id |first_name|last_name   |email                    |is_customer|amount_paid|customer_from|last_updated_ts    |\n+---+----------+------------+-------------------------+-----------+-----------+-------------+-------------------+\n|1  |Corrie    |Van den Oord|cvandenoord0@etsy.com    |true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|\n|2  |Nikolaus  |Brewitt     |nbrewitt1@dailymail.co.uk|true       |900.0      |2021-02-14   |2021-02-18 03:33:00|\n|3  |Orelie    |Penney      |openney2@vistaprint.com  |true       |850.55     |2021-01-21   |2021-03-15 15:16:55|\n|3  |Orelie    |Penney      |openney2@vistaprint.com  |true       |850.55     |2021-01-21   |2021-03-15 15:16:55|\n+---+----------+------------+-------------------------+-----------+-----------+-------------+-------------------+\nonly showing top 4 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+-------------------------+-----------+-----------+-------------+-------------------+\n|id |first_name|last_name   |email                    |is_customer|amount_paid|customer_from|last_updated_ts    |\n+---+----------+------------+-------------------------+-----------+-----------+-------------+-------------------+\n|1  |Corrie    |Van den Oord|cvandenoord0@etsy.com    |true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|\n|2  |Nikolaus  |Brewitt     |nbrewitt1@dailymail.co.uk|true       |900.0      |2021-02-14   |2021-02-18 03:33:00|\n|3  |Orelie    |Penney      |openney2@vistaprint.com  |true       |850.55     |2021-01-21   |2021-03-15 15:16:55|\n|3  |Orelie    |Penney      |openney2@vistaprint.com  |true       |850.55     |2021-01-21   |2021-03-15 15:16:55|\n+---+----------+------------+-------------------------+-----------+-----------+-------------+-------------------+\nonly showing top 4 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(users_df.count())\n## count will give the no of rows present \nprint(users_df.distinct().count())\n\n## note: distinmct will give \nusers_df.dropDuplicates().count()\n## dropDuplicates will drop duplicate rows ie taking account all columns combinations check results eg id,amount_column \nusers_df.dropDuplicates().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"857cabc8-b814-41b9-afca-ee1371328fd2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8\n6\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n6\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["users_df.dropDuplicates(['id', 'amount_paid']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"42c8358c-f242-4123-8d67-4dd610b50d49","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Here are the features that are available with respect to dropping records based on null values.\n# * Drop records when all column values are nulls.\n# * Drop records any of the column value is null.\n# * Drop records that have less than `thresh` non-null values.\n# * Drop records when any of the column value or all column values are nulls for provided subset of columns.\n# * We can use `df.na.drop` or `df.dropna` to take care of dealing with records having columns with null values."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8e5e1d6-4207-4fd0-8bee-52c72df9eac5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\nusers = [\n    {\n        \"id\": 1,\n        \"first_name\": \"Corrie\",\n        \"last_name\": \"Van den Oord\",\n        \"email\": \"cvandenoord0@etsy.com\",\n        \"is_customer\": True,\n        \"amount_paid\": 1000.55,\n        \"customer_from\": datetime.date(2021, 1, 15),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n    },\n    {\n        \"id\": None,\n        \"first_name\": None,\n        \"last_name\": None,\n        \"email\": None,\n        \"is_customer\": None,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": None\n    },\n    {\n        \"id\": 2,\n        \"first_name\": \"Nikolaus\",\n        \"last_name\": \"Brewitt\",\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"is_customer\": True,\n        \"amount_paid\": 900.0,\n        \"customer_from\": datetime.date(2021, 2, 14),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n    },\n    {\n        \"id\": 3,\n        \"first_name\": \"Orelie\",\n        \"last_name\": \"Penney\",\n        \"email\": \"openney2@vistaprint.com\",\n        \"is_customer\": True,\n        \"amount_paid\": 850.55,\n        \"customer_from\": datetime.date(2021, 1, 21),\n        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n    },\n    {\n        \"id\": 3,\n        \"first_name\": \"Orelie\",\n        \"last_name\": \"Penney\",\n        \"email\": \"openney2@vistaprint.com\",\n        \"is_customer\": True,\n        \"amount_paid\": 850.55,\n        \"customer_from\": datetime.date(2021, 1, 21),\n        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n    },\n    {\n        \"id\": 4,\n        \"first_name\": \"Ashby\",\n        \"last_name\": \"Maddocks\",\n        \"email\": \"amaddocks3@home.pl\",\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n    },\n    {\n        \"id\": 4,\n        \"first_name\": \"Ashby\",\n        \"last_name\": \"Maddocks\",\n        \"email\": \"amaddocks3@home.pl\",\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n    },\n    {\n        \"id\": 5,\n        \"first_name\": \"Kurt\",\n        \"last_name\": \"Rome\",\n        \"email\": \"krome4@shutterfly.com\",\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n    },\n    {\n        \"id\": None,\n        \"first_name\": None,\n        \"last_name\": None,\n        \"email\": None,\n        \"is_customer\": None,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": None\n    },\n    {\n        \"id\": 5,\n        \"first_name\": None,\n        \"last_name\": None,\n        \"email\": None,\n        \"is_customer\": None,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": None\n    },\n    {\n        \"id\": None,\n        \"first_name\": None,\n        \"last_name\": None,\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"is_customer\": None,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": None\n    },\n    {\n        \"id\": None,\n        \"first_name\": \"Kurt\",\n        \"last_name\": \"Rome\",\n        \"email\": None,\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n    },\n    {\n        \"id\": 2,\n        \"first_name\": \"Nikolaus\",\n        \"last_name\": \"Brewitt\",\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"is_customer\": True,\n        \"amount_paid\": 1050.0,\n        \"customer_from\": datetime.date(2021, 2, 14),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 25, 3, 33, 0)\n    }\n]\n\n\nimport pandas as pd\nusers_df = spark.createDataFrame(pd.DataFrame(users))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c6deed6-b597-4e09-9a46-104a461c93e7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["users_df.count()\nusers_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3b9ef7d3-9be5-49d4-b92a-2b5c1f5b084b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|5.0|      null|        null|                null|       null|        NaN|         null|               null|\n|NaN|      null|        null|nbrewitt1@dailyma...|       null|        NaN|         null|               null|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|5.0|      null|        null|                null|       null|        NaN|         null|               null|\n|NaN|      null|        null|nbrewitt1@dailyma...|       null|        NaN|         null|               null|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Attribute which exposes functions dealing with null records\n# drop => users_df.dropna\n# fill => users_df.fillna\n# replace => users_df.replacena\n\n## note: basically df.na.drop()\n## ie df.na === is attribute \n# note: drop(how='any', thresh=None, subset=None)\n\nusers_df.na.drop('all').show()\n# Drop if any column value is null\nusers_df.na.drop('any').show()\nusers_df.na.drop(thresh=2).show()\n\nhelp(users_df.drop())\n\n## dropDuplicates(self, subset=None)\nusers_df.na.drop(how='any', subset=['id', 'email']).show()\n\n## \n#  subset : str, tuple or list, optional optional list of column names to consider.\n# thresh: int, optiona default None  ie dropna only after thrushhold is briched \n#  If specified, drop rows that have less than `thresh` non-null values.This overwrites the `how` parameter.\nusers_df.na.drop(how='any', subset=['id', 'email']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"135ca95f-4883-442d-94b4-aa0dbab36328","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|5.0|      null|        null|                null|       null|        NaN|         null|               null|\n|NaN|      null|        null|nbrewitt1@dailyma...|       null|        NaN|         null|               null|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\nHelp on DataFrame in module pyspark.sql.dataframe object:\n\nclass DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n |  DataFrame(jdf, sql_ctx)\n |  \n |  A distributed collection of data grouped into named columns.\n |  \n |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n |  and can be created using various functions in :class:`SparkSession`::\n |  \n |      people = spark.read.parquet(\"...\")\n |  \n |  Once created, it can be manipulated using the various domain-specific-language\n |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n |  \n |  To select a column from the :class:`DataFrame`, use the apply method::\n |  \n |      ageCol = people.age\n |  \n |  A more concrete example::\n |  \n |      # To create DataFrame using SparkSession\n |      people = spark.read.parquet(\"...\")\n |      department = spark.read.parquet(\"...\")\n |  \n |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n |  \n |  .. versionadded:: 1.3.0\n |  \n |  Method resolution order:\n |      DataFrame\n |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n |      pyspark.sql.pandas.conversion.PandasConversionMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getattr__(self, name)\n |      Returns the :class:`Column` denoted by ``name``.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.select(df.age).collect()\n |      [Row(age=2), Row(age=5)]\n |  \n |  __getitem__(self, item)\n |      Returns the column as a :class:`Column`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.select(df['age']).collect()\n |      [Row(age=2), Row(age=5)]\n |      >>> df[ [\"name\", \"age\"]].collect()\n |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n |      >>> df[ df.age > 3 ].collect()\n |      [Row(age=5, name='Bob')]\n |      >>> df[df[0] > 3].collect()\n |      [Row(age=5, name='Bob')]\n |  \n |  __init__(self, jdf, sql_ctx)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  agg(self, *exprs)\n |      Aggregate on the entire :class:`DataFrame` without groups\n |      (shorthand for ``df.groupBy().agg()``).\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.agg({\"age\": \"max\"}).collect()\n |      [Row(max(age)=5)]\n |      >>> from pyspark.sql import functions as F\n |      >>> df.agg(F.min(df.age)).collect()\n |      [Row(min(age)=2)]\n |  \n |  alias(self, alias)\n |      Returns a new :class:`DataFrame` with an alias set.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      alias : str\n |          an alias name to be set for the :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.functions import *\n |      >>> df_as1 = df.alias(\"df_as1\")\n |      >>> df_as2 = df.alias(\"df_as2\")\n |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n |  \n |  approxQuantile(self, col, probabilities, relativeError)\n |      Calculates the approximate quantiles of numerical columns of a\n |      :class:`DataFrame`.\n |      \n |      The result of this algorithm has the following deterministic bound:\n |      If the :class:`DataFrame` has N elements and if we request the quantile at\n |      probability `p` up to error `err`, then the algorithm will return\n |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n |      close to (p * N). More precisely,\n |      \n |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n |      \n |      This method implements a variation of the Greenwald-Khanna\n |      algorithm (with some speed optimizations). The algorithm was first\n |      present in [[https://doi.org/10.1145/375663.375670\n |      Space-efficient Online Computation of Quantile Summaries]]\n |      by Greenwald and Khanna.\n |      \n |      Note that null values will be ignored in numerical columns before calculation.\n |      For columns only containing null values, an empty list is returned.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      col: str, tuple or list\n |          Can be a single column name, or a list of names for multiple columns.\n |      \n |          .. versionchanged:: 2.2\n |             Added support for multiple columns.\n |      probabilities : list or tuple\n |          a list of quantile probabilities\n |          Each number must belong to [0, 1].\n |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n |      relativeError : float\n |          The relative target precision to achieve\n |          (>= 0). If set to zero, the exact quantiles are computed, which\n |          could be very expensive. Note that values greater than 1 are\n |          accepted but give the same result as 1.\n |      \n |      Returns\n |      -------\n |      list\n |          the approximate quantiles at the given probabilities. If\n |          the input `col` is a string, the output is a list of floats. If the\n |          input `col` is a list or tuple of strings, the output is also a\n |          list, but each element in it is a list of floats, i.e., the output\n |          is a list of list of floats.\n |  \n |  cache(self)\n |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n |  \n |  checkpoint(self, eager=True)\n |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      eager : bool, optional\n |          Whether to checkpoint this :class:`DataFrame` immediately\n |      \n |      Notes\n |      -----\n |      This API is experimental.\n |  \n |  coalesce(self, numPartitions)\n |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n |      \n |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n |      there will not be a shuffle, instead each of the 100 new partitions will\n |      claim 10 of the current partitions. If a larger number of partitions is requested,\n |      it will stay at the current number of partitions.\n |      \n |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n |      this may result in your computation taking place on fewer nodes than\n |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n |      you can call repartition(). This will add a shuffle step, but means the\n |      current upstream partitions will be executed in parallel (per whatever\n |      the current partitioning is).\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      numPartitions : int\n |          specify the target number of partitions\n |      \n |      Examples\n |      --------\n |      >>> df.coalesce(1).rdd.getNumPartitions()\n |      1\n |  \n |  colRegex(self, colName)\n |      Selects column based on the column name specified as a regex and returns it\n |      as :class:`Column`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      colName : str\n |          string, column name specified as a regex.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n |      +----+\n |      |Col2|\n |      +----+\n |      |   1|\n |      |   2|\n |      |   3|\n |      +----+\n |  \n |  collect(self)\n |      Returns all the records as a list of :class:`Row`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.collect()\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |  \n |  corr(self, col1, col2, method=None)\n |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n |      Currently only supports the Pearson Correlation Coefficient.\n |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column\n |      col2 : str\n |          The name of the second column\n |      method : str, optional\n |          The correlation method. Currently only supports \"pearson\"\n |  \n |  count(self)\n |      Returns the number of rows in this :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.count()\n |      2\n |  \n |  cov(self, col1, col2)\n |      Calculate the sample covariance for the given columns, specified by their names, as a\n |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column\n |      col2 : str\n |          The name of the second column\n |  \n |  createGlobalTempView(self, name)\n |      Creates a global temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Examples\n |      --------\n |      >>> df.createGlobalTempView(\"people\")\n |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n |      >>> sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: u\"Temporary table 'people' already exists;\"\n |      >>> spark.catalog.dropGlobalTempView(\"people\")\n |  \n |  createOrReplaceGlobalTempView(self, name)\n |      Creates or replaces a global temporary view using the given name.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      \n |      .. versionadded:: 2.2.0\n |      \n |      Examples\n |      --------\n |      >>> df.createOrReplaceGlobalTempView(\"people\")\n |      >>> df2 = df.filter(df.age > 3)\n |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n |      >>> sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      >>> spark.catalog.dropGlobalTempView(\"people\")\n |  \n |  createOrReplaceTempView(self, name)\n |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Examples\n |      --------\n |      >>> df.createOrReplaceTempView(\"people\")\n |      >>> df2 = df.filter(df.age > 3)\n |      >>> df2.createOrReplaceTempView(\"people\")\n |      >>> df3 = spark.sql(\"select * from people\")\n |      >>> sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      >>> spark.catalog.dropTempView(\"people\")\n |  \n |  createTempView(self, name)\n |      Creates a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Examples\n |      --------\n |      >>> df.createTempView(\"people\")\n |      >>> df2 = spark.sql(\"select * from people\")\n |      >>> sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: u\"Temporary table 'people' already exists;\"\n |      >>> spark.catalog.dropTempView(\"people\")\n |  \n |  crossJoin(self, other)\n |      Returns the cartesian product with another :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`DataFrame`\n |          Right side of the cartesian product.\n |      \n |      Examples\n |      --------\n |      >>> df.select(\"age\", \"name\").collect()\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |      >>> df2.select(\"name\", \"height\").collect()\n |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n |  \n |  crosstab(self, col1, col2)\n |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n |      non-zero pair frequencies will be returned.\n |      The first column of each row will be the distinct values of `col1` and the column names\n |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n |      Pairs that have no occurrences will have zero as their counts.\n |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column. Distinct items will make the first item of\n |          each row.\n |      col2 : str\n |          The name of the second column. Distinct items will make the column names\n |          of the :class:`DataFrame`.\n |  \n |  cube(self, *cols)\n |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n |      the specified columns, so we can run aggregations on them.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Examples\n |      --------\n |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n |      +-----+----+-----+\n |      | name| age|count|\n |      +-----+----+-----+\n |      | null|null|    2|\n |      | null|   2|    1|\n |      | null|   5|    1|\n |      |Alice|null|    1|\n |      |Alice|   2|    1|\n |      |  Bob|null|    1|\n |      |  Bob|   5|    1|\n |      +-----+----+-----+\n |  \n |  describe(self, *cols)\n |      Computes basic statistics for numeric and string columns.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      This include count, mean, stddev, min, and max. If no columns are\n |      given, this function computes statistics for all numerical or string columns.\n |      \n |      Notes\n |      -----\n |      This function is meant for exploratory data analysis, as we make no\n |      guarantee about the backward compatibility of the schema of the resulting\n |      :class:`DataFrame`.\n |      \n |      Use summary for expanded statistics and control over which statistics to compute.\n |      \n |      Examples\n |      --------\n |      >>> df.describe(['age']).show()\n |      +-------+------------------+\n |      |summary|               age|\n |      +-------+------------------+\n |      |  count|                 2|\n |      |   mean|               3.5|\n |      | stddev|2.1213203435596424|\n |      |    min|                 2|\n |      |    max|                 5|\n |      +-------+------------------+\n |      >>> df.describe().show()\n |      +-------+------------------+-----+\n |      |summary|               age| name|\n |      +-------+------------------+-----+\n |      |  count|                 2|    2|\n |      |   mean|               3.5| null|\n |      | stddev|2.1213203435596424| null|\n |      |    min|                 2|Alice|\n |      |    max|                 5|  Bob|\n |      +-------+------------------+-----+\n |      \n |      See Also\n |      --------\n |      DataFrame.summary\n |  \n |  display = df_display(df, *args, **kwargs)\n |      df.display() is an alias for display(df). Run help(display) for more information.\n |  \n |  distinct(self)\n |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.distinct().count()\n |      2\n |  \n |  drop(self, *cols)\n |      Returns a new :class:`DataFrame` that drops the specified column.\n |      This is a no-op if schema doesn't contain the given column name(s).\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      cols: str or :class:`Column`\n |          a name of the column, or the :class:`Column` to drop\n |      \n |      Examples\n |      --------\n |      >>> df.drop('age').collect()\n |      [Row(name='Alice'), Row(name='Bob')]\n |      \n |      >>> df.drop(df.age).collect()\n |      [Row(name='Alice'), Row(name='Bob')]\n |      \n |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n |      [Row(age=5, height=85, name='Bob')]\n |      \n |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n |      [Row(age=5, name='Bob', height=85)]\n |      \n |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n |      [Row(name='Bob')]\n |  \n |  dropDuplicates(self, subset=None)\n |      Return a new :class:`DataFrame` with duplicate rows removed,\n |      optionally only considering certain columns.\n |      \n |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n |      be and system will accordingly limit the state. In addition, too late data older than\n |      watermark will be dropped to avoid any possibility of duplicates.\n |      \n |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = sc.parallelize([ \\\n |      ...     Row(name='Alice', age=5, height=80), \\\n |      ...     Row(name='Alice', age=5, height=80), \\\n |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n |      >>> df.dropDuplicates().show()\n |      +-----+---+------+\n |      | name|age|height|\n |      +-----+---+------+\n |      |Alice|  5|    80|\n |      |Alice| 10|    80|\n |      +-----+---+------+\n |      \n |      >>> df.dropDuplicates(['name', 'height']).show()\n |      +-----+---+------+\n |      | name|age|height|\n |      +-----+---+------+\n |      |Alice|  5|    80|\n |      +-----+---+------+\n |  \n |  drop_duplicates = dropDuplicates(self, subset=None)\n |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n |      \n |      .. versionadded:: 1.4\n |  \n |  dropna(self, how='any', thresh=None, subset=None)\n |      Returns a new :class:`DataFrame` omitting rows with null values.\n |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      Parameters\n |      ----------\n |      how : str, optional\n |          'any' or 'all'.\n |          If 'any', drop a row if it contains any nulls.\n |          If 'all', drop a row only if all its values are null.\n |      thresh: int, optional\n |          default None\n |          If specified, drop rows that have less than `thresh` non-null values.\n |          This overwrites the `how` parameter.\n |      subset : str, tuple or list, optional\n |          optional list of column names to consider.\n |      \n |      Examples\n |      --------\n |      >>> df4.na.drop().show()\n |      +---+------+-----+\n |      |age|height| name|\n |      +---+------+-----+\n |      | 10|    80|Alice|\n |      +---+------+-----+\n |  \n |  exceptAll(self, other)\n |      Return a new :clas\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nme='Alice')]\n |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n |  \n |  sortWithinPartitions(self, *cols, **kwargs)\n |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      Parameters\n |      ----------\n |      cols : str, list or :class:`Column`, optional\n |          list of :class:`Column` or column names to sort by.\n |      \n |      Other Parameters\n |      ----------------\n |      ascending : bool or list, optional\n |          boolean or list of boolean (default ``True``).\n |          Sort ascending vs. descending. Specify list for multiple sort orders.\n |          If a list is specified, length of the list must equal length of the `cols`.\n |      \n |      Examples\n |      --------\n |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n |      +---+-----+\n |      |age| name|\n |      +---+-----+\n |      |  2|Alice|\n |      |  5|  Bob|\n |      +---+-----+\n |  \n |  subtract(self, other)\n |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n |      but not in another :class:`DataFrame`.\n |      \n |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n |      \n |      .. versionadded:: 1.3\n |  \n |  summary(self, *statistics)\n |      Computes specified statistics for numeric and string columns. Available statistics are:\n |      - count\n |      - mean\n |      - stddev\n |      - min\n |      - max\n |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n |      \n |      If no statistics are given, this function computes count, mean, stddev, min,\n |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Notes\n |      -----\n |      This function is meant for exploratory data analysis, as we make no\n |      guarantee about the backward compatibility of the schema of the resulting\n |      :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> df.summary().show()\n |      +-------+------------------+-----+\n |      |summary|               age| name|\n |      +-------+------------------+-----+\n |      |  count|                 2|    2|\n |      |   mean|               3.5| null|\n |      | stddev|2.1213203435596424| null|\n |      |    min|                 2|Alice|\n |      |    25%|                 2| null|\n |      |    50%|                 2| null|\n |      |    75%|                 5| null|\n |      |    max|                 5|  Bob|\n |      +-------+------------------+-----+\n |      \n |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n |      +-------+---+-----+\n |      |summary|age| name|\n |      +-------+---+-----+\n |      |  count|  2|    2|\n |      |    min|  2|Alice|\n |      |    25%|  2| null|\n |      |    75%|  5| null|\n |      |    max|  5|  Bob|\n |      +-------+---+-----+\n |      \n |      To do a summary for specific columns first select them:\n |      \n |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n |      +-------+---+----+\n |      |summary|age|name|\n |      +-------+---+----+\n |      |  count|  2|   2|\n |      +-------+---+----+\n |      \n |      See Also\n |      --------\n |      DataFrame.display\n |  \n |  tail(self, num)\n |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n |      \n |      Running tail requires moving data into the application's driver process, and doing so with\n |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Examples\n |      --------\n |      >>> df.tail(1)\n |      [Row(age=5, name='Bob')]\n |  \n |  take(self, num)\n |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.take(2)\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |  \n |  toDF(self, *cols)\n |      Returns a new :class:`DataFrame` that with new specified column names\n |      \n |      Parameters\n |      ----------\n |      cols : str\n |          new column names\n |      \n |      Examples\n |      --------\n |      >>> df.toDF('f1', 'f2').collect()\n |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n |  \n |  toJSON(self, use_unicode=True)\n |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n |      \n |      Each row is turned into a JSON document as one element in the returned RDD.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.toJSON().first()\n |      '{\"age\":2,\"name\":\"Alice\"}'\n |  \n |  toLocalIterator(self, prefetchPartitions=False)\n |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n |      The iterator will consume as much memory as the largest partition in this\n |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n |      partitions.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      prefetchPartitions : bool, optional\n |          If Spark should pre-fetch the next partition  before it is needed.\n |      \n |      Examples\n |      --------\n |      >>> list(df.toLocalIterator())\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |  \n |  to_koalas(self, index_col=None)\n |      # Keep to_koalas for backward compatibility for now.\n |  \n |  to_pandas_on_spark(self, index_col=None)\n |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n |      \n |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n |      to pandas-on-Spark, it will lose the index information and the original index\n |      will be turned into a normal column.\n |      \n |      This is only available if Pandas is installed and available.\n |      \n |      Parameters\n |      ----------\n |      index_col: str or list of str, optional, default: None\n |          Index column of table in Spark.\n |      \n |      See Also\n |      --------\n |      pyspark.pandas.frame.DataFrame.to_spark\n |      \n |      Examples\n |      --------\n |      >>> df.show()  # doctest: +SKIP\n |      +----+----+\n |      |Col1|Col2|\n |      +----+----+\n |      |   a|   1|\n |      |   b|   2|\n |      |   c|   3|\n |      +----+----+\n |      \n |      >>> df.to_pandas_on_spark()  # doctest: +SKIP\n |        Col1  Col2\n |      0    a     1\n |      1    b     2\n |      2    c     3\n |      \n |      We can specify the index columns.\n |      \n |      >>> df.to_pandas_on_spark(index_col=\"Col1\"): # doctest: +SKIP\n |            Col2\n |      Col1\n |      a        1\n |      b        2\n |      c        3\n |  \n |  transform(self, func)\n |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          a function that takes and returns a :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.functions import col\n |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n |      >>> def cast_all_to_int(input_df):\n |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n |      >>> def sort_columns_asc(input_df):\n |      ...     return input_df.select(*sorted(input_df.columns))\n |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n |      +-----+---+\n |      |float|int|\n |      +-----+---+\n |      |    1|  1|\n |      |    2|  2|\n |      +-----+---+\n |  \n |  union(self, other)\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      .. versionadded:: 2.0\n |  \n |  unionAll(self, other)\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      .. versionadded:: 1.3\n |  \n |  unionByName(self, other, allowMissingColumns=False)\n |      Returns a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Examples\n |      --------\n |      The difference between this function and :func:`union` is that this function\n |      resolves columns by name (not by position):\n |      \n |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n |      >>> df1.unionByName(df2).show()\n |      +----+----+----+\n |      |col0|col1|col2|\n |      +----+----+----+\n |      |   1|   2|   3|\n |      |   6|   4|   5|\n |      +----+----+----+\n |      \n |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n |      in the schema of the union result:\n |      \n |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n |      +----+----+----+----+\n |      |col0|col1|col2|col3|\n |      +----+----+----+----+\n |      |   1|   2|   3|null|\n |      |null|   4|   5|   6|\n |      +----+----+----+----+\n |      \n |      .. versionchanged:: 3.1.0\n |         Added optional argument `allowMissingColumns` to specify whether to allow\n |         missing columns.\n |  \n |  unpersist(self, blocking=False)\n |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n |  \n |  where = filter(self, condition)\n |      :func:`where` is an alias for :func:`filter`.\n |      \n |      .. versionadded:: 1.3\n |  \n |  withColumn(self, colName, col)\n |      Returns a new :class:`DataFrame` by adding a column or replacing the\n |      existing column that has the same name.\n |      \n |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n |      a column from some other :class:`DataFrame` will raise an error.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      colName : str\n |          string, name of the new column.\n |      col : :class:`Column`\n |          a :class:`Column` expression for the new column.\n |      \n |      Notes\n |      -----\n |      This method introduces a projection internally. Therefore, calling it multiple\n |      times, for instance, via loops in order to add multiple columns can generate big\n |      plans which can cause performance issues and even `StackOverflowException`.\n |      To avoid this, use :func:`select` with the multiple columns at once.\n |      \n |      Examples\n |      --------\n |      >>> df.withColumn('age2', df.age + 2).collect()\n |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n |  \n |  withColumnRenamed(self, existing, new)\n |      Returns a new :class:`DataFrame` by renaming an existing column.\n |      This is a no-op if schema doesn't contain the given column name.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      existing : str\n |          string, name of the existing column to rename.\n |      new : str\n |          string, new name of the column.\n |      \n |      Examples\n |      --------\n |      >>> df.withColumnRenamed('age', 'age2').collect()\n |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n |  \n |  withColumns(self, *colsMap)\n |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n |      existing columns that has the same names.\n |      \n |      The colsMap is a map of column name and column, the column must only refer to attributes\n |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n |      \n |      .. versionadded:: 3.3.0\n |         Added support for multiple columns adding\n |      \n |      Parameters\n |      ----------\n |      colsMap : dict\n |          a dict of column name and :class:`Column`. Currently, only single map is supported.\n |      \n |      Examples\n |      --------\n |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).collect()\n |      [Row(age=2, name='Alice', age2=4, age3=5), Row(age=5, name='Bob', age2=7, age3=8)]\n |  \n |  withMetadata(self, columnName, metadata)\n |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      columnName : str\n |          string, name of the existing column to update the metadata.\n |      metadata : dict\n |          dict, new metadata to be assigned to df.schema[columnName].metadata\n |      \n |      Examples\n |      --------\n |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n |      >>> df_meta.schema['age'].metadata\n |      {'foo': 'bar'}\n |  \n |  withWatermark(self, eventTime, delayThreshold)\n |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n |      in time before which we assume no more late data is going to arrive.\n |      \n |      Spark will use this watermark for several purposes:\n |        - To know when a given time window aggregation can be finalized and thus can be emitted\n |          when using output modes that do not allow updates.\n |      \n |        - To minimize the amount of state that we need to keep for on-going aggregations.\n |      \n |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n |      process records that arrive more than `delayThreshold` late.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      eventTime : str\n |          the name of the column that contains the event time of the row.\n |      delayThreshold : str\n |          the minimum delay to wait to data to arrive late, relative to the\n |          latest record that has been processed in the form of an interval\n |          (e.g. \"1 minute\" or \"5 hours\").\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      >>> from pyspark.sql.functions import timestamp_seconds\n |      >>> sdf.select(\n |      ...    'name',\n |      ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n |      DataFrame[name: string, time: timestamp]\n |  \n |  writeTo(self, table)\n |      Create a write configuration builder for v2 sources.\n |      \n |      This builder is used to configure and execute write operations.\n |      \n |      For example, to append or create or replace existing tables.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Examples\n |      --------\n |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n |      >>> df.writeTo(                              # doctest: +SKIP\n |      ...     \"catalog.db.table\"\n |      ... ).partitionedBy(\"col\").createOrReplace()\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  columns\n |      Returns all column names as a list.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.columns\n |      ['age', 'name']\n |  \n |  dtypes\n |      Returns all column names and their data types as a list.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.dtypes\n |      [('age', 'int'), ('name', 'string')]\n |  \n |  isStreaming\n |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n |      is a streaming source present.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |  \n |  na\n |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n |      \n |      .. versionadded:: 1.3.1\n |  \n |  rdd\n |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n |      \n |      .. versionadded:: 1.3\n |  \n |  schema\n |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.schema\n |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n |  \n |  stat\n |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n |      \n |      .. versionadded:: 1.4\n |  \n |  storageLevel\n |      Get the :class:`DataFrame`'s current storage level.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Examples\n |      --------\n |      >>> df.storageLevel\n |      StorageLevel(False, False, False, False, 1)\n |      >>> df.cache().storageLevel\n |      StorageLevel(True, True, False, True, 1)\n |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n |      StorageLevel(True, False, False, False, 2)\n |  \n |  write\n |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameWriter`\n |  \n |  writeStream\n |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      :class:`DataStreamWriter`\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n |  \n |  mapInPandas(self, func, schema)\n |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n |      function that takes and outputs a pandas DataFrame, and returns the result as a\n |      :class:`DataFrame`.\n |      \n |      The function should take an iterator of `pandas.DataFrame`\\s and return\n |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n |      Each `pandas.DataFrame` size can be controlled by\n |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n |          outputs an iterator of `pandas.DataFrame`\\s.\n |      schema : :class:`pyspark.sql.types.DataType` or str\n |          the return type of the `func` in PySpark. The value can be either a\n |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.functions import pandas_udf\n |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n |      >>> def filter_func(iterator):\n |      ...     for pdf in iterator:\n |      ...         yield pdf[pdf.id == 1]\n |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n |      +---+---+\n |      | id|age|\n |      +---+---+\n |      |  1| 21|\n |      +---+---+\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |      \n |      See Also\n |      --------\n |      pyspark.sql.functions.pandas_udf\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n |  \n |  toPandas(self)\n |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n |      \n |      This is only available if Pandas is installed and available.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting Pandas's :class:`DataFrame` is\n |      expected to be small, as all the data is loaded into the driver's memory.\n |      \n |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n |      \n |      Examples\n |      --------\n |      >>> df.toPandas()  # doctest: +SKIP\n |         age   name\n |      0    2  Alice\n |      1    5    Bob\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|5.0|      null|        null|                null|       null|        NaN|         null|               null|\n|NaN|      null|        null|nbrewitt1@dailyma...|       null|        NaN|         null|               null|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\nHelp on DataFrame in module pyspark.sql.dataframe object:\n\nclass DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n |  DataFrame(jdf, sql_ctx)\n |  \n |  A distributed collection of data grouped into named columns.\n |  \n |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n |  and can be created using various functions in :class:`SparkSession`::\n |  \n |      people = spark.read.parquet(\"...\")\n |  \n |  Once created, it can be manipulated using the various domain-specific-language\n |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n |  \n |  To select a column from the :class:`DataFrame`, use the apply method::\n |  \n |      ageCol = people.age\n |  \n |  A more concrete example::\n |  \n |      # To create DataFrame using SparkSession\n |      people = spark.read.parquet(\"...\")\n |      department = spark.read.parquet(\"...\")\n |  \n |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n |  \n |  .. versionadded:: 1.3.0\n |  \n |  Method resolution order:\n |      DataFrame\n |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n |      pyspark.sql.pandas.conversion.PandasConversionMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getattr__(self, name)\n |      Returns the :class:`Column` denoted by ``name``.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.select(df.age).collect()\n |      [Row(age=2), Row(age=5)]\n |  \n |  __getitem__(self, item)\n |      Returns the column as a :class:`Column`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.select(df['age']).collect()\n |      [Row(age=2), Row(age=5)]\n |      >>> df[ [\"name\", \"age\"]].collect()\n |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n |      >>> df[ df.age > 3 ].collect()\n |      [Row(age=5, name='Bob')]\n |      >>> df[df[0] > 3].collect()\n |      [Row(age=5, name='Bob')]\n |  \n |  __init__(self, jdf, sql_ctx)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  agg(self, *exprs)\n |      Aggregate on the entire :class:`DataFrame` without groups\n |      (shorthand for ``df.groupBy().agg()``).\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.agg({\"age\": \"max\"}).collect()\n |      [Row(max(age)=5)]\n |      >>> from pyspark.sql import functions as F\n |      >>> df.agg(F.min(df.age)).collect()\n |      [Row(min(age)=2)]\n |  \n |  alias(self, alias)\n |      Returns a new :class:`DataFrame` with an alias set.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      alias : str\n |          an alias name to be set for the :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.functions import *\n |      >>> df_as1 = df.alias(\"df_as1\")\n |      >>> df_as2 = df.alias(\"df_as2\")\n |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n |  \n |  approxQuantile(self, col, probabilities, relativeError)\n |      Calculates the approximate quantiles of numerical columns of a\n |      :class:`DataFrame`.\n |      \n |      The result of this algorithm has the following deterministic bound:\n |      If the :class:`DataFrame` has N elements and if we request the quantile at\n |      probability `p` up to error `err`, then the algorithm will return\n |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n |      close to (p * N). More precisely,\n |      \n |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n |      \n |      This method implements a variation of the Greenwald-Khanna\n |      algorithm (with some speed optimizations). The algorithm was first\n |      present in [[https://doi.org/10.1145/375663.375670\n |      Space-efficient Online Computation of Quantile Summaries]]\n |      by Greenwald and Khanna.\n |      \n |      Note that null values will be ignored in numerical columns before calculation.\n |      For columns only containing null values, an empty list is returned.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      col: str, tuple or list\n |          Can be a single column name, or a list of names for multiple columns.\n |      \n |          .. versionchanged:: 2.2\n |             Added support for multiple columns.\n |      probabilities : list or tuple\n |          a list of quantile probabilities\n |          Each number must belong to [0, 1].\n |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n |      relativeError : float\n |          The relative target precision to achieve\n |          (>= 0). If set to zero, the exact quantiles are computed, which\n |          could be very expensive. Note that values greater than 1 are\n |          accepted but give the same result as 1.\n |      \n |      Returns\n |      -------\n |      list\n |          the approximate quantiles at the given probabilities. If\n |          the input `col` is a string, the output is a list of floats. If the\n |          input `col` is a list or tuple of strings, the output is also a\n |          list, but each element in it is a list of floats, i.e., the output\n |          is a list of list of floats.\n |  \n |  cache(self)\n |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n |  \n |  checkpoint(self, eager=True)\n |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      eager : bool, optional\n |          Whether to checkpoint this :class:`DataFrame` immediately\n |      \n |      Notes\n |      -----\n |      This API is experimental.\n |  \n |  coalesce(self, numPartitions)\n |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n |      \n |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n |      there will not be a shuffle, instead each of the 100 new partitions will\n |      claim 10 of the current partitions. If a larger number of partitions is requested,\n |      it will stay at the current number of partitions.\n |      \n |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n |      this may result in your computation taking place on fewer nodes than\n |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n |      you can call repartition(). This will add a shuffle step, but means the\n |      current upstream partitions will be executed in parallel (per whatever\n |      the current partitioning is).\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      numPartitions : int\n |          specify the target number of partitions\n |      \n |      Examples\n |      --------\n |      >>> df.coalesce(1).rdd.getNumPartitions()\n |      1\n |  \n |  colRegex(self, colName)\n |      Selects column based on the column name specified as a regex and returns it\n |      as :class:`Column`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      colName : str\n |          string, column name specified as a regex.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n |      +----+\n |      |Col2|\n |      +----+\n |      |   1|\n |      |   2|\n |      |   3|\n |      +----+\n |  \n |  collect(self)\n |      Returns all the records as a list of :class:`Row`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.collect()\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |  \n |  corr(self, col1, col2, method=None)\n |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n |      Currently only supports the Pearson Correlation Coefficient.\n |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column\n |      col2 : str\n |          The name of the second column\n |      method : str, optional\n |          The correlation method. Currently only supports \"pearson\"\n |  \n |  count(self)\n |      Returns the number of rows in this :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.count()\n |      2\n |  \n |  cov(self, col1, col2)\n |      Calculate the sample covariance for the given columns, specified by their names, as a\n |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column\n |      col2 : str\n |          The name of the second column\n |  \n |  createGlobalTempView(self, name)\n |      Creates a global temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Examples\n |      --------\n |      >>> df.createGlobalTempView(\"people\")\n |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n |      >>> sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: u\"Temporary table 'people' already exists;\"\n |      >>> spark.catalog.dropGlobalTempView(\"people\")\n |  \n |  createOrReplaceGlobalTempView(self, name)\n |      Creates or replaces a global temporary view using the given name.\n |      \n |      The lifetime of this temporary view is tied to this Spark application.\n |      \n |      .. versionadded:: 2.2.0\n |      \n |      Examples\n |      --------\n |      >>> df.createOrReplaceGlobalTempView(\"people\")\n |      >>> df2 = df.filter(df.age > 3)\n |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n |      >>> sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      >>> spark.catalog.dropGlobalTempView(\"people\")\n |  \n |  createOrReplaceTempView(self, name)\n |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Examples\n |      --------\n |      >>> df.createOrReplaceTempView(\"people\")\n |      >>> df2 = df.filter(df.age > 3)\n |      >>> df2.createOrReplaceTempView(\"people\")\n |      >>> df3 = spark.sql(\"select * from people\")\n |      >>> sorted(df3.collect()) == sorted(df2.collect())\n |      True\n |      >>> spark.catalog.dropTempView(\"people\")\n |  \n |  createTempView(self, name)\n |      Creates a local temporary view with this :class:`DataFrame`.\n |      \n |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n |      that was used to create this :class:`DataFrame`.\n |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n |      catalog.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Examples\n |      --------\n |      >>> df.createTempView(\"people\")\n |      >>> df2 = spark.sql(\"select * from people\")\n |      >>> sorted(df.collect()) == sorted(df2.collect())\n |      True\n |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |      ...\n |      AnalysisException: u\"Temporary table 'people' already exists;\"\n |      >>> spark.catalog.dropTempView(\"people\")\n |  \n |  crossJoin(self, other)\n |      Returns the cartesian product with another :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`DataFrame`\n |          Right side of the cartesian product.\n |      \n |      Examples\n |      --------\n |      >>> df.select(\"age\", \"name\").collect()\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |      >>> df2.select(\"name\", \"height\").collect()\n |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n |  \n |  crosstab(self, col1, col2)\n |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n |      non-zero pair frequencies will be returned.\n |      The first column of each row will be the distinct values of `col1` and the column names\n |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n |      Pairs that have no occurrences will have zero as their counts.\n |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      col1 : str\n |          The name of the first column. Distinct items will make the first item of\n |          each row.\n |      col2 : str\n |          The name of the second column. Distinct items will make the column names\n |          of the :class:`DataFrame`.\n |  \n |  cube(self, *cols)\n |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n |      the specified columns, so we can run aggregations on them.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Examples\n |      --------\n |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n |      +-----+----+-----+\n |      | name| age|count|\n |      +-----+----+-----+\n |      | null|null|    2|\n |      | null|   2|    1|\n |      | null|   5|    1|\n |      |Alice|null|    1|\n |      |Alice|   2|    1|\n |      |  Bob|null|    1|\n |      |  Bob|   5|    1|\n |      +-----+----+-----+\n |  \n |  describe(self, *cols)\n |      Computes basic statistics for numeric and string columns.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      This include count, mean, stddev, min, and max. If no columns are\n |      given, this function computes statistics for all numerical or string columns.\n |      \n |      Notes\n |      -----\n |      This function is meant for exploratory data analysis, as we make no\n |      guarantee about the backward compatibility of the schema of the resulting\n |      :class:`DataFrame`.\n |      \n |      Use summary for expanded statistics and control over which statistics to compute.\n |      \n |      Examples\n |      --------\n |      >>> df.describe(['age']).show()\n |      +-------+------------------+\n |      |summary|               age|\n |      +-------+------------------+\n |      |  count|                 2|\n |      |   mean|               3.5|\n |      | stddev|2.1213203435596424|\n |      |    min|                 2|\n |      |    max|                 5|\n |      +-------+------------------+\n |      >>> df.describe().show()\n |      +-------+------------------+-----+\n |      |summary|               age| name|\n |      +-------+------------------+-----+\n |      |  count|                 2|    2|\n |      |   mean|               3.5| null|\n |      | stddev|2.1213203435596424| null|\n |      |    min|                 2|Alice|\n |      |    max|                 5|  Bob|\n |      +-------+------------------+-----+\n |      \n |      See Also\n |      --------\n |      DataFrame.summary\n |  \n |  display = df_display(df, *args, **kwargs)\n |      df.display() is an alias for display(df). Run help(display) for more information.\n |  \n |  distinct(self)\n |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.distinct().count()\n |      2\n |  \n |  drop(self, *cols)\n |      Returns a new :class:`DataFrame` that drops the specified column.\n |      This is a no-op if schema doesn't contain the given column name(s).\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      cols: str or :class:`Column`\n |          a name of the column, or the :class:`Column` to drop\n |      \n |      Examples\n |      --------\n |      >>> df.drop('age').collect()\n |      [Row(name='Alice'), Row(name='Bob')]\n |      \n |      >>> df.drop(df.age).collect()\n |      [Row(name='Alice'), Row(name='Bob')]\n |      \n |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n |      [Row(age=5, height=85, name='Bob')]\n |      \n |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n |      [Row(age=5, name='Bob', height=85)]\n |      \n |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n |      [Row(name='Bob')]\n |  \n |  dropDuplicates(self, subset=None)\n |      Return a new :class:`DataFrame` with duplicate rows removed,\n |      optionally only considering certain columns.\n |      \n |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n |      be and system will accordingly limit the state. In addition, too late data older than\n |      watermark will be dropped to avoid any possibility of duplicates.\n |      \n |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = sc.parallelize([ \\\n |      ...     Row(name='Alice', age=5, height=80), \\\n |      ...     Row(name='Alice', age=5, height=80), \\\n |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n |      >>> df.dropDuplicates().show()\n |      +-----+---+------+\n |      | name|age|height|\n |      +-----+---+------+\n |      |Alice|  5|    80|\n |      |Alice| 10|    80|\n |      +-----+---+------+\n |      \n |      >>> df.dropDuplicates(['name', 'height']).show()\n |      +-----+---+------+\n |      | name|age|height|\n |      +-----+---+------+\n |      |Alice|  5|    80|\n |      +-----+---+------+\n |  \n |  drop_duplicates = dropDuplicates(self, subset=None)\n |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n |      \n |      .. versionadded:: 1.4\n |  \n |  dropna(self, how='any', thresh=None, subset=None)\n |      Returns a new :class:`DataFrame` omitting rows with null values.\n |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      Parameters\n |      ----------\n |      how : str, optional\n |          'any' or 'all'.\n |          If 'any', drop a row if it contains any nulls.\n |          If 'all', drop a row only if all its values are null.\n |      thresh: int, optional\n |          default None\n |          If specified, drop rows that have less than `thresh` non-null values.\n |          This overwrites the `how` parameter.\n |      subset : str, tuple or list, optional\n |          optional list of column names to consider.\n |      \n |      Examples\n |      --------\n |      >>> df4.na.drop().show()\n |      +---+------+-----+\n |      |age|height| name|\n |      +---+------+-----+\n |      | 10|    80|Alice|\n |      +---+------+-----+\n |  \n |  exceptAll(self, other)\n |      Return a new :clas\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nme='Alice')]\n |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n |  \n |  sortWithinPartitions(self, *cols, **kwargs)\n |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      Parameters\n |      ----------\n |      cols : str, list or :class:`Column`, optional\n |          list of :class:`Column` or column names to sort by.\n |      \n |      Other Parameters\n |      ----------------\n |      ascending : bool or list, optional\n |          boolean or list of boolean (default ``True``).\n |          Sort ascending vs. descending. Specify list for multiple sort orders.\n |          If a list is specified, length of the list must equal length of the `cols`.\n |      \n |      Examples\n |      --------\n |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n |      +---+-----+\n |      |age| name|\n |      +---+-----+\n |      |  2|Alice|\n |      |  5|  Bob|\n |      +---+-----+\n |  \n |  subtract(self, other)\n |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n |      but not in another :class:`DataFrame`.\n |      \n |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n |      \n |      .. versionadded:: 1.3\n |  \n |  summary(self, *statistics)\n |      Computes specified statistics for numeric and string columns. Available statistics are:\n |      - count\n |      - mean\n |      - stddev\n |      - min\n |      - max\n |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n |      \n |      If no statistics are given, this function computes count, mean, stddev, min,\n |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Notes\n |      -----\n |      This function is meant for exploratory data analysis, as we make no\n |      guarantee about the backward compatibility of the schema of the resulting\n |      :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> df.summary().show()\n |      +-------+------------------+-----+\n |      |summary|               age| name|\n |      +-------+------------------+-----+\n |      |  count|                 2|    2|\n |      |   mean|               3.5| null|\n |      | stddev|2.1213203435596424| null|\n |      |    min|                 2|Alice|\n |      |    25%|                 2| null|\n |      |    50%|                 2| null|\n |      |    75%|                 5| null|\n |      |    max|                 5|  Bob|\n |      +-------+------------------+-----+\n |      \n |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n |      +-------+---+-----+\n |      |summary|age| name|\n |      +-------+---+-----+\n |      |  count|  2|    2|\n |      |    min|  2|Alice|\n |      |    25%|  2| null|\n |      |    75%|  5| null|\n |      |    max|  5|  Bob|\n |      +-------+---+-----+\n |      \n |      To do a summary for specific columns first select them:\n |      \n |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n |      +-------+---+----+\n |      |summary|age|name|\n |      +-------+---+----+\n |      |  count|  2|   2|\n |      +-------+---+----+\n |      \n |      See Also\n |      --------\n |      DataFrame.display\n |  \n |  tail(self, num)\n |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n |      \n |      Running tail requires moving data into the application's driver process, and doing so with\n |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Examples\n |      --------\n |      >>> df.tail(1)\n |      [Row(age=5, name='Bob')]\n |  \n |  take(self, num)\n |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.take(2)\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |  \n |  toDF(self, *cols)\n |      Returns a new :class:`DataFrame` that with new specified column names\n |      \n |      Parameters\n |      ----------\n |      cols : str\n |          new column names\n |      \n |      Examples\n |      --------\n |      >>> df.toDF('f1', 'f2').collect()\n |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n |  \n |  toJSON(self, use_unicode=True)\n |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n |      \n |      Each row is turned into a JSON document as one element in the returned RDD.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.toJSON().first()\n |      '{\"age\":2,\"name\":\"Alice\"}'\n |  \n |  toLocalIterator(self, prefetchPartitions=False)\n |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n |      The iterator will consume as much memory as the largest partition in this\n |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n |      partitions.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      prefetchPartitions : bool, optional\n |          If Spark should pre-fetch the next partition  before it is needed.\n |      \n |      Examples\n |      --------\n |      >>> list(df.toLocalIterator())\n |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n |  \n |  to_koalas(self, index_col=None)\n |      # Keep to_koalas for backward compatibility for now.\n |  \n |  to_pandas_on_spark(self, index_col=None)\n |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n |      \n |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n |      to pandas-on-Spark, it will lose the index information and the original index\n |      will be turned into a normal column.\n |      \n |      This is only available if Pandas is installed and available.\n |      \n |      Parameters\n |      ----------\n |      index_col: str or list of str, optional, default: None\n |          Index column of table in Spark.\n |      \n |      See Also\n |      --------\n |      pyspark.pandas.frame.DataFrame.to_spark\n |      \n |      Examples\n |      --------\n |      >>> df.show()  # doctest: +SKIP\n |      +----+----+\n |      |Col1|Col2|\n |      +----+----+\n |      |   a|   1|\n |      |   b|   2|\n |      |   c|   3|\n |      +----+----+\n |      \n |      >>> df.to_pandas_on_spark()  # doctest: +SKIP\n |        Col1  Col2\n |      0    a     1\n |      1    b     2\n |      2    c     3\n |      \n |      We can specify the index columns.\n |      \n |      >>> df.to_pandas_on_spark(index_col=\"Col1\"): # doctest: +SKIP\n |            Col2\n |      Col1\n |      a        1\n |      b        2\n |      c        3\n |  \n |  transform(self, func)\n |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          a function that takes and returns a :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.functions import col\n |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n |      >>> def cast_all_to_int(input_df):\n |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n |      >>> def sort_columns_asc(input_df):\n |      ...     return input_df.select(*sorted(input_df.columns))\n |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n |      +-----+---+\n |      |float|int|\n |      +-----+---+\n |      |    1|  1|\n |      |    2|  2|\n |      +-----+---+\n |  \n |  union(self, other)\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      .. versionadded:: 2.0\n |  \n |  unionAll(self, other)\n |      Return a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      Also as standard in SQL, this function resolves columns by position (not by name).\n |      \n |      .. versionadded:: 1.3\n |  \n |  unionByName(self, other, allowMissingColumns=False)\n |      Returns a new :class:`DataFrame` containing union of rows in this and another\n |      :class:`DataFrame`.\n |      \n |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Examples\n |      --------\n |      The difference between this function and :func:`union` is that this function\n |      resolves columns by name (not by position):\n |      \n |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n |      >>> df1.unionByName(df2).show()\n |      +----+----+----+\n |      |col0|col1|col2|\n |      +----+----+----+\n |      |   1|   2|   3|\n |      |   6|   4|   5|\n |      +----+----+----+\n |      \n |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n |      in the schema of the union result:\n |      \n |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n |      +----+----+----+----+\n |      |col0|col1|col2|col3|\n |      +----+----+----+----+\n |      |   1|   2|   3|null|\n |      |null|   4|   5|   6|\n |      +----+----+----+----+\n |      \n |      .. versionchanged:: 3.1.0\n |         Added optional argument `allowMissingColumns` to specify whether to allow\n |         missing columns.\n |  \n |  unpersist(self, blocking=False)\n |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n |  \n |  where = filter(self, condition)\n |      :func:`where` is an alias for :func:`filter`.\n |      \n |      .. versionadded:: 1.3\n |  \n |  withColumn(self, colName, col)\n |      Returns a new :class:`DataFrame` by adding a column or replacing the\n |      existing column that has the same name.\n |      \n |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n |      a column from some other :class:`DataFrame` will raise an error.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      colName : str\n |          string, name of the new column.\n |      col : :class:`Column`\n |          a :class:`Column` expression for the new column.\n |      \n |      Notes\n |      -----\n |      This method introduces a projection internally. Therefore, calling it multiple\n |      times, for instance, via loops in order to add multiple columns can generate big\n |      plans which can cause performance issues and even `StackOverflowException`.\n |      To avoid this, use :func:`select` with the multiple columns at once.\n |      \n |      Examples\n |      --------\n |      >>> df.withColumn('age2', df.age + 2).collect()\n |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n |  \n |  withColumnRenamed(self, existing, new)\n |      Returns a new :class:`DataFrame` by renaming an existing column.\n |      This is a no-op if schema doesn't contain the given column name.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      existing : str\n |          string, name of the existing column to rename.\n |      new : str\n |          string, new name of the column.\n |      \n |      Examples\n |      --------\n |      >>> df.withColumnRenamed('age', 'age2').collect()\n |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n |  \n |  withColumns(self, *colsMap)\n |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n |      existing columns that has the same names.\n |      \n |      The colsMap is a map of column name and column, the column must only refer to attributes\n |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n |      \n |      .. versionadded:: 3.3.0\n |         Added support for multiple columns adding\n |      \n |      Parameters\n |      ----------\n |      colsMap : dict\n |          a dict of column name and :class:`Column`. Currently, only single map is supported.\n |      \n |      Examples\n |      --------\n |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).collect()\n |      [Row(age=2, name='Alice', age2=4, age3=5), Row(age=5, name='Bob', age2=7, age3=8)]\n |  \n |  withMetadata(self, columnName, metadata)\n |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      columnName : str\n |          string, name of the existing column to update the metadata.\n |      metadata : dict\n |          dict, new metadata to be assigned to df.schema[columnName].metadata\n |      \n |      Examples\n |      --------\n |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n |      >>> df_meta.schema['age'].metadata\n |      {'foo': 'bar'}\n |  \n |  withWatermark(self, eventTime, delayThreshold)\n |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n |      in time before which we assume no more late data is going to arrive.\n |      \n |      Spark will use this watermark for several purposes:\n |        - To know when a given time window aggregation can be finalized and thus can be emitted\n |          when using output modes that do not allow updates.\n |      \n |        - To minimize the amount of state that we need to keep for on-going aggregations.\n |      \n |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n |      process records that arrive more than `delayThreshold` late.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Parameters\n |      ----------\n |      eventTime : str\n |          the name of the column that contains the event time of the row.\n |      delayThreshold : str\n |          the minimum delay to wait to data to arrive late, relative to the\n |          latest record that has been processed in the form of an interval\n |          (e.g. \"1 minute\" or \"5 hours\").\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      >>> from pyspark.sql.functions import timestamp_seconds\n |      >>> sdf.select(\n |      ...    'name',\n |      ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n |      DataFrame[name: string, time: timestamp]\n |  \n |  writeTo(self, table)\n |      Create a write configuration builder for v2 sources.\n |      \n |      This builder is used to configure and execute write operations.\n |      \n |      For example, to append or create or replace existing tables.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Examples\n |      --------\n |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n |      >>> df.writeTo(                              # doctest: +SKIP\n |      ...     \"catalog.db.table\"\n |      ... ).partitionedBy(\"col\").createOrReplace()\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  columns\n |      Returns all column names as a list.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.columns\n |      ['age', 'name']\n |  \n |  dtypes\n |      Returns all column names and their data types as a list.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.dtypes\n |      [('age', 'int'), ('name', 'string')]\n |  \n |  isStreaming\n |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n |      is a streaming source present.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |  \n |  na\n |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n |      \n |      .. versionadded:: 1.3.1\n |  \n |  rdd\n |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n |      \n |      .. versionadded:: 1.3\n |  \n |  schema\n |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.schema\n |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n |  \n |  stat\n |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n |      \n |      .. versionadded:: 1.4\n |  \n |  storageLevel\n |      Get the :class:`DataFrame`'s current storage level.\n |      \n |      .. versionadded:: 2.1.0\n |      \n |      Examples\n |      --------\n |      >>> df.storageLevel\n |      StorageLevel(False, False, False, False, 1)\n |      >>> df.cache().storageLevel\n |      StorageLevel(True, True, False, True, 1)\n |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n |      StorageLevel(True, False, False, False, 2)\n |  \n |  write\n |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameWriter`\n |  \n |  writeStream\n |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n |      storage.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      :class:`DataStreamWriter`\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n |  \n |  mapInPandas(self, func, schema)\n |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n |      function that takes and outputs a pandas DataFrame, and returns the result as a\n |      :class:`DataFrame`.\n |      \n |      The function should take an iterator of `pandas.DataFrame`\\s and return\n |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n |      Each `pandas.DataFrame` size can be controlled by\n |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n |          outputs an iterator of `pandas.DataFrame`\\s.\n |      schema : :class:`pyspark.sql.types.DataType` or str\n |          the return type of the `func` in PySpark. The value can be either a\n |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.functions import pandas_udf\n |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n |      >>> def filter_func(iterator):\n |      ...     for pdf in iterator:\n |      ...         yield pdf[pdf.id == 1]\n |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n |      +---+---+\n |      | id|age|\n |      +---+---+\n |      |  1| 21|\n |      +---+---+\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |      \n |      See Also\n |      --------\n |      pyspark.sql.functions.pandas_udf\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n |  \n |  toPandas(self)\n |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n |      \n |      This is only available if Pandas is installed and available.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting Pandas's :class:`DataFrame` is\n |      expected to be small, as all the data is loaded into the driver's memory.\n |      \n |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n |      \n |      Examples\n |      --------\n |      >>> df.toPandas()  # doctest: +SKIP\n |         age   name\n |      0    2  Alice\n |      1    5    Bob\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["users_df = spark.createDataFrame(pd.DataFrame(users)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"673ee6a4-118e-418e-9c1f-9dfe724682e5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|5.0|      null|        null|                null|       null|        NaN|         null|               null|\n|NaN|      null|        null|nbrewitt1@dailyma...|       null|        NaN|         null|               null|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n|5.0|      null|        null|                null|       null|        NaN|         null|               null|\n|NaN|      null|        null|nbrewitt1@dailyma...|       null|        NaN|         null|               null|\n|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Here are the sorting scenarios which we should be familiar with.\n# * Sort a data frame using ascending order by a specific column.\n# * Sort a data frame using descending order by a specific column.\n# * Dealing with nulls while sorting the data (having the null values at the beginning or at the end).\n# * Sort a data frame using multiple columns (composite sorting). We also need to be aware of how to sort the data in ascending order by first column and then descending order by second column as well as vice versa.\n# * We also need to make sure how to perform prioritized sorting. For example, let's say we want to get USA at the top and rest of the countries in ascending order by their respective names."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b81a8483-44de-40ff-8725-3ccd3575d3b4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\nusers = [\n    {\n        \"id\": 1,\n        \"first_name\": \"Corrie\",\n        \"last_name\": \"Van den Oord\",\n        \"email\": \"cvandenoord0@etsy.com\",\n        \"phone_numbers\": Row(mobile=\"+1 234 567 8901\", home=\"+1 234 567 8911\"),\n        \"courses\": [1, 2],\n        \"is_customer\": True,\n        \"amount_paid\": 1000.55,\n        \"customer_from\": datetime.date(2021, 1, 15),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n    },\n    {\n        \"id\": 2,\n        \"first_name\": \"Nikolaus\",\n        \"last_name\": \"Brewitt\",\n        \"email\": \"nbrewitt1@dailymail.co.uk\",\n        \"phone_numbers\":  Row(mobile=\"+1 234 567 8923\", home=\"1 234 567 8934\"),\n        \"courses\": [3],\n        \"is_customer\": True,\n        \"amount_paid\": 900.0,\n        \"customer_from\": datetime.date(2021, 2, 14),\n        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n    },\n    {\n        \"id\": 3,\n        \"first_name\": \"Orelie\",\n        \"last_name\": \"Penney\",\n        \"email\": \"openney2@vistaprint.com\",\n        \"phone_numbers\": Row(mobile=\"+1 714 512 9752\", home=\"+1 714 512 6601\"),\n        \"courses\": [2, 4],\n        \"is_customer\": True,\n        \"amount_paid\": 850.55,\n        \"customer_from\": datetime.date(2021, 1, 21),\n        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n    },\n    {\n        \"id\": 4,\n        \"first_name\": \"Ashby\",\n        \"last_name\": \"Maddocks\",\n        \"email\": \"amaddocks3@home.pl\",\n        \"phone_numbers\": Row(mobile=None, home=None),\n        \"courses\": [],\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n    },\n    {\n        \"id\": 5,\n        \"first_name\": \"Kurt\",\n        \"last_name\": \"Rome\",\n        \"email\": \"krome4@shutterfly.com\",\n        \"phone_numbers\": Row(mobile=\"+1 817 934 7142\", home=None),\n        \"courses\": [],\n        \"is_customer\": False,\n        \"amount_paid\": None,\n        \"customer_from\": None,\n        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n    }\n]\nimport pandas as pd\nspark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)\nusers_df = spark.createDataFrame(pd.DataFrame(users))\nusers_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3cacc1b-9a79-4563-aae6-c1d01f5ee5b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## sort data in ascending order , sort data in descending order \nfrom pyspark.sql.functions import col\nusers_df.show()\nusers_df.sort(col('first_name')).show()\n## note : this will sort data in ascending order \n## note : this will sort data in descending order\n\n# Questions : * Sort the **data** in ascending order by **number of enrolled courses**.\n\n##\nusers_df.select('id', 'courses').withColumn('no_of_courses', size('courses')).sort(size('courses')).show()\n## sort(,ascending=False)\nusers_df.sort('first_name', ascending=False).show()\n\n###########\nusers_df.sort(users_df['first_name'].desc()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce19cc58-843e-4e49-8309-b9e3774ab5ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+-------+-------------+\n| id|courses|no_of_courses|\n+---+-------+-------------+\n|  4|     []|            0|\n|  5|     []|            0|\n|  2|    [3]|            1|\n|  3| [2, 4]|            2|\n|  1| [1, 2]|            2|\n+---+-------+-------------+\n\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+-------+-------------+\n| id|courses|no_of_courses|\n+---+-------+-------------+\n|  4|     []|            0|\n|  5|     []|            0|\n|  2|    [3]|            1|\n|  3| [2, 4]|            2|\n|  1| [1, 2]|            2|\n+---+-------+-------------+\n\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## note: * Sort the data in ascending order by **customer_from** with null values coming at the end.\n\n\nusers_df.select('id', 'customer_from').orderBy(users_df['customer_from'].asc_nulls_last()).show()\n\n## asc_nulls_last ==== give null at last \n###desc_nulls_first == give null at fist \nusers_df.select('id', 'customer_from'). orderBy(users_df['customer_from'].desc_nulls_first()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3faec231-1141-428c-a56f-a352ec7b1095","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  1|   2021-01-15|\n|  3|   2021-01-21|\n|  2|   2021-02-14|\n|  4|         null|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  5|         null|\n|  4|         null|\n|  2|   2021-02-14|\n|  3|   2021-01-21|\n|  1|   2021-01-15|\n+---+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  1|   2021-01-15|\n|  3|   2021-01-21|\n|  2|   2021-02-14|\n|  4|         null|\n|  5|         null|\n+---+-------------+\n\n+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  5|         null|\n|  4|         null|\n|  2|   2021-02-14|\n|  3|   2021-01-21|\n|  1|   2021-01-15|\n+---+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["### sort by and orderBy \n# The difference between \"order by\" and \"sort by\" is that the former guarantees total order in the output while the latter only guarantees ordering of the rows within a reducer."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9d07a56-8f5c-46fb-a593-28f70545ee11","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## composite sorting"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"584b0436-6479-4de5-9cd9-3b8f888ab98d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["courses = [{'course_id': 1,\n  'course_name': '2020 Complete Python Bootcamp: From Zero to Hero in Python',\n  'suitable_for': 'Beginner',\n  'enrollment': 1100093,\n  'stars': 4.6,\n  'number_of_ratings': 318066},\n {'course_id': 4,\n  'course_name': 'Angular - The Complete Guide (2020 Edition)',\n  'suitable_for': 'Intermediate',\n  'enrollment': 422557,\n  'stars': 4.6,\n  'number_of_ratings': 129984},\n {'course_id': 12,\n  'course_name': 'Automate the Boring Stuff with Python Programming',\n  'suitable_for': 'Advanced',\n  'enrollment': 692617,\n  'stars': 4.6,\n  'number_of_ratings': 70508},\n {'course_id': 10,\n  'course_name': 'Complete C# Unity Game Developer 2D',\n  'suitable_for': 'Advanced',\n  'enrollment': 364934,\n  'stars': 4.6,\n  'number_of_ratings': 78989},\n {'course_id': 5,\n  'course_name': 'Java Programming Masterclass for Software Developers',\n  'suitable_for': 'Advanced',\n  'enrollment': 502572,\n  'stars': 4.6,\n  'number_of_ratings': 123798},\n {'course_id': 15,\n  'course_name': 'Learn Python Programming Masterclass',\n  'suitable_for': 'Advanced',\n  'enrollment': 240790,\n  'stars': 4.5,\n  'number_of_ratings': 58677},\n {'course_id': 3,\n  'course_name': 'Machine Learning A-Z™: Hands-On Python & R In Data Science',\n  'suitable_for': 'Intermediate',\n  'enrollment': 692812,\n  'stars': 4.5,\n  'number_of_ratings': 132228},\n {'course_id': 14,\n  'course_name': 'Modern React with Redux [2020 Update]',\n  'suitable_for': 'Intermediate',\n  'enrollment': 203214,\n  'stars': 4.7,\n  'number_of_ratings': 60835},\n {'course_id': 8,\n  'course_name': 'Python for Data Science and Machine Learning Bootcamp',\n  'suitable_for': 'Intermediate',\n  'enrollment': 387789,\n  'stars': 4.6,\n  'number_of_ratings': 87403},\n {'course_id': 6,\n  'course_name': 'React - The Complete Guide (incl Hooks, React Router, Redux)',\n  'suitable_for': 'Intermediate',\n  'enrollment': 304670,\n  'stars': 4.6,\n  'number_of_ratings': 90964},\n {'course_id': 18,\n  'course_name': 'Selenium WebDriver with Java -Basics to Advanced+Frameworks',\n  'suitable_for': 'Advanced',\n  'enrollment': 148562,\n  'stars': 4.6,\n  'number_of_ratings': 49947},\n {'course_id': 21,\n  'course_name': 'Spring & Hibernate for Beginners (includes Spring Boot)',\n  'suitable_for': 'Advanced',\n  'enrollment': 177053,\n  'stars': 4.6,\n  'number_of_ratings': 45329},\n {'course_id': 7,\n  'course_name': 'The Complete 2020 Web Development Bootcamp',\n  'suitable_for': 'Beginner',\n  'enrollment': 270656,\n  'stars': 4.7,\n  'number_of_ratings': 88098},\n {'course_id': 9,\n  'course_name': 'The Complete JavaScript Course 2020: Build Real Projects!',\n  'suitable_for': 'Intermediate',\n  'enrollment': 347979,\n  'stars': 4.6,\n  'number_of_ratings': 83521},\n {'course_id': 16,\n  'course_name': 'The Complete Node.js Developer Course (3rd Edition)',\n  'suitable_for': 'Advanced',\n  'enrollment': 202922,\n  'stars': 4.7,\n  'number_of_ratings': 50885},\n {'course_id': 13,\n  'course_name': 'The Complete Web Developer Course 2.0',\n  'suitable_for': 'Intermediate',\n  'enrollment': 273598,\n  'stars': 4.5,\n  'number_of_ratings': 63175},\n {'course_id': 11,\n  'course_name': 'The Data Science Course 2020: Complete Data Science Bootcamp',\n  'suitable_for': 'Beginner',\n  'enrollment': 325047,\n  'stars': 4.5,\n  'number_of_ratings': 76907},\n {'course_id': 20,\n  'course_name': 'The Ultimate MySQL Bootcamp: Go from SQL Beginner to Expert',\n  'suitable_for': 'Beginner',\n  'enrollment': 203366,\n  'stars': 4.6,\n  'number_of_ratings': 45382},\n {'course_id': 2,\n  'course_name': 'The Web Developer Bootcamp',\n  'suitable_for': 'Beginner',\n  'enrollment': 596726,\n  'stars': 4.6,\n  'number_of_ratings': 182997},\n {'course_id': 19,\n  'course_name': 'Unreal Engine C++ Developer: Learn C++ and Make Video Games',\n  'suitable_for': 'Advanced',\n  'enrollment': 229005,\n  'stars': 4.5,\n  'number_of_ratings': 45860},\n {'course_id': 17,\n  'course_name': 'iOS 13 & Swift 5 - The Complete iOS App Development Bootcamp',\n  'suitable_for': 'Advanced',\n  'enrollment': 179598,\n  'stars': 4.8,\n  'number_of_ratings': 49972}]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15cada83-b39c-412c-aec0-6c271c472d10","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\ncourses_df = spark.createDataFrame([Row(**course) for course in courses])\ncourses_df.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6fdc620b-d5e8-482b-87ef-a8a8ebfa9eb0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+--------------------+------------+----------+-----+-----------------+\n|course_id|         course_name|suitable_for|enrollment|stars|number_of_ratings|\n+---------+--------------------+------------+----------+-----+-----------------+\n|        1|2020 Complete Pyt...|    Beginner|   1100093|  4.6|           318066|\n|        4|Angular - The Com...|Intermediate|    422557|  4.6|           129984|\n+---------+--------------------+------------+----------+-----+-----------------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------------------+------------+----------+-----+-----------------+\n|course_id|         course_name|suitable_for|enrollment|stars|number_of_ratings|\n+---------+--------------------+------------+----------+-----+-----------------+\n|        1|2020 Complete Pyt...|    Beginner|   1100093|  4.6|           318066|\n|        4|Angular - The Com...|Intermediate|    422557|  4.6|           129984|\n+---------+--------------------+------------+----------+-----+-----------------+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["courses_df.dtypes\ncourses_df.sort(courses_df['suitable_for'], courses_df['number_of_ratings'].desc()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d5e1201-9a2d-48f1-9482-cd7e207c1959","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+--------------------+------------+----------+-----+-----------------+\n|course_id|         course_name|suitable_for|enrollment|stars|number_of_ratings|\n+---------+--------------------+------------+----------+-----+-----------------+\n|        5|Java Programming ...|    Advanced|    502572|  4.6|           123798|\n|       10|Complete C# Unity...|    Advanced|    364934|  4.6|            78989|\n|       12|Automate the Bori...|    Advanced|    692617|  4.6|            70508|\n|       15|Learn Python Prog...|    Advanced|    240790|  4.5|            58677|\n|       16|The Complete Node...|    Advanced|    202922|  4.7|            50885|\n|       17|iOS 13 & Swift 5 ...|    Advanced|    179598|  4.8|            49972|\n|       18|Selenium WebDrive...|    Advanced|    148562|  4.6|            49947|\n|       19|Unreal Engine C++...|    Advanced|    229005|  4.5|            45860|\n|       21|Spring & Hibernat...|    Advanced|    177053|  4.6|            45329|\n|        1|2020 Complete Pyt...|    Beginner|   1100093|  4.6|           318066|\n|        2|The Web Developer...|    Beginner|    596726|  4.6|           182997|\n|        7|The Complete 2020...|    Beginner|    270656|  4.7|            88098|\n|       11|The Data Science ...|    Beginner|    325047|  4.5|            76907|\n|       20|The Ultimate MySQ...|    Beginner|    203366|  4.6|            45382|\n|        3|Machine Learning ...|Intermediate|    692812|  4.5|           132228|\n|        4|Angular - The Com...|Intermediate|    422557|  4.6|           129984|\n|        6|React - The Compl...|Intermediate|    304670|  4.6|            90964|\n|        8|Python for Data S...|Intermediate|    387789|  4.6|            87403|\n|        9|The Complete Java...|Intermediate|    347979|  4.6|            83521|\n|       13|The Complete Web ...|Intermediate|    273598|  4.5|            63175|\n+---------+--------------------+------------+----------+-----+-----------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------------------+------------+----------+-----+-----------------+\n|course_id|         course_name|suitable_for|enrollment|stars|number_of_ratings|\n+---------+--------------------+------------+----------+-----+-----------------+\n|        5|Java Programming ...|    Advanced|    502572|  4.6|           123798|\n|       10|Complete C# Unity...|    Advanced|    364934|  4.6|            78989|\n|       12|Automate the Bori...|    Advanced|    692617|  4.6|            70508|\n|       15|Learn Python Prog...|    Advanced|    240790|  4.5|            58677|\n|       16|The Complete Node...|    Advanced|    202922|  4.7|            50885|\n|       17|iOS 13 & Swift 5 ...|    Advanced|    179598|  4.8|            49972|\n|       18|Selenium WebDrive...|    Advanced|    148562|  4.6|            49947|\n|       19|Unreal Engine C++...|    Advanced|    229005|  4.5|            45860|\n|       21|Spring & Hibernat...|    Advanced|    177053|  4.6|            45329|\n|        1|2020 Complete Pyt...|    Beginner|   1100093|  4.6|           318066|\n|        2|The Web Developer...|    Beginner|    596726|  4.6|           182997|\n|        7|The Complete 2020...|    Beginner|    270656|  4.7|            88098|\n|       11|The Data Science ...|    Beginner|    325047|  4.5|            76907|\n|       20|The Ultimate MySQ...|    Beginner|    203366|  4.6|            45382|\n|        3|Machine Learning ...|Intermediate|    692812|  4.5|           132228|\n|        4|Angular - The Com...|Intermediate|    422557|  4.6|           129984|\n|        6|React - The Compl...|Intermediate|    304670|  4.6|            90964|\n|        8|Python for Data S...|Intermediate|    387789|  4.6|            87403|\n|        9|The Complete Java...|Intermediate|    347979|  4.6|            83521|\n|       13|The Complete Web ...|Intermediate|    273598|  4.5|            63175|\n+---------+--------------------+------------+----------+-----+-----------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## prioterizing the sorting of DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d57f805-ab93-4c23-92b8-796871fb34d7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["courses = [{'course_id': 1,\n  'course_name': '2020 Complete Python Bootcamp: From Zero to Hero in Python',\n  'suitable_for': 'Beginner',\n  'enrollment': 1100093,\n  'stars': 4.6,\n  'number_of_ratings': 318066},\n {'course_id': 4,\n  'course_name': 'Angular - The Complete Guide (2020 Edition)',\n  'suitable_for': 'Intermediate',\n  'enrollment': 422557,\n  'stars': 4.6,\n  'number_of_ratings': 129984},\n {'course_id': 12,\n  'course_name': 'Automate the Boring Stuff with Python Programming',\n  'suitable_for': 'Advanced',\n  'enrollment': 692617,\n  'stars': 4.6,\n  'number_of_ratings': 70508},\n {'course_id': 10,\n  'course_name': 'Complete C# Unity Game Developer 2D',\n  'suitable_for': 'Advanced',\n  'enrollment': 364934,\n  'stars': 4.6,\n  'number_of_ratings': 78989},\n {'course_id': 5,\n  'course_name': 'Java Programming Masterclass for Software Developers',\n  'suitable_for': 'Advanced',\n  'enrollment': 502572,\n  'stars': 4.6,\n  'number_of_ratings': 123798},\n {'course_id': 15,\n  'course_name': 'Learn Python Programming Masterclass',\n  'suitable_for': 'Advanced',\n  'enrollment': 240790,\n  'stars': 4.5,\n  'number_of_ratings': 58677},\n {'course_id': 3,\n  'course_name': 'Machine Learning A-Z™: Hands-On Python & R In Data Science',\n  'suitable_for': 'Intermediate',\n  'enrollment': 692812,\n  'stars': 4.5,\n  'number_of_ratings': 132228},\n {'course_id': 14,\n  'course_name': 'Modern React with Redux [2020 Update]',\n  'suitable_for': 'Intermediate',\n  'enrollment': 203214,\n  'stars': 4.7,\n  'number_of_ratings': 60835},\n {'course_id': 8,\n  'course_name': 'Python for Data Science and Machine Learning Bootcamp',\n  'suitable_for': 'Intermediate',\n  'enrollment': 387789,\n  'stars': 4.6,\n  'number_of_ratings': 87403},\n {'course_id': 6,\n  'course_name': 'React - The Complete Guide (incl Hooks, React Router, Redux)',\n  'suitable_for': 'Intermediate',\n  'enrollment': 304670,\n  'stars': 4.6,\n  'number_of_ratings': 90964},\n {'course_id': 18,\n  'course_name': 'Selenium WebDriver with Java -Basics to Advanced+Frameworks',\n  'suitable_for': 'Advanced',\n  'enrollment': 148562,\n  'stars': 4.6,\n  'number_of_ratings': 49947},\n {'course_id': 21,\n  'course_name': 'Spring & Hibernate for Beginners (includes Spring Boot)',\n  'suitable_for': 'Advanced',\n  'enrollment': 177053,\n  'stars': 4.6,\n  'number_of_ratings': 45329},\n {'course_id': 7,\n  'course_name': 'The Complete 2020 Web Development Bootcamp',\n  'suitable_for': 'Beginner',\n  'enrollment': 270656,\n  'stars': 4.7,\n  'number_of_ratings': 88098},\n {'course_id': 9,\n  'course_name': 'The Complete JavaScript Course 2020: Build Real Projects!',\n  'suitable_for': 'Intermediate',\n  'enrollment': 347979,\n  'stars': 4.6,\n  'number_of_ratings': 83521},\n {'course_id': 16,\n  'course_name': 'The Complete Node.js Developer Course (3rd Edition)',\n  'suitable_for': 'Advanced',\n  'enrollment': 202922,\n  'stars': 4.7,\n  'number_of_ratings': 50885},\n {'course_id': 13,\n  'course_name': 'The Complete Web Developer Course 2.0',\n  'suitable_for': 'Intermediate',\n  'enrollment': 273598,\n  'stars': 4.5,\n  'number_of_ratings': 63175},\n {'course_id': 11,\n  'course_name': 'The Data Science Course 2020: Complete Data Science Bootcamp',\n  'suitable_for': 'Beginner',\n  'enrollment': 325047,\n  'stars': 4.5,\n  'number_of_ratings': 76907},\n {'course_id': 20,\n  'course_name': 'The Ultimate MySQL Bootcamp: Go from SQL Beginner to Expert',\n  'suitable_for': 'Beginner',\n  'enrollment': 203366,\n  'stars': 4.6,\n  'number_of_ratings': 45382},\n {'course_id': 2,\n  'course_name': 'The Web Developer Bootcamp',\n  'suitable_for': 'Beginner',\n  'enrollment': 596726,\n  'stars': 4.6,\n  'number_of_ratings': 182997},\n {'course_id': 19,\n  'course_name': 'Unreal Engine C++ Developer: Learn C++ and Make Video Games',\n  'suitable_for': 'Advanced',\n  'enrollment': 229005,\n  'stars': 4.5,\n  'number_of_ratings': 45860},\n {'course_id': 17,\n  'course_name': 'iOS 13 & Swift 5 - The Complete iOS App Development Bootcamp',\n  'suitable_for': 'Advanced',\n  'enrollment': 179598,\n  'stars': 4.8,\n  'number_of_ratings': 49972}]\nfrom pyspark.sql import Row\ncourses_df = spark.createDataFrame([Row(**course) for course in courses])\nfrom pyspark.sql.functions import col, when"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0dbeb795-5fba-4610-aa19-7cbe55248c1b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["course_level = when(col('suitable_for') == 'Beginner', 0).otherwise(when(col('suitable_for') == 'Intermediate', 1).otherwise(2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"515fcf2d-3924-4eba-bf5c-3965abfea207","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Using expr and then case and when\n\nfrom pyspark.sql.functions import expr\n\ncourse_level = expr(\"\"\"\n    CASE \n        WHEN suitable_for = 'Beginner'\n            THEN 0\n        WHEN suitable_for = 'Intermediate'\n            THEN 1\n        ELSE 2\n    END\n\"\"\")\n\ncourses_df. \\\n    sort(course_level, col('number_of_ratings').desc()). \\\n    show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12277a29-34f1-4679-b03f-5cc5c13f9646","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+--------------------+------------+----------+-----+-----------------+\n|course_id|         course_name|suitable_for|enrollment|stars|number_of_ratings|\n+---------+--------------------+------------+----------+-----+-----------------+\n|        1|2020 Complete Pyt...|    Beginner|   1100093|  4.6|           318066|\n|        2|The Web Developer...|    Beginner|    596726|  4.6|           182997|\n|        7|The Complete 2020...|    Beginner|    270656|  4.7|            88098|\n|       11|The Data Science ...|    Beginner|    325047|  4.5|            76907|\n|       20|The Ultimate MySQ...|    Beginner|    203366|  4.6|            45382|\n|        3|Machine Learning ...|Intermediate|    692812|  4.5|           132228|\n|        4|Angular - The Com...|Intermediate|    422557|  4.6|           129984|\n|        6|React - The Compl...|Intermediate|    304670|  4.6|            90964|\n|        8|Python for Data S...|Intermediate|    387789|  4.6|            87403|\n|        9|The Complete Java...|Intermediate|    347979|  4.6|            83521|\n|       13|The Complete Web ...|Intermediate|    273598|  4.5|            63175|\n|       14|Modern React with...|Intermediate|    203214|  4.7|            60835|\n|        5|Java Programming ...|    Advanced|    502572|  4.6|           123798|\n|       10|Complete C# Unity...|    Advanced|    364934|  4.6|            78989|\n|       12|Automate the Bori...|    Advanced|    692617|  4.6|            70508|\n|       15|Learn Python Prog...|    Advanced|    240790|  4.5|            58677|\n|       16|The Complete Node...|    Advanced|    202922|  4.7|            50885|\n|       17|iOS 13 & Swift 5 ...|    Advanced|    179598|  4.8|            49972|\n|       18|Selenium WebDrive...|    Advanced|    148562|  4.6|            49947|\n|       19|Unreal Engine C++...|    Advanced|    229005|  4.5|            45860|\n+---------+--------------------+------------+----------+-----+-----------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------------------+------------+----------+-----+-----------------+\n|course_id|         course_name|suitable_for|enrollment|stars|number_of_ratings|\n+---------+--------------------+------------+----------+-----+-----------------+\n|        1|2020 Complete Pyt...|    Beginner|   1100093|  4.6|           318066|\n|        2|The Web Developer...|    Beginner|    596726|  4.6|           182997|\n|        7|The Complete 2020...|    Beginner|    270656|  4.7|            88098|\n|       11|The Data Science ...|    Beginner|    325047|  4.5|            76907|\n|       20|The Ultimate MySQ...|    Beginner|    203366|  4.6|            45382|\n|        3|Machine Learning ...|Intermediate|    692812|  4.5|           132228|\n|        4|Angular - The Com...|Intermediate|    422557|  4.6|           129984|\n|        6|React - The Compl...|Intermediate|    304670|  4.6|            90964|\n|        8|Python for Data S...|Intermediate|    387789|  4.6|            87403|\n|        9|The Complete Java...|Intermediate|    347979|  4.6|            83521|\n|       13|The Complete Web ...|Intermediate|    273598|  4.5|            63175|\n|       14|Modern React with...|Intermediate|    203214|  4.7|            60835|\n|        5|Java Programming ...|    Advanced|    502572|  4.6|           123798|\n|       10|Complete C# Unity...|    Advanced|    364934|  4.6|            78989|\n|       12|Automate the Bori...|    Advanced|    692617|  4.6|            70508|\n|       15|Learn Python Prog...|    Advanced|    240790|  4.5|            58677|\n|       16|The Complete Node...|    Advanced|    202922|  4.7|            50885|\n|       17|iOS 13 & Swift 5 ...|    Advanced|    179598|  4.8|            49972|\n|       18|Selenium WebDrive...|    Advanced|    148562|  4.6|            49947|\n|       19|Unreal Engine C++...|    Advanced|    229005|  4.5|            45860|\n+---------+--------------------+------------+----------+-----+-----------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"data_bricks_associate_developer_exam_basic4.ipynb","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":402239863734888}},"nbformat":4,"nbformat_minor":0}
