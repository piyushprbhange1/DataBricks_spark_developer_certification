{"cells":[{"cell_type":"markdown","source":["# Processing Column Data\n\nAs part of this module we will explore the functions available under `pyspark.sql.functions` to derive new values from existing column values with in a Data Frame.\n\n* Pre-defined Functions\n* Create Dummy Data Frame\n* Categories of Functions\n* Special Functions - col and lit\n* String Manipulation Functions - 1\n* String Manipulation Functions - 2\n* Date and Time Overview\n* Date and Time Arithmetic\n* Date and Time - trunc and date_trunc\n* Date and Time - Extracting Information\n* Dealing with Unix Timestamp\n* Example - Word Count\n* Conclusion\n\n%md\n## Pre-defined Functions\n\nWe typically process data in the columns using functions in `pyspark.sql.functions`. Let us understand details about these functions in detail as part of this module.\n\n%md\n* Let us recap about Functions or APIs to process Data Frames.\n * Projection - `select` or `withColumn` or `drop` or `selectExpr`\n * Filtering - `filter` or `where`\n * Grouping data by key and perform aggregations - `groupBy`\n * Sorting data - `sort` or `orderBy` \n* We can pass column names or literals or expressions to all the Data Frame APIs.\n* Expressions include arithmetic operations, transformations using functions from `pyspark.sql.functions`.\n* There are approximately 300 functions under `pyspark.sql.functions`.\n* We will talk about some of the important functions used for String Manipulation, Date Manipulation etc.\n\n\n* String Manipulation Functions\n  * Case Conversion - `lower`,  `upper`\n  * Getting Length -  `length`\n  * Extracting substrings - `substring`, `split`\n  * Trimming - `trim`, `ltrim`, `rtrim`\n  * Padding - `lpad`, `rpad`\n  * Concatenating string - `concat`, `concat_ws`\n* Date Manipulation Functions\n  * Getting current date and time - `current_date`, `current_timestamp`\n  * Date Arithmetic - `date_add`, `date_sub`, `datediff`, `months_between`, `add_months`, `next_day`\n  * Beginning and Ending Date or Time - `last_day`, `trunc`, `date_trunc`\n  * Formatting Date - `date_format`\n  * Extracting Information - `dayofyear`, `dayofmonth`, `dayofweek`, `year`, `month`\n* Aggregate Functions\n  * `count`, `countDistinct`\n  * `sum`, `avg`\n  * `min`, `max`\n* Other Functions - We will explore depending on the use cases.\n  * `CASE` and `WHEN`\n  * `CAST` for type casting\n  * Functions to manage special types such as `ARRAY`, `MAP`, `STRUCT` type columns\n  * Many others"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3910a163-2048-4d87-8875-6691f07d8a05","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Reading data \n# note: the public retail_db is \n\norders = spark.read.csv(\n    '/FileStore/tables/orders.csv',\n    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n)\n\n# note: /FileStore/tables/orders.csv nee to uploaded at dbfs \norders.show(12)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72b64a6d-3949-41ab-800f-41fe6a162c07","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------------------+-----------------+---------------+\n|order_id|          order_date|order_customer_id|   order_status|\n+--------+--------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n+--------+--------------------+-----------------+---------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------------------+-----------------+---------------+\n|order_id|          order_date|order_customer_id|   order_status|\n+--------+--------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n+--------+--------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Importing functions\n\nfrom pyspark.sql.functions import date_format,grouping\n# Function as part of projections\n\nprint(orders.select('*', date_format('order_date', 'yyyyMM').alias('order_month')).show(2))\n\nprint(orders.withColumn('order_month', date_format('order_date', 'yyyyMM')).show(2))\n\n#  note : withColumn will add the  column to original dataFrame  ,\n#  syntax (\"<NewColumnName>\",'old ColumnName')\n# Function as part of groupBy\nprint(orders.groupBy(date_format('order_date', 'yyyyMM').alias('order_month')).count().show(2))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6628117-0672-4fa3-918a-b57746072563","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------------------+-----------------+---------------+-----------+\n|order_id|          order_date|order_customer_id|   order_status|order_month|\n+--------+--------------------+-----------------+---------------+-----------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n+--------+--------------------+-----------------+---------------+-----------+\nonly showing top 2 rows\n\nNone\n+--------+--------------------+-----------------+---------------+-----------+\n|order_id|          order_date|order_customer_id|   order_status|order_month|\n+--------+--------------------+-----------------+---------------+-----------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n+--------+--------------------+-----------------+---------------+-----------+\nonly showing top 2 rows\n\nNone\n+-----------+-----+\n|order_month|count|\n+-----------+-----+\n|     201401| 5908|\n|     201405| 5467|\n+-----------+-----+\nonly showing top 2 rows\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------------------+-----------------+---------------+-----------+\n|order_id|          order_date|order_customer_id|   order_status|order_month|\n+--------+--------------------+-----------------+---------------+-----------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n+--------+--------------------+-----------------+---------------+-----------+\nonly showing top 2 rows\n\nNone\n+--------+--------------------+-----------------+---------------+-----------+\n|order_id|          order_date|order_customer_id|   order_status|order_month|\n+--------+--------------------+-----------------+---------------+-----------+\n|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n+--------+--------------------+-----------------+---------------+-----------+\nonly showing top 2 rows\n\nNone\n+-----------+-----+\n|order_month|count|\n+-----------+-----+\n|     201401| 5908|\n|     201405| 5467|\n+-----------+-----+\nonly showing top 2 rows\n\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Create Dummy Data Frame\n# Let us go ahead and create data frame using dummy data to explore Spark functions.\nl = [('X', )]\nfrom pyspark.sql.functions import current_date\n# Oracle dual (view)\n# dual - dummy CHAR(1)\n# \"X\" - One record\ndf = spark.createDataFrame(l, \"dummy STRING\")\ndf.printSchema()\ndf.show(1)\ndf.select(current_date()).show(1)\n\n#  note : pyspark.sql. functions ie s \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa5758c5-15ef-41c2-9ac6-c5c4f790c445","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dummy: string (nullable = true)\n\n+-----+\n|dummy|\n+-----+\n|    X|\n+-----+\n\n+--------------+\n|current_date()|\n+--------------+\n|    2022-12-12|\n+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dummy: string (nullable = true)\n\n+-----+\n|dummy|\n+-----+\n|    X|\n+-----+\n\n+--------------+\n|current_date()|\n+--------------+\n|    2022-12-12|\n+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["employees = [\n    (1, \"Scott\", \"Tiger\", 1000.0, \n      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n    ),\n     (2, \"Henry\", \"Ford\", 1250.0, \n      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n     ),\n     (3, \"Nick\", \"Junior\", 750.0, \n      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n     ),\n     (4, \"Bill\", \"Gomes\", 1500.0, \n      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n     )\n]\n\nemployeesDF = spark. \\\n    createDataFrame(employees,\n                    schema=\"\"\"employee_id INT, first_name STRING, \n                    last_name STRING, salary FLOAT, nationality STRING,\n                    phone_number STRING, ssn STRING\"\"\"\n                   )\n\nemployeesDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c38ba7be-2220-4102-9a90-0beb7f47c7aa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|nationality   |phone_number    |ssn        |\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890 |123 45 6789|\n|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111|222 33 4444|\n|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|nationality   |phone_number    |ssn        |\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890 |123 45 6789|\n|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111|222 33 4444|\n|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import date_format, col, lit, concat, concat_ws\n\n#  note: Let us understand special functions such as col and lit. These functions are typically used to convert the strings to column type.\n# * If there are no transformations on any column in any function then we should be able to pass all column names as strings.\n# * If not we need to pass all columns as type column by using col function.\n# * If we want to apply transformations using some of the functions then passing column names as strings will not suffice. We have to pass them as column type."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14e5e131-d16e-417f-b4d2-85af504a307e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n                     ),\n                     (2, \"Henry\", \"Ford\", 1250.0, \n                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n                     ),\n                     (3, \"Nick\", \"Junior\", 750.0, \n                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n                     ),\n                     (4, \"Bill\", \"Gomes\", 1500.0, \n                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n                     )\n                ]\n\n\nemployeesDF = spark. \\\n    createDataFrame(employees,\n                    schema=\"\"\"employee_id INT, first_name STRING, \n                    last_name STRING, salary FLOAT, nationality STRING,\n                    phone_number STRING, ssn STRING\"\"\"\n                   )\n\nfrom pyspark.sql.functions import col, upper\nemployeesDF.groupBy(upper(col(\"nationality\"))).count().show()\n\n## here we are "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e428435b-7eb3-4588-a982-8cefb85990be","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------+-----+\n|upper(nationality)|count|\n+------------------+-----+\n|     UNITED STATES|    1|\n|             INDIA|    1|\n|    UNITED KINGDOM|    1|\n|         AUSTRALIA|    1|\n+------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------+-----+\n|upper(nationality)|count|\n+------------------+-----+\n|     UNITED STATES|    1|\n|             INDIA|    1|\n|    UNITED KINGDOM|    1|\n|         AUSTRALIA|    1|\n+------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Alternative - we can also refer column names using Data Frame like this\nemployeesDF.orderBy(upper(employeesDF.first_name).alias('first_name')).show()\n# Alternative - we can also refer column names using Data Frame like this\nemployeesDF.orderBy(upper(employeesDF['first_name']).alias('first_name')).show()\n# Alternative - we can also refer column names using Data Frame like this\nemployeesDF.orderBy(upper(employeesDF.first_name).alias('first_name')).show()\n\n\n#  note all are same "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b85515bd-073b-4b3d-9e88-2012a167c23e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import concat, col, lit\n\nemployeesDF.select(concat(col(\"first_name\"),lit(\", \"),col(\"last_name\")).alias(\"full_name\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ed012b2-d830-4686-8ed5-5f7b048d369f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## functions upper, lower , initcap,length  take 4 argument from \nfrom pyspark.sql.functions import lower,upper,length,initcap\nemployeesDF. \\\n  select(\"employee_id\", \"nationality\"). \\\n  withColumn(\"nationality_upper\", upper(col(\"nationality\"))). \\\n  withColumn(\"nationality_lower\", lower(col(\"nationality\"))). \\\n  withColumn(\"nationality_initcap\", initcap(col(\"nationality\"))). \\\n  withColumn(\"nationality_length\", length(col(\"nationality\"))). \\\n  show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51c7bde1-2d03-4d20-bbc7-b728ee160f02","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n|          1| united states|    UNITED STATES|    united states|      United States|                13|\n|          2|         India|            INDIA|            india|              India|                 5|\n|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n|          1| united states|    UNITED STATES|    united states|      United States|                13|\n|          2|         India|            INDIA|            india|              India|                 5|\n|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# * `substring` function takes 3 arguments, **column**, **position**, **length**. We can also provide position from the end by passing negative value.\n## substring also take negative value \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57d226b4-ce4d-4222-8771-462cc1f4f5ee","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import substring, lit"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca740c9a-44cb-4dcf-b62a-0f74fb37b72e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n                     ),\n                     (2, \"Henry\", \"Ford\", 1250.0, \n                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n                     ),\n                     (3, \"Nick\", \"Junior\", 750.0, \n                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n                     ),\n                     (4, \"Bill\", \"Gomes\", 1500.0, \n                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n                     )\n                ]\n\ndf_emp= spark.createDataFrame(employees,schema=\"\"\"employee_id INT, first_name STRING, \n                    last_name STRING, salary FLOAT, nationality STRING,\n                    phone_number STRING, ssn STRING\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39e549eb-a82e-4d34-9e7c-0c52e4ba70a3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(df_emp.show(4,truncate=False))\n\n## find last 4 digit of phone no \nemployeesDF. \\\n    select(\"employee_id\", \"phone_number\", \"ssn\"). \\\n    withColumn(\"phone_last4\", substring(col(\"phone_number\"), -4, 4).cast(\"int\")). \\\n    withColumn(\"ssn_last4\", substring(col(\"ssn\"), 8, 4).cast(\"int\")). \\\n    show()\n\nhelp(substring)\n# # note: substring(str, pos, len) where position -4 position from last start and take 4 values \n## ie from postion it start taking values of certain length \n## ssn are 11 char length "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b492f01-cada-47d0-abd9-41de3fc2a4e0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|nationality   |phone_number    |ssn        |\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890 |123 45 6789|\n|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111|222 33 4444|\n|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\nNone\n+-----------+----------------+-----------+-----------+---------+\n|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n+-----------+----------------+-----------+-----------+---------+\n|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n+-----------+----------------+-----------+-----------+---------+\n\nHelp on function substring in module pyspark.sql.functions:\n\nsubstring(str, pos, len)\n    Substring starts at `pos` and is of length `len` when str is String type or\n    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n    when str is Binary type.\n    \n    .. versionadded:: 1.5.0\n    \n    Notes\n    -----\n    The position is not zero based, but 1 based index.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n    [Row(s='ab')]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|nationality   |phone_number    |ssn        |\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890 |123 45 6789|\n|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111|222 33 4444|\n|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\nNone\n+-----------+----------------+-----------+-----------+---------+\n|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n+-----------+----------------+-----------+-----------+---------+\n|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n+-----------+----------------+-----------+-----------+---------+\n\nHelp on function substring in module pyspark.sql.functions:\n\nsubstring(str, pos, len)\n    Substring starts at `pos` and is of length `len` when str is String type or\n    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n    when str is Binary type.\n    \n    .. versionadded:: 1.5.0\n    \n    Notes\n    -----\n    The position is not zero based, but 1 based index.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n    [Row(s='ab')]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# split takes 2 arguments, column and delimiter.\n# split convert each string into array and we can access the elements using index.\n## note : split () ===convert stringh to array then can be access via array \n#  split (colName , pattern ,limit ) === no of times regex pattern can be applied\nfrom pyspark.sql.functions import explode, split\nemployeesDF.select(\"employee_id\", \"phone_number\", \"ssn\"). \\\n    withColumn(\"area_code\", split(\"phone_number\", \" \")[1].cast(\"int\")). \\\n    withColumn(\"phone_last4\", split(\"phone_number\", \" \")[3].cast(\"int\")). \\\n    withColumn(\"ssn_last4\", split(\"ssn\", \" \")[2].cast(\"int\")). \\\n    show()\n## access array element s by [] index start from 0 in array *\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4c0bd34-4257-4018-8fae-31c45d515f07","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------------+-----------+---------+-----------+---------+\n|employee_id|    phone_number|        ssn|area_code|phone_last4|ssn_last4|\n+-----------+----------------+-----------+---------+-----------+---------+\n|          1| +1 123 456 7890|123 45 6789|      123|       7890|     6789|\n|          2|+91 234 567 8901|456 78 9123|      234|       8901|     9123|\n|          3|+44 111 111 1111|222 33 4444|      111|       1111|     4444|\n|          4|+61 987 654 3210|789 12 6118|      987|       3210|     6118|\n+-----------+----------------+-----------+---------+-----------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------------+-----------+---------+-----------+---------+\n|employee_id|    phone_number|        ssn|area_code|phone_last4|ssn_last4|\n+-----------+----------------+-----------+---------+-----------+---------+\n|          1| +1 123 456 7890|123 45 6789|      123|       7890|     6789|\n|          2|+91 234 567 8901|456 78 9123|      234|       8901|     9123|\n|          3|+44 111 111 1111|222 33 4444|      111|       1111|     4444|\n|          4|+61 987 654 3210|789 12 6118|      987|       3210|     6118|\n+-----------+----------------+-----------+---------+-----------+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# * Both lpad and rpad, take 3 arguments - column or expression, desired length and the character need to be padded.\n## lpad , rpad \nfrom pyspark.sql.functions import lpad, rpad, concat\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cfaa392f-913d-4f4b-8da8-1f4c97d1e8b9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lpad, rpad, concat\n\nemployees = [(1, \"Scott\", \"Tiger\", 1000.0, \n                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n                     ),\n                     (2, \"Henry\", \"Ford\", 1250.0, \n                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n                     ),\n                     (3, \"Nick\", \"Junior\", 750.0, \n                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n                     ),\n                     (4, \"Bill\", \"Gomes\", 1500.0, \n                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n                     )\n                ]\nemployeesDF = spark.createDataFrame(employees). \\\n    toDF(\"employee_id\", \"first_name\",\n         \"last_name\", \"salary\",\n         \"nationality\", \"phone_number\",\n         \"ssn\"\n        )\nprint(employeesDF.show())\n\nempFixedDF = employeesDF.select(\n    concat(\n        lpad(\"employee_id\", 5, \"0\"), \n        rpad(\"first_name\", 10, \"-\"), \n        rpad(\"last_name\", 10, \"-\"),\n        lpad(\"salary\", 10, \"0\"), \n        rpad(\"nationality\", 15, \"-\"), \n        rpad(\"phone_number\", 17, \"-\"), \n        \"ssn\"\n    ).alias(\"employee\")\n)\nempFixedDF.show(2, truncate=False)\n\n#  note padding means adding data ata start or end  , syntax pad(colName, totalengthofstringtoshown,chartobepadded )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83650eb1-8cec-4b07-8908-f13931d8cb40","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\nNone\n+------------------------------------------------------------------------------+\n|employee                                                                      |\n+------------------------------------------------------------------------------+\n|00001Scott-----Tiger-----00001000.0united states--+1 123 456 7890--123 45 6789|\n|00002Henry-----Ford------00001250.0India----------+91 234 567 8901-456 78 9123|\n+------------------------------------------------------------------------------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\nNone\n+------------------------------------------------------------------------------+\n|employee                                                                      |\n+------------------------------------------------------------------------------+\n|00001Scott-----Tiger-----00001000.0united states--+1 123 456 7890--123 45 6789|\n|00002Henry-----Ford------00001250.0India----------+91 234 567 8901-456 78 9123|\n+------------------------------------------------------------------------------+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# As of now Spark trim functions take the column as argument and remove leading or trailing spaces. However, we can use expr or selectExpr to use Spark SQL based trim functions to remove leading or trailing spaces or any other such characters.\n#     Trim spaces towards left - ltrim\n#     Trim spaces towards right - rtrim\n#     Trim spaces on both sides - trim\n# note:  rtrim(str) - Removes the trailing space characters from `str`\n#  ltrim(str) --- remove leading space char from string \n# # if we do not specify trimStr, it will be defaulted to space\n## syntax trim('chartoTrim ',columns) === both left and right side \n## rtrim \n#     trim(str) - Removes the leading and trailing space characters from `str`.\n\n#     trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\n\n#     trim(LEADING FROM str) - Removes the leading space characters from `str`.\n\n#     trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\n\n\nfrom pyspark.sql.functions import ltrim,rtrim,trim \nprint(employeesDF.show(4))\nemployeesDF.withColumn('firstnameFirst 2 char',ltrim('first_name'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3690cb59-c899-4523-87b4-be38642fd451","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\nNone\nOut[49]: DataFrame[employee_id: bigint, first_name: string, last_name: string, salary: double, nationality: string, phone_number: string, ssn: string, firstnameFirst 2 char: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+---------+------+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+--------------+----------------+-----------+\n\nNone\nOut[49]: DataFrame[employee_id: bigint, first_name: string, last_name: string, salary: double, nationality: string, phone_number: string, ssn: string, firstnameFirst 2 char: string]"]}}],"execution_count":0},{"cell_type":"code","source":["l = [(\"   Hello.    \",) ]\ndf = spark.createDataFrame(l).toDF(\"dummy\")\ndf.show()\nfrom pyspark.sql.functions import col, ltrim, rtrim, trim,expr\ndf.withColumn(\"ltrim\", ltrim(col(\"dummy\"))).withColumn(\"rtrim\", rtrim(col(\"dummy\"))).withColumn(\"trim\", trim(col(\"dummy\"))).show()\n\n# if we do not specify trimStr, it will be defaulted to space\ndf.withColumn(\"ltrim\", expr(\"ltrim(dummy)\")). \\\n  withColumn(\"rtrim\", expr(\"rtrim('.', rtrim(dummy))\")). \\\n  withColumn(\"trim\", trim(col(\"dummy\"))). \\\n  show()\n\n# note : we jhave imported expr \ndf.withColumn(\"ltrim\", expr(\"trim(LEADING ' ' FROM dummy)\")). \\\n  withColumn(\"rtrim\", expr(\"trim(TRAILING '.' FROM rtrim(dummy))\")). \\\n  withColumn(\"trim\", expr(\"trim(BOTH ' ' FROM dummy)\")). \\\n  show()\n\n## leading \" char\" being removed "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"331d1964-ec57-445f-a303-c5cc7f897763","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+\n|        dummy|\n+-------------+\n|   Hello.    |\n+-------------+\n\n+-------------+----------+---------+------+\n|        dummy|     ltrim|    rtrim|  trim|\n+-------------+----------+---------+------+\n|   Hello.    |Hello.    |   Hello.|Hello.|\n+-------------+----------+---------+------+\n\n+-------------+----------+--------+------+\n|        dummy|     ltrim|   rtrim|  trim|\n+-------------+----------+--------+------+\n|   Hello.    |Hello.    |   Hello|Hello.|\n+-------------+----------+--------+------+\n\n+-------------+----------+--------+------+\n|        dummy|     ltrim|   rtrim|  trim|\n+-------------+----------+--------+------+\n|   Hello.    |Hello.    |   Hello|Hello.|\n+-------------+----------+--------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+\n|        dummy|\n+-------------+\n|   Hello.    |\n+-------------+\n\n+-------------+----------+---------+------+\n|        dummy|     ltrim|    rtrim|  trim|\n+-------------+----------+---------+------+\n|   Hello.    |Hello.    |   Hello.|Hello.|\n+-------------+----------+---------+------+\n\n+-------------+----------+--------+------+\n|        dummy|     ltrim|   rtrim|  trim|\n+-------------+----------+--------+------+\n|   Hello.    |Hello.    |   Hello|Hello.|\n+-------------+----------+--------+------+\n\n+-------------+----------+--------+------+\n|        dummy|     ltrim|   rtrim|  trim|\n+-------------+----------+--------+------+\n|   Hello.    |Hello.    |   Hello|Hello.|\n+-------------+----------+--------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import current_date, current_timestamp,lit,to_date\nl = [(\"X\", )]\ndf = spark.createDataFrame(l).toDF(\"dummy\")\ndf.select(current_timestamp()).show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS\ndf.select(to_date(lit('20210228'), 'yyyyMMdd').alias('to_date')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"05eb90e9-2700-4274-8795-dfba6215e239","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------+\n|current_timestamp()    |\n+-----------------------+\n|2022-12-12 03:33:18.953|\n+-----------------------+\n\n+----------+\n|   to_date|\n+----------+\n|2021-02-28|\n+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------+\n|current_timestamp()    |\n+-----------------------+\n|2022-12-12 03:33:18.953|\n+-----------------------+\n\n+----------+\n|   to_date|\n+----------+\n|2021-02-28|\n+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# * Adding days to a date or timestamp - `date_add`\n# * Subtracting days from a date or timestamp - `date_sub`\n# * Getting difference between 2 dates or timestamps - `datediff`\n# * Getting the number of months between 2 dates or timestamps - `months_between`\n# * Adding months to a date or timestamp - `add_months`\n# * Getting next day from a given date - `next_day`\n# * All the functions are self explanatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp field.\n# date_add == add days to date or timestamp \n# date_sub === substracting days from date or timestamp \n# datediff === difference between 2 dates \n# add_month === add month to date\n# next_date --- get next date to given date \ndatetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n                ]\ndatetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")\nfrom pyspark.sql.functions import date_add, date_sub\n\ndatetimesDF. \\\n    withColumn(\"date_add_date\", date_add(\"date\", 10)). \\\n    withColumn(\"date_add_time\", date_add(\"time\", 10)). \\\n    withColumn(\"date_sub_date\", date_sub(\"date\", 10)). \\\n    withColumn(\"date_sub_time\", date_sub(\"time\", 10)). \\\n    show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f577df40-91f3-400a-86d4-6a837c0f0b0d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+--------------------+-------------+-------------+-------------+-------------+\n|      date|                time|date_add_date|date_add_time|date_sub_date|date_sub_time|\n+----------+--------------------+-------------+-------------+-------------+-------------+\n|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-03-10|   2014-02-18|   2014-02-18|\n|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-03-10|   2016-02-19|   2016-02-19|\n|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2018-01-10|   2017-10-21|   2017-12-21|\n|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-09-10|   2019-11-20|   2019-08-21|\n+----------+--------------------+-------------+-------------+-------------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+--------------------+-------------+-------------+-------------+-------------+\n|      date|                time|date_add_date|date_add_time|date_sub_date|date_sub_time|\n+----------+--------------------+-------------+-------------+-------------+-------------+\n|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-03-10|   2014-02-18|   2014-02-18|\n|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-03-10|   2016-02-19|   2016-02-19|\n|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2018-01-10|   2017-10-21|   2017-12-21|\n|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-09-10|   2019-11-20|   2019-08-21|\n+----------+--------------------+-------------+-------------+-------------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import months_between, add_months, round\ndatetimesDF. \\\n    withColumn(\"months_between_date\", round(months_between(current_date(), \"date\"), 2)). \\\n    withColumn(\"months_between_time\", round(months_between(current_timestamp(), \"time\"), 2)). \\\n    withColumn(\"add_months_date\", add_months(\"date\", 3)). \\\n    withColumn(\"add_months_time\", add_months(\"time\", 3)). \\\n    show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"baa675ba-a968-4582-9d33-7ef0dd0d7251","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|2014-02-28|2014-02-28 10:00:00.123|105.48             |105.48             |2014-05-28     |2014-05-28     |\n|2016-02-29|2016-02-29 08:08:08.999|81.45              |81.45              |2016-05-29     |2016-05-29     |\n|2017-10-31|2017-12-31 11:59:59.123|61.39              |59.38              |2018-01-31     |2018-03-31     |\n|2019-11-30|2019-08-31 00:00:00.000|36.42              |39.39              |2020-02-29     |2019-11-30     |\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|2014-02-28|2014-02-28 10:00:00.123|105.48             |105.48             |2014-05-28     |2014-05-28     |\n|2016-02-29|2016-02-29 08:08:08.999|81.45              |81.45              |2016-05-29     |2016-05-29     |\n|2017-10-31|2017-12-31 11:59:59.123|61.39              |59.38              |2018-01-31     |2018-03-31     |\n|2019-11-30|2019-08-31 00:00:00.000|36.42              |39.39              |2020-02-29     |2019-11-30     |\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# * We can use `trunc` or `date_trunc` for the same to get the beginning date of the week, month, current year etc by passing date or timestamp to it.\n# * We can use `trunc` to get beginning date of the month or year by passing date or timestamp to it - for example `trunc(current_date(), \"MM\")` will give the first of the current month.\n# * We can use `date_trunc` to get beginning date of the month or year as well as beginning time of the day or hour by passing timestamp to it.\n#   * Get beginning date based on month - `date_trunc(\"MM\", current_timestamp())`\n#   * Get beginning time based on day - `date_trunc(\"DAY\", current_timestamp())`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8be47163-ae28-486c-b91f-887dfc739973","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import date_trunc\ndatetimesDF. \\\n    withColumn(\"date_dt\", date_trunc(\"HOUR\", \"date\")). \\\n    withColumn(\"time_dt\", date_trunc(\"HOUR\", \"time\")). \\\n    withColumn(\"time_dt1\", date_trunc(\"dd\", \"time\")). \\\n    show(truncate=False)\n\n##  We can use `trunc` or `date_trunc` for the same to get the beginning date of the week, month, current year etc by passing date or timestamp to it.\n## trunc === on time is getting begnning of day,time,month,current ,year"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c45485c2-bb44-4d61-9a04-8b708a919e4c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----------------------+-------------------+-------------------+-------------------+\n|date      |time                   |date_dt            |time_dt            |time_dt1           |\n+----------+-----------------------+-------------------+-------------------+-------------------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|2016-02-29 00:00:00|\n|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|2017-12-31 00:00:00|\n|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|2019-08-31 00:00:00|\n+----------+-----------------------+-------------------+-------------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----------------------+-------------------+-------------------+-------------------+\n|date      |time                   |date_dt            |time_dt            |time_dt1           |\n+----------+-----------------------+-------------------+-------------------+-------------------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|2016-02-29 00:00:00|\n|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|2017-12-31 00:00:00|\n|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|2019-08-31 00:00:00|\n+----------+-----------------------+-------------------+-------------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# * `year`,* `month`,* `weekofyear`,* `dayofyear`, `dayofmonth`,* `dayofweek`,* `hour`,* `minute`,* `second`\nl = [(\"X\", )]\ndf = spark.createDataFrame(l).toDF(\"dummy\")\nfrom pyspark.sql.functions import year, month, weekofyear, dayofmonth,dayofyear, dayofweek, current_date\nfrom pyspark.sql.functions import current_timestamp, hour, minute, second\ndf.select(\n    current_timestamp().alias('current_timestamp'), \n    year(current_timestamp()).alias('year'),\n    month(current_timestamp()).alias('month'),\n    dayofmonth(current_timestamp()).alias('dayofmonth'),\n    hour(current_timestamp()).alias('hour'),\n    minute(current_timestamp()).alias('minute'),\n    second(current_timestamp()).alias('second')\n).show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9ec2984-50fa-44f8-9859-e0464a646842","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------+----+-----+----------+----+------+------+\n|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n+-----------------------+----+-----+----------+----+------+------+\n|2022-12-12 03:45:22.095|2022|12   |12        |3   |45    |22    |\n+-----------------------+----+-----+----------+----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------+----+-----+----------+----+------+------+\n|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n+-----------------------+----+-----+----------+----+------+------+\n|2022-12-12 03:45:22.095|2022|12   |12        |3   |45    |22    |\n+-----------------------+----+-----+----------+----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# * `yyyy-MM-dd` is the standard date format\n# * `yyyy-MM-dd HH:mm:ss.SSS` is the standard timestamp format\n\nfrom pyspark.sql.functions import col, to_date, to_timestamp\ndatetimes = [(20140228, \"28-Feb-2014 10:00:00.123\"),\n                     (20160229, \"20-Feb-2016 08:08:08.999\"),\n                     (20171031, \"31-Dec-2017 11:59:59.123\"),\n                     (20191130, \"31-Aug-2019 00:00:00.000\")\n                ]\ndatetimesDF = spark.createDataFrame(datetimes, schema=\"date BIGINT, time STRING\")\ndatetimesDF. \\\n    withColumn('to_date', to_date(col('date').cast('string'), 'yyyyMMdd')). \\\n    withColumn('to_timestamp', to_timestamp(col('time'), 'dd-MMM-yyyy HH:mm:ss.SSS')). \\\n    show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18d82814-23d6-41a4-a6d2-028cda32eea1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+------------------------+----------+-----------------------+\n|date    |time                    |to_date   |to_timestamp           |\n+--------+------------------------+----------+-----------------------+\n|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n|20160229|20-Feb-2016 08:08:08.999|2016-02-29|2016-02-20 08:08:08.999|\n|20171031|31-Dec-2017 11:59:59.123|2017-10-31|2017-12-31 11:59:59.123|\n|20191130|31-Aug-2019 00:00:00.000|2019-11-30|2019-08-31 00:00:00    |\n+--------+------------------------+----------+-----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+------------------------+----------+-----------------------+\n|date    |time                    |to_date   |to_timestamp           |\n+--------+------------------------+----------+-----------------------+\n|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n|20160229|20-Feb-2016 08:08:08.999|2016-02-29|2016-02-20 08:08:08.999|\n|20171031|31-Dec-2017 11:59:59.123|2017-10-31|2017-12-31 11:59:59.123|\n|20191130|31-Aug-2019 00:00:00.000|2019-11-30|2019-08-31 00:00:00    |\n+--------+------------------------+----------+-----------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \n\n#     We can use date_format to extract the requ\n# ired information in a desired format from standard date or timestamp. Earlier we have explored to_date and to_timestamp to convert non standard date or timestamp to standard ones respectively.\nfrom pyspark.sql.functions import date_format\n\n\ndatetimesDF.withColumn(\"day_name_abbr\",to_date(col('date').cast('string'),'yyyyMMdd')).\\\n            withColumn(\"day_name_abbr\",date_format(\"day_name_abbr\",'EEEE')).show()\n\n\n### spark 3 : day name in string can be shown with \"EEEE\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ad43688-216d-4c52-a9aa-4c424628a5d6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------------------+-------------+\n|    date|                time|day_name_abbr|\n+--------+--------------------+-------------+\n|20140228|28-Feb-2014 10:00...|       Friday|\n|20160229|20-Feb-2016 08:08...|       Monday|\n|20171031|31-Dec-2017 11:59...|      Tuesday|\n|20191130|31-Aug-2019 00:00...|     Saturday|\n+--------+--------------------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------------------+-------------+\n|    date|                time|day_name_abbr|\n+--------+--------------------+-------------+\n|20140228|28-Feb-2014 10:00...|       Friday|\n|20160229|20-Feb-2016 08:08...|       Monday|\n|20171031|31-Dec-2017 11:59...|      Tuesday|\n|20191130|31-Aug-2019 00:00...|     Saturday|\n+--------+--------------------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# It is an integer and started from January 1st 1970 Midnight UTC.\n# Beginning time is also known as epoch and is incremented by 1 every second.\n# We can convert Unix Timestamp to regular date or timestamp and vice versa.\n# We can use unix_timestamp to convert regular date or timestamp to a unix timestamp value. For example unix_timestamp(lit(\"2019-11-19 00:00:00\"))\n# We can use from_unixtime to convert unix timestamp to regular date or timestamp. For example from_unixtime(lit(1574101800))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21ea0361-e3bf-4085-b1dd-fcb6bba9f806","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import unix_timestamp, col,from_unixtime\nunixtimes = [(1393561800, ),\n             (1456713488, ),\n             (1514701799, ),\n             (1567189800, )\n            ]\nunixtimesDF = spark.createDataFrame(unixtimes).toDF(\"unixtime\")\nunixtimesDF.show()\n\nunixtimesDF.withColumn(\"date\", from_unixtime(\"unixtime\", \"yyyyMMdd\")).withColumn(\"time\", from_unixtime(\"unixtime\")). \\\n    show()\n#yyyyMMdd\n\n### from_unixtime ===  to convert unix time to date string \n## iff not specified * Get date in yyyyMMdd format and also complete timestamp."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d95308a-6d65-45e9-91ef-9072a5a4e0bc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+\n|  unixtime|\n+----------+\n|1393561800|\n|1456713488|\n|1514701799|\n|1567189800|\n+----------+\n\n+----------+--------+-------------------+\n|  unixtime|    date|               time|\n+----------+--------+-------------------+\n|1393561800|20140228|2014-02-28 04:30:00|\n|1456713488|20160229|2016-02-29 02:38:08|\n|1514701799|20171231|2017-12-31 06:29:59|\n|1567189800|20190830|2019-08-30 18:30:00|\n+----------+--------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+\n|  unixtime|\n+----------+\n|1393561800|\n|1456713488|\n|1514701799|\n|1567189800|\n+----------+\n\n+----------+--------+-------------------+\n|  unixtime|    date|               time|\n+----------+--------+-------------------+\n|1393561800|20140228|2014-02-28 04:30:00|\n|1456713488|20160229|2016-02-29 02:38:08|\n|1514701799|20171231|2017-12-31 06:29:59|\n|1567189800|20190830|2019-08-30 18:30:00|\n+----------+--------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["### dealing with nulls \n\nfrom pyspark.sql.functions import coalesce\nfrom pyspark.sql.functions import lit\n\n\nemployees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n                     ),\n                     (2, \"Henry\", \"Ford\", 1250.0, None,\n                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n                     ),\n                     (3, \"Nick\", \"Junior\", 750.0, '',\n                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n                     ),\n                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n                     )\n                ]\n\nemployeesDF = spark.createDataFrame(employees,\n                    schema=\"\"\"employee_id INT, first_name STRING, \n                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n                    phone_number STRING, ssn STRING\"\"\"\n                   )\n\nemployeesDF.withColumn('bonus', expr(\"nvl(nullif(bonus, ''), 0)\")).show()\n\n\nemployeesDF.withColumn('bonus', expr(\"nvl(nullif(bonus, ''), 0)\")).show()\n\nemployeesDF.withColumn('payment', col('salary') + (col('salary') * coalesce(col('bonus').cast('int'), lit(0)) / 100)).show()\n### note: expr  null and nullIf will remove  nulls "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1be616b4-7680-4779-8fa2-56610c9ba58a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## using case when \n# * `CASE` and `WHEN` is typically used to apply transformations based up on conditions. We can use `CASE` and `WHEN` similar to SQL using `expr` or `selectExpr`.\n# * If we want to use APIs, Spark provides functions such as `when` and `otherwise`. `when` is available as part of `pyspark.sql.functions`. On top of column type that is generated using `when` \n# we should be able to invoke `otherwise`.\n\n# note: In spark api  we use when function imported from pyspark.sql.functions \n## .when have otherwise function on columns \nfrom pyspark.sql.functions import when\nfrom pyspark.sql.functions import coalesce, lit, col\n#when(condition, value)\n#df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n## when (<conditiontrue >, <do or Assign>)\nemployees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n                     ),\n                     (2, \"Henry\", \"Ford\", 1250.0, None,\n                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n                     ),\n                     (3, \"Nick\", \"Junior\", 750.0, '',\n                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n                     ),\n                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n                     )\n                ]\n\n\nemployeesDF = spark. \\\n    createDataFrame(employees,\n                    schema=\"\"\"employee_id INT, first_name STRING, \n                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n                    phone_number STRING, ssn STRING\"\"\"\n                   )\n\nemployeesDF.show()\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"539c481b-2257-4ba1-a8fb-c623ec849140","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## use of case when in pyspark\n\nemployeesDF.withColumn('bonus',when((col('bonus').isNull()) | (col('bonus') == lit('')), 0).otherwise(col('bonus'))).show()\n\n### to handle null and '' values in when \n## when can take multiple conditions with and shown as  & == \"|\", \n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c378035-f0f0-429c-abf4-c5b7a9ed4490","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["persons = [\n    (1, 1),\n    (2, 13),\n    (3, 18),\n    (4, 60),\n    (5, 120),\n    (6, 0),\n    (7, 12),\n    (8, 160)\n]\n\npersonsDF = spark.createDataFrame(persons, schema='id INT, age INT')\n\n\npersonsDF.withColumn(\n        'category',\n        when(col('age').between(0, 2), 'New Born').\n        when((col('age') > 2) & (col('age') <= 12), 'Infant').\n        when((col('age') > 12) & (col('age') <= 48), 'Toddler').\n        when((col('age') > 48) & (col('age') <= 144), 'Kid').\n        otherwise('Teenager or Adult')\n    ).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7dbbfe70-9f23-4c63-bdea-31181da502b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---+-----------------+\n| id|age|         category|\n+---+---+-----------------+\n|  1|  1|         New Born|\n|  2| 13|          Toddler|\n|  3| 18|          Toddler|\n|  4| 60|              Kid|\n|  5|120|              Kid|\n|  6|  0|         New Born|\n|  7| 12|           Infant|\n|  8|160|Teenager or Adult|\n+---+---+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---+-----------------+\n| id|age|         category|\n+---+---+-----------------+\n|  1|  1|         New Born|\n|  2| 13|          Toddler|\n|  3| 18|          Toddler|\n|  4| 60|              Kid|\n|  5|120|              Kid|\n|  6|  0|         New Born|\n|  7| 12|           Infant|\n|  8|160|Teenager or Adult|\n+---+---+-----------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["### between (no,no)\npersonsDF. \\\n    withColumn(\n        'category',\n        when(col('age').between(0, 2), 'New Born').\n        when((col('age') > 2) & (col('age') <= 12), 'Infant').\n        when((col('age') > 12) & (col('age') <= 48), 'Toddler').\n        when((col('age') > 48) & (col('age') <= 144), 'Kid').\n        otherwise('Teenager or Adult')\n    ). \\\n    show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"05ed826e-2caa-4de5-a823-8be570a0d102","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---+-----------------+\n| id|age|         category|\n+---+---+-----------------+\n|  1|  1|         New Born|\n|  2| 13|          Toddler|\n|  3| 18|          Toddler|\n|  4| 60|              Kid|\n|  5|120|              Kid|\n|  6|  0|         New Born|\n|  7| 12|           Infant|\n|  8|160|Teenager or Adult|\n+---+---+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---+-----------------+\n| id|age|         category|\n+---+---+-----------------+\n|  1|  1|         New Born|\n|  2| 13|          Toddler|\n|  3| 18|          Toddler|\n|  4| 60|              Kid|\n|  5|120|              Kid|\n|  6|  0|         New Born|\n|  7| 12|           Infant|\n|  8|160|Teenager or Adult|\n+---+---+-----------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"data_bricks_associate_developer_exam_basic3.ipynb","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1963588101191875}},"nbformat":4,"nbformat_minor":0}
