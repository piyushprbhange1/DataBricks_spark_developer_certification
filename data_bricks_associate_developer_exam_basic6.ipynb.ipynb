{"cells":[{"cell_type":"markdown","source":["## joining in dataframes\n#types of join \n1 inner\n2 left\n3 right \n4 left outer\n* 5 right outer \n* 6 full outer join  ===  ie all rows which are not common \n* 7 broadcast join \n8 cartition product , cross join \n\n* Inner Join - **join** or **inner join**\n* **Left** or **Right Outer Join**\n* Full Outer Join - **a left outer join b** union **a right outer join b**\n* Cross Join\n* Spark Data Frames have a function called `join`. It can be used to perform inner or outer or full outer join.\n* We need to specify **join condition** for Inner or Outer or Full Outer Join."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"862f37f4-2f90-4ef7-8a29-aaaa2c1ce5b4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import Row"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d1872a8-9eab-454a-91c5-221000e0e2d3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\ncourses = [\n    {\n        'course_id': 1,\n        'course_title': 'Mastering Python',\n        'course_published_dt': datetime.date(2021, 1, 14),\n        'is_active': True,\n        'last_updated_ts': datetime.datetime(2021, 2, 18, 16, 57, 25)\n    },\n    {\n        'course_id': 2,\n        'course_title': 'Data Engineering Essentials',\n        'course_published_dt': datetime.date(2021, 2, 10),\n        'is_active': True,\n        'last_updated_ts': datetime.datetime(2021, 3, 5, 12, 7, 33)\n    },\n    {\n        'course_id': 3,\n        'course_title': 'Mastering Pyspark',\n        'course_published_dt': datetime.date(2021, 1, 7),\n        'is_active': True,\n        'last_updated_ts': datetime.datetime(2021, 4, 6, 10, 5, 42)\n    },\n    {\n        'course_id': 4,\n        'course_title': 'AWS Essentials',\n        'course_published_dt': datetime.date(2021, 3, 19),\n        'is_active': False,\n        'last_updated_ts': datetime.datetime(2021, 4, 10, 2, 25, 36)\n    },\n    {\n        'course_id': 5,\n        'course_title': 'Docker 101',\n        'course_published_dt': datetime.date(2021, 2, 28),\n        'is_active': True,\n        'last_updated_ts': datetime.datetime(2021, 3, 21, 7, 18, 52)\n    }\n]\n\ncourses_df = spark.createDataFrame([Row(**course) for course in courses])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2868c0ea-a6c7-4736-8a24-de3b40070153","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["users = [{\n  \"user_id\": 1,\n  \"user_first_name\": \"Sandra\",\n  \"user_last_name\": \"Karpov\",\n  \"user_email\": \"skarpov0@ovh.net\"\n}, {\n  \"user_id\": 2,\n  \"user_first_name\": \"Kari\",\n  \"user_last_name\": \"Dearth\",\n  \"user_email\": \"kdearth1@so-net.ne.jp\"\n}, {\n  \"user_id\": 3,\n  \"user_first_name\": \"Joanna\",\n  \"user_last_name\": \"Spennock\",\n  \"user_email\": \"jspennock2@redcross.org\"\n}, {\n  \"user_id\": 4,\n  \"user_first_name\": \"Hirsch\",\n  \"user_last_name\": \"Conaboy\",\n  \"user_email\": \"hconaboy3@barnesandnoble.com\"\n}, {\n  \"user_id\": 5,\n  \"user_first_name\": \"Loreen\",\n  \"user_last_name\": \"Malin\",\n  \"user_email\": \"lmalin4@independent.co.uk\"\n}, {\n  \"user_id\": 6,\n  \"user_first_name\": \"Augy\",\n  \"user_last_name\": \"Christon\",\n  \"user_email\": \"achriston5@mlb.com\"\n}, {\n  \"user_id\": 7,\n  \"user_first_name\": \"Trudey\",\n  \"user_last_name\": \"Choupin\",\n  \"user_email\": \"tchoupin6@de.vu\"\n}, {\n  \"user_id\": 8,\n  \"user_first_name\": \"Nadine\",\n  \"user_last_name\": \"Grimsdell\",\n  \"user_email\": \"ngrimsdell7@sohu.com\"\n}, {\n  \"user_id\": 9,\n  \"user_first_name\": \"Vassily\",\n  \"user_last_name\": \"Tamas\",\n  \"user_email\": \"vtamas8@businessweek.com\"\n}, {\n  \"user_id\": 10,\n  \"user_first_name\": \"Wells\",\n  \"user_last_name\": \"Simpkins\",\n  \"user_email\": \"wsimpkins9@amazon.co.uk\"\n}]\n\nusers_df = spark.createDataFrame([Row(**user) for user in users])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebb05c3c-09e8-4005-be03-56c0e0fe5e4e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["course_enrolments = [{\n  \"course_enrolment_id\": 1,\n  \"user_id\": 10,\n  \"course_id\": 2,\n  \"price_paid\": 9.99\n}, {\n  \"course_enrolment_id\": 2,\n  \"user_id\": 5,\n  \"course_id\": 2,\n  \"price_paid\": 9.99\n}, {\n  \"course_enrolment_id\": 3,\n  \"user_id\": 7,\n  \"course_id\": 5,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 4,\n  \"user_id\": 9,\n  \"course_id\": 2,\n  \"price_paid\": 9.99\n}, {\n  \"course_enrolment_id\": 5,\n  \"user_id\": 8,\n  \"course_id\": 2,\n  \"price_paid\": 9.99\n}, {\n  \"course_enrolment_id\": 6,\n  \"user_id\": 5,\n  \"course_id\": 5,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 7,\n  \"user_id\": 4,\n  \"course_id\": 5,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 8,\n  \"user_id\": 7,\n  \"course_id\": 3,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 9,\n  \"user_id\": 8,\n  \"course_id\": 5,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 10,\n  \"user_id\": 3,\n  \"course_id\": 3,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 11,\n  \"user_id\": 7,\n  \"course_id\": 5,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 12,\n  \"user_id\": 3,\n  \"course_id\": 2,\n  \"price_paid\": 9.99\n}, {\n  \"course_enrolment_id\": 13,\n  \"user_id\": 5,\n  \"course_id\": 2,\n  \"price_paid\": 9.99\n}, {\n  \"course_enrolment_id\": 14,\n  \"user_id\": 4,\n  \"course_id\": 3,\n  \"price_paid\": 10.99\n}, {\n  \"course_enrolment_id\": 15,\n  \"user_id\": 8,\n  \"course_id\": 2,\n  \"price_paid\": 9.99\n}]\n\ncourse_enrolments_df = spark.createDataFrame([Row(**ce) for ce in course_enrolments])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e23f977f-8961-4a62-9309-ed343f10d68e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["courses_df.show(3)\nusers_df.show(3)\ncourse_enrolments_df.show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e84891aa-f789-46d1-92ae-31f3cb1c3fd9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+--------------------+-------------------+---------+-------------------+\n|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n+---------+--------------------+-------------------+---------+-------------------+\n|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n+---------+--------------------+-------------------+---------+-------------------+\nonly showing top 3 rows\n\n+-------+---------------+--------------+--------------------+\n|user_id|user_first_name|user_last_name|          user_email|\n+-------+---------------+--------------+--------------------+\n|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|\n+-------+---------------+--------------+--------------------+\nonly showing top 3 rows\n\n+-------------------+-------+---------+----------+\n|course_enrolment_id|user_id|course_id|price_paid|\n+-------------------+-------+---------+----------+\n|                  1|     10|        2|      9.99|\n|                  2|      5|        2|      9.99|\n|                  3|      7|        5|     10.99|\n+-------------------+-------+---------+----------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------------------+-------------------+---------+-------------------+\n|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n+---------+--------------------+-------------------+---------+-------------------+\n|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n+---------+--------------------+-------------------+---------+-------------------+\nonly showing top 3 rows\n\n+-------+---------------+--------------+--------------------+\n|user_id|user_first_name|user_last_name|          user_email|\n+-------+---------------+--------------+--------------------+\n|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|\n+-------+---------------+--------------+--------------------+\nonly showing top 3 rows\n\n+-------------------+-------+---------+----------+\n|course_enrolment_id|user_id|course_id|price_paid|\n+-------------------+-------+---------+----------+\n|                  1|     10|        2|      9.99|\n|                  2|      5|        2|      9.99|\n|                  3|      7|        5|     10.99|\n+-------------------+-------+---------+----------+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["## defining alias \n\n# alias(alias) method of pyspark.sql.dataframe.DataFrame instance\n#     Returns a new :class:`DataFrame` with an alias set\ncourses_df.alias('c').select('c.course_id').show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16422bea-5738-4be9-9468-8c0bdbbaf740","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+\n|course_id|\n+---------+\n|        1|\n|        2|\n|        3|\n+---------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+\n|course_id|\n+---------+\n|        1|\n|        2|\n|        3|\n+---------+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["* Get the user details who have enrolled for the courses.\n  * Need to join **users_df** and **course_enrolments_df**.\n  * Here are the fields that needs to be displayed.\n    * All fields from `users_df`\n    * `course_id` and `course_enrolment_id` from `course_enrolments`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2e139aa-da8f-48f0-8703-c3bf1ef57b52","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["users_df.join(course_enrolments_df,users_df.user_id == course_enrolments_df.user_id).show(2)\n\n# as both data frames have user id using same name, we can pass column name as string as well\nusers_df.join(course_enrolments_df,'user_id').show(2)\n#note as both have same name column for joining condition  we can directly pass it \n## there is no nee to give on condition \nusers_df.join(course_enrolments_df,users_df.user_id == course_enrolments_df.user_id)\\\n        .select(users_df['*'],course_enrolments_df['course_id'],course_enrolments_df['course_enrolment_id']).show(2)\n\n# users_df.join(course_enrolments_df,users_df.user_id == course_enrolments_df.user_id)\\\n#         .select('users_df.','course_enrolments_df.course_id','course_enrolments_df.course_enrolment_id').show(2)\n\n# Note: above geting fail so we are using alias \nusers_df.alias('u'). \\\n    join(course_enrolments_df.alias('ce'), users_df.user_id == course_enrolments_df.user_id). \\\n    select('u.*', 'course_id', 'course_enrolment_id'). \\\n    show(2)\n\n# note: alias is usefull while selecting all columns from dataFrame ,\n#  Imp notes : alias also usefull while grouping the data using  grouping when both dataframe have same name \nusers_df.join(course_enrolments_df, 'user_id').groupBy('user_id').count().show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24fe9f28-7bbf-4b20-8ef8-aae5f2107063","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|      3|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|      3|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\nonly showing top 2 rows\n\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\nonly showing top 2 rows\n\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|          user_email|course_id|course_enrolment_id|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        3|                 10|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        2|                 12|\n+-------+---------------+--------------+--------------------+---------+-------------------+\nonly showing top 2 rows\n\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|          user_email|course_id|course_enrolment_id|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        3|                 10|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        2|                 12|\n+-------+---------------+--------------+--------------------+---------+-------------------+\nonly showing top 2 rows\n\n+-------+-----+\n|user_id|count|\n+-------+-----+\n|      3|    2|\n|      4|    2|\n+-------+-----+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|      3|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|      3|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\nonly showing top 2 rows\n\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\nonly showing top 2 rows\n\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|          user_email|course_id|course_enrolment_id|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        3|                 10|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        2|                 12|\n+-------+---------------+--------------+--------------------+---------+-------------------+\nonly showing top 2 rows\n\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|          user_email|course_id|course_enrolment_id|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        3|                 10|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        2|                 12|\n+-------+---------------+--------------+--------------------+---------+-------------------+\nonly showing top 2 rows\n\n+-------+-----+\n|user_id|count|\n+-------+-----+\n|      3|    2|\n|      4|    2|\n+-------+-----+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## performing left join,right join , left_outer_join"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ebc7b9d-3019-444b-965e-6f981a0624ca","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["## note: join(other, on=None, how=None)\n###mostImp :\n## how = left_outer,left,right,right_outer\n### outer or full or fullouter or full_outer are same\n\n# note: for crossJoin we use .crossJoin insted of join  === get cartition product  OR can use 'cross '\n\nusers_df.join(course_enrolments_df, users_df.user_id == course_enrolments_df.user_id, 'left').show(4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce6b6b5f-660b-42fb-ad1e-a926ebd19560","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|      1|         Sandra|        Karpov|    skarpov0@ovh.net|               null|   null|     null|      null|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|               null|   null|     null|      null|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|      3|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|      3|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\nonly showing top 4 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|      1|         Sandra|        Karpov|    skarpov0@ovh.net|               null|   null|     null|      null|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|               null|   null|     null|      null|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|      3|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|      3|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\nonly showing top 4 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr,col\nfrom pyspark.sql.functions import sum, when\nusers_df.alias('u'). \\\n    join(course_enrolments_df.alias('ce'), users_df.user_id == course_enrolments_df.user_id, 'left'). \\\n    groupBy('u.user_id'). \\\n    agg(sum(expr('''\n        CASE WHEN ce.course_enrolment_id IS NULL\n            THEN 0\n        ELSE 1\n        END\n    ''')).alias('course_count')). \\\n    orderBy('u.user_id'). \\\n    show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0539386b-fa27-42f0-a933-20974bd27fb0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------+\n|user_id|course_count|\n+-------+------------+\n|      1|           0|\n|      2|           0|\n|      3|           2|\n|      4|           2|\n|      5|           3|\n|      6|           0|\n|      7|           3|\n|      8|           3|\n|      9|           1|\n|     10|           1|\n+-------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------+\n|user_id|course_count|\n+-------+------------+\n|      1|           0|\n|      2|           0|\n|      3|           2|\n|      4|           2|\n|      5|           3|\n|      6|           0|\n|      7|           3|\n|      8|           3|\n|      9|           1|\n|     10|           1|\n+-------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import sum, when\ncourse_enrolments_df.alias('ce'). \\\n    join(users_df.alias('u'), users_df.user_id == course_enrolments_df.user_id, 'right'). \\\n    groupBy('u.user_id'). \\\n    agg(sum(when(course_enrolments_df['course_enrolment_id'].isNull(), 0).otherwise(1)).alias('course_count')). \\\n    orderBy('u.user_id'). \\\n    show()\n\n\n# note: giving multiple alias "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43757d80-505f-4ac1-9998-02d3ab088a1d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------+\n|user_id|course_count|\n+-------+------------+\n|      1|           0|\n|      2|           0|\n|      3|           2|\n|      4|           2|\n|      5|           3|\n|      6|           0|\n|      7|           3|\n|      8|           3|\n|      9|           1|\n|     10|           1|\n+-------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------+\n|user_id|course_count|\n+-------+------------+\n|      1|           0|\n|      2|           0|\n|      3|           2|\n|      4|           2|\n|      5|           3|\n|      6|           0|\n|      7|           3|\n|      8|           3|\n|      9|           1|\n|     10|           1|\n+-------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# importing the data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d955df57-40e9-441a-854f-44a2b00019c2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["users1 = [\n    {\n        \"email\":\"alovett0@nsw.gov.au\",\n        \"first_name\":\"Aundrea\",\n        \"last_name\":\"Lovett\",\n        \"gender\":\"Male\",\n        \"ip_address\":\"62.72.1.143\"\n    },\n    {\n        \"email\":\"bjowling1@spiegel.de\",\n        \"first_name\":\"Bettine\",\n        \"last_name\":\"Jowling\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"26.250.197.47\"\n    },\n    {\n        \"email\":\"rablitt2@technorati.com\",\n        \"first_name\":\"Reggie\",\n        \"last_name\":\"Ablitt\",\n        \"gender\":\"Male\",\n        \"ip_address\":\"104.181.218.238\"\n    },\n    {\n        \"email\":\"tgavahan3@printfriendly.com\",\n        \"first_name\":\"Ted\",\n        \"last_name\":\"Gavahan\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"216.80.86.100\"\n    },\n    {\n        \"email\":\"ccastellan4@bloglovin.com\",\n        \"first_name\":\"Chantal\",\n        \"last_name\":\"Castellan\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"178.93.82.145\"\n    },\n    {\n        \"email\":\"hcurrier5@hexun.com\",\n        \"first_name\":\"Herrick\",\n        \"last_name\":\"Currier\",\n        \"gender\":\"Male\",\n        \"ip_address\":\"98.120.5.78\"\n    },\n    {\n        \"email\":\"zlendrem6@columbia.edu\",\n        \"first_name\":\"Zorina\",\n        \"last_name\":\"Lendrem\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"219.128.213.53\"\n    },\n    {\n        \"email\":\"lbutland7@time.com\",\n        \"first_name\":\"Lilas\",\n        \"last_name\":\"Butland\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"109.110.131.151\"\n    },\n    {\n        \"email\":\"palfonsetti8@ask.com\",\n        \"first_name\":\"Putnam\",\n        \"last_name\":\"Alfonsetti\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"167.97.48.246\"\n    },\n    {\n        \"email\":\"hunitt9@bizjournals.com\",\n        \"first_name\":\"Holden\",\n        \"last_name\":\"Unitt\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"142.228.161.192\"\n    },\n    {\n        \"email\":\"dmcmorrana@reference.com\",\n        \"first_name\":\"Dorice\",\n        \"last_name\":\"McMorran\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"233.1.28.220\"\n    },\n    {\n        \"email\":\"afaulconerb@barnesandnoble.com\",\n        \"first_name\":\"Andris\",\n        \"last_name\":\"Faulconer\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"109.40.175.103\"\n    },\n    {\n        \"email\":\"kupexc@sun.com\",\n        \"first_name\":\"Krispin\",\n        \"last_name\":\"Upex\",\n        \"gender\":\"Male\",\n        \"ip_address\":\"154.110.22.75\"\n    },\n    {\n        \"email\":\"fmancktelowd@youku.com\",\n        \"first_name\":\"Farand\",\n        \"last_name\":\"Mancktelow\",\n        \"gender\":\"Genderqueer\",\n        \"ip_address\":\"190.20.187.10\"\n    },\n    {\n        \"email\":\"kdodgshune@google.com\",\n        \"first_name\":\"Kellyann\",\n        \"last_name\":\"Dodgshun\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"80.247.105.228\"\n    }\n]\n\nfrom pyspark.sql import Row\nusers1_df = spark.createDataFrame([Row(**user) for user in users1])\n\n\nusers2 = [{\n        \"email\":\"lbutland7@time.com\",\n        \"first_name\":\"Lilas\",\n        \"last_name\":\"Butland\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"109.110.131.151\"\n    },\n    {\n        \"email\":\"palfonsetti8@ask.com\",\n        \"first_name\":\"Putnam\",\n        \"last_name\":\"Alfonsetti\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"167.97.48.246\"\n    },\n    {\n        \"email\":\"hunitt9@bizjournals.com\",\n        \"first_name\":\"Holden\",\n        \"last_name\":\"Unitt\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"142.228.161.192\"\n    },\n    {\n        \"email\":\"dmcmorrana@reference.com\",\n        \"first_name\":\"Dorice\",\n        \"last_name\":\"McMorran\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"233.1.28.220\"\n    },\n    {\n        \"email\":\"afaulconerb@barnesandnoble.com\",\n        \"first_name\":\"Andris\",\n        \"last_name\":\"Faulconer\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"109.40.175.103\"\n    },\n    {\n        \"email\":\"kupexc@sun.com\",\n        \"first_name\":\"Krispin\",\n        \"last_name\":\"Upex\",\n        \"gender\":\"Male\",\n        \"ip_address\":\"154.110.22.75\"\n    },\n    {\n        \"email\":\"fmancktelowd@youku.com\",\n        \"first_name\":\"Farand\",\n        \"last_name\":\"Mancktelow\",\n        \"gender\":\"Genderqueer\",\n        \"ip_address\":\"190.20.187.10\"\n    },\n    {\n        \"email\":\"kdodgshune@google.com\",\n        \"first_name\":\"Kellyann\",\n        \"last_name\":\"Dodgshun\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"80.247.105.228\"\n    },\n    {\n        \"email\":\"kbaressf@geocities.jp\",\n        \"first_name\":\"Karly\",\n        \"last_name\":\"Baress\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"145.232.153.145\"\n    },\n    {\n        \"email\":\"amillinsg@com.com\",\n        \"first_name\":\"Adelaide\",\n        \"last_name\":\"Millins\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"75.160.220.182\"\n    },\n    {\n        \"email\":\"skemsleyh@quantcast.com\",\n        \"first_name\":\"Shir\",\n        \"last_name\":\"Kemsley\",\n        \"gender\":\"Male\",\n        \"ip_address\":\"234.195.73.177\"\n    },\n    {\n        \"email\":\"kchomiszewskii@simplemachines.org\",\n        \"first_name\":\"Kristo\",\n        \"last_name\":\"Chomiszewski\",\n        \"gender\":\"Female\",\n        \"ip_address\":\"60.91.73.198\"\n    },\n    {\n        \"email\":\"rkelwickj@baidu.com\",\n        \"first_name\":\"Rosemonde\",\n        \"last_name\":\"Kelwick\",\n        \"gender\":\"Genderfluid\",\n        \"ip_address\":\"42.50.134.65\"\n    }\n]\n\nfrom pyspark.sql import Row\nusers2_df = spark.createDataFrame([Row(**user) for user in users2])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"94d3a087-53d7-4154-9ae7-f6ea635db9fd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Using alias\nfrom pyspark.sql.functions import coalesce\nusers1_df.alias('u1'). \\\n    join(users2_df.alias('u2'), users1_df.email == users2_df.email, 'full'). \\\n    select(\n        coalesce('u1.email', 'u2.email').alias('email'),\n        coalesce('u1.first_name', 'u2.first_name').alias('first_name'),\n        coalesce('u1.last_name', 'u2.last_name').alias('last_name'),\n        coalesce('u1.gender', 'u2.gender').alias('gender'),\n        coalesce('u1.ip_address', 'u2.ip_address').alias('ip_address'),\n    ). \\\n    show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"27d896f9-dcaa-4380-8adb-4996b85b73ae","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+----------+---------+------+--------------+\n|               email|first_name|last_name|gender|    ip_address|\n+--------------------+----------+---------+------+--------------+\n|afaulconerb@barne...|    Andris|Faulconer|Female|109.40.175.103|\n| alovett0@nsw.gov.au|   Aundrea|   Lovett|  Male|   62.72.1.143|\n+--------------------+----------+---------+------+--------------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+----------+---------+------+--------------+\n|               email|first_name|last_name|gender|    ip_address|\n+--------------------+----------+---------+------+--------------+\n|afaulconerb@barne...|    Andris|Faulconer|Female|109.40.175.103|\n| alovett0@nsw.gov.au|   Aundrea|   Lovett|  Male|   62.72.1.143|\n+--------------------+----------+---------+------+--------------+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["* It is also known as map side as well as replicated join.\n* The smaller data set will be broadcasted to all the executors in the cluster.\n* The size of the smaller data set is driven by `spark.sql.autoBroadcastJoinThreshold`.\n* We can even perform broadcast join when the smaller data set size is greater than `spark.sql.autoBroadcastJoinThreshold` by using `broadcast` function from `pyspark.sql.functions`.\n* We can disable broadcast join by setting `spark.sql.autoBroadcastJoinThreshold` value to 0.\n* If broadcast join is disabled then it will result in reduce side join.\n* Make sure to setup multinode cluster using 28 GB Memory, 4 Cores each. Configure scaling between 2 and 4 nodes. Driver can be of minimum configuration."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04c41a90-57c8-4e7d-958d-a6c89c7c33c2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Default size is 10 MB.\nspark.conf.get('spark.sql.autoBroadcastJoinThreshold')\n# We can disable broadcast join using this approach\nspark.conf.set('spark.sql.autoBroadcastJoinThreshold', '0')\n# We can enable broadcast join using this approach\nspark.conf.set('spark.sql.autoBroadcastJoinThreshold', '1')\n# Resetting to original value\nspark.conf.set('spark.sql.autoBroadcastJoinThreshold', '10485760b')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e3ce4a9-27bf-41b2-8236-a0341a1c63ee","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import broadcast\n\n## note : for importing the broadCast join "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1050e64d-de7d-44ef-85ea-1d10a00efa64","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%%time\n\n# Default will be reduce side join as the size of smaller data set is more than 10 MB (default broadcast size)\nclickstream.join(articles, articles.id == clickstream.curr_id).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72573fc2-fbda-4b7e-a4a1-9bc6f13accaa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%%time\n# We can use broadcast function to override existing broadcast join threshold\n# We can also override by using this code spark.conf.set('spark.sql.autoBroadcastJoinThreshold', '1500m')\nbroadcast(clickstream).join(articles, articles.id == clickstream.curr_id).count()\n\n## note: we use broadcast(dataframe) === to braodcast the dataFrame "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ade9376-9485-422e-b443-bdbb94413659","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## cross join"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ddeb859-14a6-43dd-af5b-fcdd444cb32d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Number of records will be equal to \n# number of records in first data frame multipled by number of records in second data frame\nprint(users_df.crossJoin(courses_df).count())\n\n# Even join with out conditions result in cross join or cartesian product\nusers_df.join(courses_df).count()\n## note : here we have not specified joining condition  so it will be \nusers_df.join(courses_df, how='cross').show(2)\n## all are basically same "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"26596a1d-1c63-4eef-938f-ab3bac7aa20d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"50\n+-------+---------------+--------------+----------------+---------+--------------------+-------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|      user_email|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n+-------+---------------+--------------+----------------+---------+--------------------+-------------------+---------+-------------------+\n|      1|         Sandra|        Karpov|skarpov0@ovh.net|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n|      1|         Sandra|        Karpov|skarpov0@ovh.net|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n+-------+---------------+--------------+----------------+---------+--------------------+-------------------+---------+-------------------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["50\n+-------+---------------+--------------+----------------+---------+--------------------+-------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|      user_email|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n+-------+---------------+--------------+----------------+---------+--------------------+-------------------+---------+-------------------+\n|      1|         Sandra|        Karpov|skarpov0@ovh.net|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n|      1|         Sandra|        Karpov|skarpov0@ovh.net|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n+-------+---------------+--------------+----------------+---------+--------------------+-------------------+---------+-------------------+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# reading dataframe from csv,json,tsf,txt,parquet"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb54890c-db3b-400e-abf4-12f0c1f09d95","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["schema = \"\"\"\n    order_id INT,\n    order_date TIMESTAMP,\n    order_customer_id INT,\n    order_status STRING\n\"\"\"\n\norders1 = spark.read.schema(schema).csv('/FileStore/tables/orders-1.csv')\norders1.show(2)\norders1.printSchema()\norders1.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4b5ef15-7695-4554-9bfe-bc0f234d1dee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n+--------+-------------------+-----------------+---------------+\nonly showing top 2 rows\n\nroot\n |-- order_id: integer (nullable = true)\n |-- order_date: timestamp (nullable = true)\n |-- order_customer_id: integer (nullable = true)\n |-- order_status: string (nullable = true)\n\nOut[65]: [('order_id', 'int'),\n ('order_date', 'timestamp'),\n ('order_customer_id', 'int'),\n ('order_status', 'string')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n+--------+-------------------+-----------------+---------------+\nonly showing top 2 rows\n\nroot\n |-- order_id: integer (nullable = true)\n |-- order_date: timestamp (nullable = true)\n |-- order_customer_id: integer (nullable = true)\n |-- order_status: string (nullable = true)\n\nOut[65]: [('order_id', 'int'),\n ('order_date', 'timestamp'),\n ('order_customer_id', 'int'),\n ('order_status', 'string')]"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /FileStore/tables"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f050747-54b8-4c51-8800-2423064c7330","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/FileStore/tables/Exp_Salaries.csv","Exp_Salaries.csv",1183,1612520390000],["dbfs:/FileStore/tables/PHive/","PHive/",0,0],["dbfs:/FileStore/tables/airtrafic_all_snappy-1.parquet","airtrafic_all_snappy-1.parquet",14654075,1670621745000],["dbfs:/FileStore/tables/airtrafic_all_snappy.parquet","airtrafic_all_snappy.parquet",14654075,1670598033000],["dbfs:/FileStore/tables/annual_enterprise_survey_2019_financial_year_provisional_csv-1.csv","annual_enterprise_survey_2019_financial_year_provisional_csv-1.csv",5134576,1611592031000],["dbfs:/FileStore/tables/annual_enterprise_survey_2019_financial_year_provisional_csv.csv","annual_enterprise_survey_2019_financial_year_provisional_csv.csv",5134576,1611591930000],["dbfs:/FileStore/tables/catsvdogs.xlsx","catsvdogs.xlsx",12943,1612519825000],["dbfs:/FileStore/tables/customer.json","customer.json",3105168,1670598431000],["dbfs:/FileStore/tables/delta_lake.txt","delta_lake.txt",318,1612517335000],["dbfs:/FileStore/tables/movie_dataset.txt","movie_dataset.txt",23432179,1614010505000],["dbfs:/FileStore/tables/orderItems.json","orderItems.json",28655610,1670598265000],["dbfs:/FileStore/tables/orders-1.csv","orders-1.csv",2999944,1670843400000],["dbfs:/FileStore/tables/orders-1.json","orders-1.json",7477339,1670626261000],["dbfs:/FileStore/tables/orders-2.json","orders-2.json",7477339,1670626328000],["dbfs:/FileStore/tables/orders-3.json","orders-3.json",7477339,1670626536000],["dbfs:/FileStore/tables/orders-4.json","orders-4.json",7477339,1670835162000],["dbfs:/FileStore/tables/orders.csv","orders.csv",2999944,1670809503000],["dbfs:/FileStore/tables/orders.json","orders.json",7477339,1670597773000],["dbfs:/FileStore/tables/ordersItems.json","ordersItems.json",28655610,1670835527000],["dbfs:/FileStore/tables/part_00000.csv","part_00000.csv",8548,1670598762000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/Exp_Salaries.csv</td><td>Exp_Salaries.csv</td><td>1183</td><td>1612520390000</td></tr><tr><td>dbfs:/FileStore/tables/PHive/</td><td>PHive/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/airtrafic_all_snappy-1.parquet</td><td>airtrafic_all_snappy-1.parquet</td><td>14654075</td><td>1670621745000</td></tr><tr><td>dbfs:/FileStore/tables/airtrafic_all_snappy.parquet</td><td>airtrafic_all_snappy.parquet</td><td>14654075</td><td>1670598033000</td></tr><tr><td>dbfs:/FileStore/tables/annual_enterprise_survey_2019_financial_year_provisional_csv-1.csv</td><td>annual_enterprise_survey_2019_financial_year_provisional_csv-1.csv</td><td>5134576</td><td>1611592031000</td></tr><tr><td>dbfs:/FileStore/tables/annual_enterprise_survey_2019_financial_year_provisional_csv.csv</td><td>annual_enterprise_survey_2019_financial_year_provisional_csv.csv</td><td>5134576</td><td>1611591930000</td></tr><tr><td>dbfs:/FileStore/tables/catsvdogs.xlsx</td><td>catsvdogs.xlsx</td><td>12943</td><td>1612519825000</td></tr><tr><td>dbfs:/FileStore/tables/customer.json</td><td>customer.json</td><td>3105168</td><td>1670598431000</td></tr><tr><td>dbfs:/FileStore/tables/delta_lake.txt</td><td>delta_lake.txt</td><td>318</td><td>1612517335000</td></tr><tr><td>dbfs:/FileStore/tables/movie_dataset.txt</td><td>movie_dataset.txt</td><td>23432179</td><td>1614010505000</td></tr><tr><td>dbfs:/FileStore/tables/orderItems.json</td><td>orderItems.json</td><td>28655610</td><td>1670598265000</td></tr><tr><td>dbfs:/FileStore/tables/orders-1.csv</td><td>orders-1.csv</td><td>2999944</td><td>1670843400000</td></tr><tr><td>dbfs:/FileStore/tables/orders-1.json</td><td>orders-1.json</td><td>7477339</td><td>1670626261000</td></tr><tr><td>dbfs:/FileStore/tables/orders-2.json</td><td>orders-2.json</td><td>7477339</td><td>1670626328000</td></tr><tr><td>dbfs:/FileStore/tables/orders-3.json</td><td>orders-3.json</td><td>7477339</td><td>1670626536000</td></tr><tr><td>dbfs:/FileStore/tables/orders-4.json</td><td>orders-4.json</td><td>7477339</td><td>1670835162000</td></tr><tr><td>dbfs:/FileStore/tables/orders.csv</td><td>orders.csv</td><td>2999944</td><td>1670809503000</td></tr><tr><td>dbfs:/FileStore/tables/orders.json</td><td>orders.json</td><td>7477339</td><td>1670597773000</td></tr><tr><td>dbfs:/FileStore/tables/ordersItems.json</td><td>ordersItems.json</td><td>28655610</td><td>1670835527000</td></tr><tr><td>dbfs:/FileStore/tables/part_00000.csv</td><td>part_00000.csv</td><td>8548</td><td>1670598762000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["orders2 = spark.read.json('/FileStore/tables/orders-5.json')\norders2.printSchema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa5e11bd-c68b-45c9-ab86-6acced993d46","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[67]: <bound method DataFrame.printSchema of DataFrame[order_customer_id: bigint, order_date: string, order_id: bigint, order_status: string]>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[67]: <bound method DataFrame.printSchema of DataFrame[order_customer_id: bigint, order_date: string, order_id: bigint, order_status: string]>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# converting json data to paraquate"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5695dbd-23d3-419a-8c7b-acf154db1d13","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import getpass\nusername = getpass.getuser()\ninput_dir = '/public/retail_db_json'\noutput_dir = f'/user/{username}/retail_db_parquet'\n\n## note: above we are reading the directories "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d7e337d-130a-42f5-983e-7b6bafc56e3b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## note :\n## write.parquet\nfor file_details in dbutils.fs.ls(input_dir):\n    if not ('.git' in file_details.path or file_details.path.endswith('sql')):\n        print(f'Converting data in {file_details.path} folder from json to parquet')\n        data_set_dir = file_details.path.split('/')[-2]\n        df = spark.read.json(file_details.path)\n        df.coalesce(1).write.parquet(f'{output_dir}/{data_set_dir}', mode='overwrite')\n        \n# here we are reading all the files in directory  in dataFrame \n## then we are using coalesce(1) == ie making single partion for one dataFrame and writng this to .write.paraquet(f'{}',mode='overwrite')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58245f64-c116-4bc4-83bb-e53f188e43d3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## # Generate CSV files with pipe delimiter"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"94085641-7dac-47ec-9f8f-dbad93c4b0fc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for file_details in dbutils.fs.ls(input_dir):\n    if 'git' not in file_details.path and 'sql' not in file_details.path:\n        print(f'Converting data in {file_details.path} folder from comma separated to pipe separated')\n        df = spark.read.csv(file_details.path)\n        folder_name = file_details.path.split('/')[-2]\n        df.coalesce(1).write.mode('overwrite').csv(f'{output_dir}/{folder_name}', sep='|')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b021fa74-e389-4e84-b5c5-27a40bb0658f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Copy all the data with comma separator to pipe separator\nimport getpass\nusername = getpass.getuser()\n\ninput_dir = '/public/retail_db'\noutput_dir = f'/user/{username}/retail_db_pipe'\ndbutils.fs.ls('/public/retail_db')\n\n## note: specifing the folder"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe34420d-324d-4f08-8070-2cb8867b231e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Generate CSV files with pipe delimiter\nfor file_details in dbutils.fs.ls(input_dir):\n    if 'git' not in file_details.path and 'sql' not in file_details.path:\n        print(f'Converting data in {file_details.path} folder from comma separated to pipe separated')\n        df = spark.read.csv(file_details.path)\n        folder_name = file_details.path.split('/')[-2]\n        df.coalesce(1).write.mode('overwrite').csv(f'{output_dir}/{folder_name}', sep='|')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c4bd4cf-85e9-492e-b86d-c3ffa4653b7c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Overview of Reading Data Files into Spark Data Frames"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc77b092-e481-4b10-a087-6052b8fd3ff8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["## note: important \n##Spark.read.format('<foramtName>').option(<>,<>).load(<path of file/folder>)\n# Specifying options as arguments as well as using functions such as `option` and `options`.\n\n## files can directly read : csv,text , json,paraquet,orc \n## ie by using spark.read.json(),spark.read.csv(),spark.read.parquet(),spark.read.orc('').\n## %fs ls <path> === cheach files in DBFS \n# * Check if the files are compressed (gz, snappy, bz2, etc). Most common ones are gz and snappy."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20274fb8-638f-44b1-a16c-d71cccbbc446","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["reading csv file \n* Approach 1: spark.read.csv('path_to_folder')\n* Approach 2: spark.read.format('csv').load('path_to_folder')\n* We can explicitly specify the schema as string or using StructType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03fdbe6f-92cc-4cc5-9806-0388147cdf2c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Default behavior\n# It will delimit the data using comma as separator\n# Column names will be system generated\n# All the fields will be of type strings\norders = spark.read.csv('/public/retail_db/orders')\n\n# note:  deafault give column Name as _c ..."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6bb12a6a-db68-4239-9e4d-baac44a7d02c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## reading with scheam \nschema = \"\"\"\n    order_id INT,\n    order_date TIMESTAMP,\n    order_customer_id INT,\n    order_status STRING\n\"\"\"\ndf=spark.read.csv(<path>,schema=schema)\ndf.types\n## ddl style reading schema "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c047fdb8-53fd-449e-bfcd-7b5b922c748c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## note: need to add files \nfrom pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType\nschema = StructType([\n    StructField('order_id', IntegerType(), nullable=False),\n    StructField('order_date', TimestampType(), nullable=False),\n    StructField('order_customer_id', IntegerType(), nullable=False),\n    StructField('order_status', StringType(), nullable=False)\n])\n\n## note: here nullable indicate wheteher columns can have null or not \nspark.read.csv('/public/retail_db/orders', schema=schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50a96e10-3545-410a-81dd-35604a3bd919","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## using .toDF \ncolumns = ['order_id', 'order_date', 'order_customer_id', 'order_status']\nspark.read.option('inferSchema', True).csv('/public/retail_db/orders').dtypes\nspark.read.option('inferSchema', True).csv('/public/retail_db/orders').toDF(*columns)\n\n## * used for unpacking the columns "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd617f0e-97e7-4221-b47a-175ac071477e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## specifing the delimiter \nspark.read.csv(f'/user/{username}/retail_db_pipe/orders', sep='|', schema=schema).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1123910-039a-4447-be9f-3f3be2c0dd5f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## with options \norders = spark. \\\n    read. \\\n    csv(\n        f'/user/{username}/retail_db_pipe/orders',\n        sep='|',\n        header=None,\n        inferSchema=True\n    ). \\\n    toDF('order_id', 'order_date', 'order_customer_id', 'order_status')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14af7f20-8e08-4192-8831-645af6484e58","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## with options 2\norders = spark. \\\n    read. \\\n    option('sep', '|'). \\\n    option('header', None). \\\n    option('inferSchema', True). \\\n    csv(f'/user/{username}/retail_db_pipe/orders'). \\\n    toDF('order_id', 'order_date', 'order_customer_id', 'order_status')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6dc8784d-d84e-4c2d-b617-b52f96899a91","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## best way to add\noptions = {\n    'sep': '|',\n    'header': None,\n    'inferSchema': True\n}\n\norders = spark. \\\n    read. \\\n    options(**options). \\\n    csv(f'/user/{username}/retail_db_pipe/orders'). \\\n    toDF('order_id', 'order_date', 'order_customer_id', 'order_status')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"faa4b227-f29a-4124-ac63-6dfdc7cf47da","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# reading json files"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fece6f30-e871-4ffb-ac3e-f5a05df04d9a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.read.format('json').load('/public/retail_db_json/orders')\n\n## or \ndf = spark.read.json(r'')\n\n# Able to type cast order_id and order_customer_id to int\n# Able to type case order_date to string\nspark.read.json('/public/retail_db_json/orders', schema=schema).dtypes\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60a567a4-05bf-4cb7-89fd-65abca531621","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema = StructType([\n    StructField('order_id', IntegerType()),\n    StructField('order_date', TimestampType()),\n    StructField('order_customer_id', IntegerType()),\n    StructField('order_status', StringType())\n])\nspark.read.json('/public/retail_db_json/orders', schema=schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e435d844-6d9e-4ade-84b8-cbfd321ae45c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ## why not to infer schema \n# * If inferSchema is used entire data need to be read to infer the schema accurately while creating the Data Frame.\n# * If the data size is too big then additional time will be spent to infer the schema.\n# * When we explicitly specify the schema, data will not be read while creating the Data Frame.\n# * As we have seen we should be able to explicitly specify the schema using string or StructType.\n# * Inferring Schema will come handy to quickly understand the structure of the data as part of proof of concepts as well as design.\n# * Schema will be inferred by default for files of type JSON, Parquet and ORC. Column names and data types will be inferred using metadata that will be associated with these types of files.\n# * Inferring the schema on CSV files will create data frames with system generated column names. If inferSchema is used, then the data frame will determine the data types. If the files contain header, then column names can be inherited using it. If not, we need to explicitly pass the columns using `toDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4fd89d9b-cc1a-4300-b7aa-fca29b19b77f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#reading paraquet file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8b9dd31-0f37-4609-8804-1941bba24c38","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.read.parquet(f'/user/{username}/retail_db_parquet/orders')\ndf.inputFiles()\n\n## note: df.inputFiles gives the loaction of file "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f28d3bd-672e-4f0c-b2e9-e930e4a4e6b9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## with schema \nschema1 = \"\"\"\n    order_id INT,\n    order_date TIMESTAMP,\n    order_customer_id INT,\n    order_status STRING\n\"\"\"\n\nschema = StructType([\n    StructField('order_id', LongType()),\n    StructField('order_date', StringType()),\n    StructField('order_customer_id', LongType()),\n    StructField('order_status', StringType())\n])\n\ndf =spark.read.paraquet(r'<file>',schema=scheam1)\n\ndf2 =spark.read.paraquet(r'<file>',schema=scheam2)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de6baa1b-f8df-4c91-b4a5-597fc4bbe9dc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# writing the files in folders"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82a2dac2-99c0-4a3d-bfff-cb8827b94180","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["## note: very imp\n## we can write the files using spark.write.<mode>(r<fileLocation>,mode='overwrite/append/ignore/error')\n## we can also write files using spark.write.format(<json/paraquet/csv>).save(r<path>,mode='overwrite/append/')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0a43ed2-ba2c-40d9-b3ae-0881fb74c7e1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Note: while writing the files to dataFrame\n# * Make sure to analyze the schema of the Data Frame.\n# * Make sure you have write permissions on the target location.\n# * Understand whether you want to overwrite or append or ignore or throw exception in case target folder already exists.\n# * Decide whether you would like to compress the data or not.\n# * Make sure you understand whether the files will be compressed or not by default.\n# * Use appropriate APIs along with right arguments based up on the requirements."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"94ea9c46-2074-4537-8a6d-d188705b1abe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## writing the csv file \n\n\n#     We can write the data from Spark Data Frame into CSV files using multiple approaches.\n#     Approach 1: df.write.csv('path_to_folder')\n#     Approach 2: df.write.format('csv').save('path_to_folder')\n#     The column names from the schema can be added as header to each of the files by saying header=True.\n#     We can also write the data into files using characters other than comma as delimiters or separators.\n#     We can also compress the data while writing into files using csv.\n\ncourses_df.write.format('csv').save(f'/user/{username}/courses')\n\ncourses_df.coalesce(1).write.csv(f'/user/{username}/courses', mode='overwrite', header=True)\n\n## note : coalesce(1) will write as single partion file \n## note: partitions will create multiple part file partions \n\n\n## for using comprensions \n## here we give compression == <format> \n\ncourses_df.coalesce(1).write.format('csv').save(f'/user/{username}/courses',mode='overwrite',compression='snappy',header=True)\n\n\n## specifing delimiter while writing the file  \n## converting csv to text file with delimiters \ndf.coalesce(1).write.mode('overwrite').csv(f'{output_dir}/orders', sep='|')\n\n\n## using options while writing the csv file \norders.coalesce(1).write.mode('overwrite').option('compression', 'gzip').option('header', True).option('sep', '|').csv(f'{output_dir}/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1da691e2-59cb-4fdc-90cd-58f6d321da7b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### db utils commands handy\n* dbutils.fs.ls(f'/user/{username}/courses')\n* dbutils.fs.rm(f'/user/{username}/courses', recurse=True) === to remove the files recursively"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"666df041-f049-44ad-b1f1-00953e851b6f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# writing file to json files"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"faf1d864-b510-401e-bde3-a54a526a3ca9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["orders.coalesce(1).write.mode('overwrite').option('compression', 'gzip').option('header', True).option('sep', '|').csv(f'{output_dir}/orders')\n## note with options \ncourses_df.coalesce(1).write.json( f'/user/{username}/courses',mode='overwrite')\n\ncourses_df.coalesce(1).write.json(f'/user/{username}/courses',mode='overwrite',compression='gzip')\n\n## with compression technique "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cdcc17fd-5ed4-4da2-9493-625ee4da81b9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# writing in to paraquet file format"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebc1ccdd-5656-492b-8ab9-69d438dec907","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["courses_df.coalesce(1).write.parquet(f'/user/{username}/courses', mode='overwrite')\n#or \ncourses_df.coalesce(1).write.format('parquet').save(f'/user/{username}/courses', mode='overwrite')\n\n## with comprehension \ncourses_df.coalesce(1).write.parquet(f'/user/{username}/courses',mode='overwrite',compression='gzip')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9de29249-4397-4d66-876b-7e682096b510","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# modes to write the file in spark \n%md\n\nDifferent ways mode can be specified while writing data frame into files. `file_format` can be any valid out of the box format such as `text`, `csv`, `json`, `parquet`, `orc`.\n* `courses_df.write.mode(saveMode).file_format(path_to_folder)`\n* `courses_df.write.file_format(path_to_folder, mode=saveMode)`\n* `courses_df.write.mode(saveMode).format('file_format').save(path_to_folder)`\n* `courses_df.write.format('file_format').save(path_to_folder, mode=saveMode)`\n * mode = default is error ie it will through error if folder exist , \n * mode have save , format,overite,error if exit , ignore  === into files\n\n* ie modes like append ,overrite ... will work on default file format such as csv,json,parquet"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f43a75ed-a28e-4f8c-a052-74c53235d2e8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["* Understand default behavior.\n  * Fails if folder exists.\n  * Creates folder and then adds files to it.\n  \n*  important note: spark can read entire folders  ie all files at a time while reading"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9b21786f-5f9f-44ef-a5fe-a5b754a1052d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dbutils.fs.rm(f'/user/{username}/courses', recurse=True)\n## note: recursive true will remove all the files in folder"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfcd1406-2cfd-4bc4-a861-150210137cf1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# controlling the no of files in folder\n* by repartition === reduce or increase the no of partions  === incure shuffling \n* and coalsce === reduce no of partions but dont gurantee same size of patitions === no shufflings === data locality principle \n* .rdd.getNumberOfPartitions() === get no of partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc74e36d-132d-43d9-83d9-b527aa4e4979","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# prtitioning with columns \n * note: repartition  === .repartion() ==controll the no of file \n  * note: partitioning === .partionBy() === column prunning === ie read data by filter which is only required\n  \n  ## note for partitonBy \n  * write.json() dont have partitonBy keyword argument so we need to use partitionBy(<colName>) befor write.json \n  * parquet have partitionBy keyword argument \n  \n  ## note: only parquet have partitonBy(col) in keyword argument while writing the column via prunning"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4024ceb5-f3a8-4c77-b8d7-5967f14d9a3f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.repartition(16).write.mode('overwrite').csv(f'/user/{username}/airlines', header=True, compression='gzip')\n## repartions will create 16 part files \ndf.coalesce(16).write.mode('overwrite').csv(f'/user/{username}/airlines', header=True, compression='gzip')\n## controlling the no of files with repartitons and coalsece "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da016515-66c3-4162-90f2-1048b775322c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## partitonby will write the files according to the partitions \norders.withColumn('order_month', date_format('order_date', 'yyyyMM')).coalesce(1).write.partitionBy('order_month').\\\nparquet(f'/user/{username}/retail_db/orders_partitioned_by_month')\n\n## this will create the partions folders according to order_month columns  and no of files within partion folder will controlled by repartitons and coalsce \n\norders.withColumn('order_month', date_format('order_date', 'yyyyMM')).partitionBy('order_month')..coalesce(1).write.csv(f'/user/{username}/retail_db/orders_partitioned_by_month')\n## for other files use above commands \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b647c336-7c3b-4b4f-ba18-b35be07b1cf7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# reading from partition by column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa6602ec-b6c5-4f08-b955-726fb21e3a84","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.read.csv('dbfs:/databricks-datasets/asa/airlines', header=True).filter('Year = 2004').count()\n\n## reading only year partition data folder data "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39f21ccd-85a1-403f-ab60-7f6906ad9189","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["%md\n\n* Spark provides robust set of pre-defined functions as part of `pyspark.sql.functions`.\n* However, they might not fulfill all our requirements.\n* At times, we might have to develop custom UDFs for these scenarios.\n  * No function available for our requirement while applying row level transformations.\n  * Also, we might have to use multiple functions due to which readability of the code is compromised.\n\n\nHere are the steps we need to follow to develop and use Spark User Defined Functions.\n* Develop required logic using Python as programming language.\n* Register the function using `spark.udf.register`. Also assign it to a variable.\n* Variable can be used as part of Data Frame APIs such as `select`, `filter`, etc.\n* When we register, we register with a name. That name can be used as part of `selectExpr` or as part of Spark SQL queries using `spark.sql`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04c652e5-cf08-465b-a5f9-7957e450d46f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["## spark.udf.register(name for spark cataloge ,function ) == registering the udf functions \n\n## we use UDF for code readablity , find below python code \n\ndc = spark.udf.register('date_convert', lambda d: int(d[:10].replace('-', '')))\n\n## here dc is spark udf function  register as date_convert \n\ndf = spark.read.json('/FileStore/tables/orders-5.json')\ndf.select('order_date').show(2)\n\ndf.select(dc('order_date')).show(2)\n\n\n## although we can alternativly "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24a4c4a1-f194-4351-bfe1-821ecb99d51c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# cleaning the data with spark UDF"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03187473-1eb1-4fd4-961c-83a88f1e78c8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pandas as pd\n\ncourses = {'course_id': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n           'course_name': ['Mastering SQL', 'Streaming Pipelines - Python', 'Head First Python',\n                           'Designing Data-Intensive Applications', 'Distributed Systems', 'Database Internals',\n                           'Art of Immutable Architecture', 'Graph Databases', 'Building MicroServices',\n                           'Kubernetes Patterns'],\n           'course_author': ['Mike Jack', 'Bob Davis', 'Elvis Presley', 'Martin Kleppmann', 'Sukumar Ghosh',\n                             'Alex Petrov',\n                             'Michael L. Perry', 'Ian Robinson', 'Sam Newman', 'Rolan Hub'],\n           'course_status': ['   published   ', '   inactive   ', '\\\\N', 'published  ', '\\\\N', '   inactive',\n                             'published   ', '\\\\N', '  inactive ', 'published   '],\n           'course_published_dt': ['2020-07-08', '2020-03-10', '\\\\N', '2021-02-27', '\\\\N', '2021-05-14',\n                                   '2021-04-18', '\\\\N',\n                                   '2020-12-15', '2021-07-11']}\n\ncourses_df = spark.createDataFrame(pd.DataFrame(courses))\n\n\nusers = {'user_id': ['1001', '1002', '1003', '1004', '1005', '1006'],\n         'user_name': ['BenJohnson   ', '  Halley Battles ', '  Laura Anderson  ', '  Rolanda Garza ',\n                       'Angela Fox  ', 'Kerl Goldinger '],\n         'user_email': ['benjohn@gmail.com', '\\\\N', '\\\\N', 'garza.roland@gmail.com', 'nshaiary@aol.com',\n                        'k.gold@live.com1'],\n         'user_gender': ['Male', 'Male', 'Female', 'Male', 'Female', 'Male']}\n\nusers_df = spark.createDataFrame(pd.DataFrame(users))\n\n\ncourse_enrolments = {'course_id': ['3', '5', '8', '5', '6', '8', '7', '3'],\n                     'user_id': ['1001', '1001', '1003', '1003', '1005', '1006', '1001', '1001'],\n                     'enrollment_id': ['9010', '9020', '9030', '9040', '9050', '9060', '9070', '9080'],\n                     'grade': ['A', '\\\\N', 'A', '\\\\N', 'B', 'C', '\\\\N', 'A'],\n                     'department': ['AI  ', 'ML', '  CS', '  DS', '  AI', 'ML', '  CS', 'DS  ']}\n\ncourse_enrolments_df = spark.createDataFrame(pd.DataFrame(course_enrolments))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ac3d53c-e51d-4550-bc1b-2745d1b86a75","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def data_cleanse(c):\n    return c.strip() if c.strip() != '\\\\N' else None\n  \ndata_cleanse = spark.udf.register('data_cleanse', data_cleanse)\n\ncourses_df.show(15,False)\n\n## here we can see the data in course_status  being dirty data not in std Format\n## we are cleaning \\\\n with null  and removing spaces with strip functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3969c5b9-0219-4efc-b6b0-ca60bd7935b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+-------------------------------------+----------------+---------------+-------------------+\n|course_id|course_name                          |course_author   |course_status  |course_published_dt|\n+---------+-------------------------------------+----------------+---------------+-------------------+\n|1        |Mastering SQL                        |Mike Jack       |   published   |2020-07-08         |\n|2        |Streaming Pipelines - Python         |Bob Davis       |   inactive    |2020-03-10         |\n|3        |Head First Python                    |Elvis Presley   |\\N             |\\N                 |\n|4        |Designing Data-Intensive Applications|Martin Kleppmann|published      |2021-02-27         |\n|5        |Distributed Systems                  |Sukumar Ghosh   |\\N             |\\N                 |\n|6        |Database Internals                   |Alex Petrov     |   inactive    |2021-05-14         |\n|7        |Art of Immutable Architecture        |Michael L. Perry|published      |2021-04-18         |\n|8        |Graph Databases                      |Ian Robinson    |\\N             |\\N                 |\n|9        |Building MicroServices               |Sam Newman      |  inactive     |2020-12-15         |\n|10       |Kubernetes Patterns                  |Rolan Hub       |published      |2021-07-11         |\n+---------+-------------------------------------+----------------+---------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+-------------------------------------+----------------+---------------+-------------------+\n|course_id|course_name                          |course_author   |course_status  |course_published_dt|\n+---------+-------------------------------------+----------------+---------------+-------------------+\n|1        |Mastering SQL                        |Mike Jack       |   published   |2020-07-08         |\n|2        |Streaming Pipelines - Python         |Bob Davis       |   inactive    |2020-03-10         |\n|3        |Head First Python                    |Elvis Presley   |\\N             |\\N                 |\n|4        |Designing Data-Intensive Applications|Martin Kleppmann|published      |2021-02-27         |\n|5        |Distributed Systems                  |Sukumar Ghosh   |\\N             |\\N                 |\n|6        |Database Internals                   |Alex Petrov     |   inactive    |2021-05-14         |\n|7        |Art of Immutable Architecture        |Michael L. Perry|published      |2021-04-18         |\n|8        |Graph Databases                      |Ian Robinson    |\\N             |\\N                 |\n|9        |Building MicroServices               |Sam Newman      |  inactive     |2020-12-15         |\n|10       |Kubernetes Patterns                  |Rolan Hub       |published      |2021-07-11         |\n+---------+-------------------------------------+----------------+---------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60ff0859-8647-4d2d-9942-307b72fa7a7f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## applying spark udf function for data cleaning \ncourses_df.select(\n    data_cleanse(col('course_id')).alias('course_id'),\n    data_cleanse(col('course_status').alias('course_status'))\n).show()\n\n## with spark api \n\ncourses_df.createOrReplaceTempView('courses')\n\nspark.sql('''\n    SELECT course_id, data_cleanse(course_status) AS course_status\n    FROM courses\n'''). \\\n    show()\n\n## with spark sql "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f55d1725-e40b-4188-9bec-a388931ab0da","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+--------------------------------------------+\n|course_id|data_cleanse(course_status AS course_status)|\n+---------+--------------------------------------------+\n|        1|                                   published|\n|        2|                                    inactive|\n|        3|                                        null|\n|        4|                                   published|\n|        5|                                        null|\n|        6|                                    inactive|\n|        7|                                   published|\n|        8|                                        null|\n|        9|                                    inactive|\n|       10|                                   published|\n+---------+--------------------------------------------+\n\n+---------+-------------+\n|course_id|course_status|\n+---------+-------------+\n|        1|    published|\n|        2|     inactive|\n|        3|         null|\n|        4|    published|\n|        5|         null|\n|        6|     inactive|\n|        7|    published|\n|        8|         null|\n|        9|     inactive|\n|       10|    published|\n+---------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------------------------------------------+\n|course_id|data_cleanse(course_status AS course_status)|\n+---------+--------------------------------------------+\n|        1|                                   published|\n|        2|                                    inactive|\n|        3|                                        null|\n|        4|                                   published|\n|        5|                                        null|\n|        6|                                    inactive|\n|        7|                                   published|\n|        8|                                        null|\n|        9|                                    inactive|\n|       10|                                   published|\n+---------+--------------------------------------------+\n\n+---------+-------------+\n|course_id|course_status|\n+---------+-------------+\n|        1|    published|\n|        2|     inactive|\n|        3|         null|\n|        4|    published|\n|        5|         null|\n|        6|     inactive|\n|        7|    published|\n|        8|         null|\n|        9|     inactive|\n|       10|    published|\n+---------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# advance sql trnsformations \n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Use **`.`** and **`:`** syntax to query nested data\n- Work with JSON\n- Flatten and unpacking arrays and structs\n- Combine datasets using joins and set operators\n- Reshape data using pivot tables\n- Use higher order functions for working with arrays"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"148556a1-b417-4145-a2bc-f710952485bc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark SQL has built-in functionality to directly interact with JSON data stored as strings. We can use the **`:`** syntax to traverse nested data structures."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"437a723f-39b9-4e55-835c-b1f22a046f0f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark SQL also has the ability to parse JSON objects into struct types (a native Spark type with nested attributes).\n\nHowever, the from_json function requires a schema. To derive the schema of our current data, we'll start by executing a query we know will return a JSON value with no null fields.\n\n\nSpark SQL also has a **`schema_of_json`** function to derive the JSON schema from an example. Here, we copy and paste an example JSON to the function and chain it into the **`from_json`** function to cast our **`value`** field to a struct type."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7732359c-38a5-44e7-b966-08ff9b9cba4c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark SQL also has a **`schema_of_json`** function to derive the JSON schema from an example. Here, we copy and paste an example JSON to the function and chain it into the **`from_json`** function to cast our **`value`** field to a struct type.\n\n* schema_od_json(value,from_json({<sampleJson>})) === this will give schema of json in string format"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f315975-1349-4577-bf7c-eff97917f224","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["*# ##CREATE OR REPLACE TEMP VIEW parsed_events AS\n*#   SELECT from_json(value, schema_of_json('{\"device\":\"Linux\",\"ecommerce\":**{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}')) AS json \n*   FROM events_strings;\n  \n* SELECT * FROM parsed_events\n\n*## note : in above example we have created view  named parsed_event from table event_string and now we can directly query them."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e49e9ce7-e193-4720-8760-fb90468d277b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["## SAMPLE QUERY TO QUERY NESTED JSON AS STRING\n# SELECT value:device, value:geo:city \n# FROM events_strings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c81468a3-96f5-48ce-aa39-ec0e9af5edf2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["%md\n\n \n## Collect Arrays\n\nThe **`collect_set`** function can collect unique values for a field, including fields within arrays.\n\nThe **`flatten`** function allows multiple arrays to be combined into a single array.\n\nThe **`array_distinct`** function removes duplicate elements from an array.\n\nHere, we combine these queries to create a simple table that shows the unique collection of actions and the items in a user's cart."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a7ae4281-e457-4434-979e-c436b8c4a1ac","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Higher Order Functions\nHigher order functions in Spark SQL allow you to work directly with complex data types. When working with hierarchical data, records are frequently stored as array or map type objects. Higher-order functions allow you to transform data while preserving the original structure.\n\nHigher order functions include:\n- **`FILTER`** filters an array using the given lambda function.\n- **`EXIST`** tests whether a statement is true for one or more elements in an array. \n- **`TRANSFORM`** uses the given lambda function to transform all elements in an array.\n- **`REDUCE`** takes two lambda functions to reduce the elements of an array to a single value by merging the elements into a buffer, and the apply a finishing function on the final buffer."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddba7525-b0df-4a72-906d-9de262fe3b40","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Transform\nBuilt-in functions are designed to operate on a single, simple data type within a cell; they cannot process array values. **`TRANSFORM`** can be particularly useful when you want to apply an existing function to each element in an array. \n\nCompute the total revenue from king-sized items per order.\n\n**`TRANSFORM(king_items, k -> CAST(k.item_revenue_in_usd * 100 AS INT)) AS item_revenues`**\n\nIn the statement above, for each value in the input array, we extract the item's revenue value, multiply it by 100, and cast the result to integer. Note that we're using the same kind as references as in the previous command, but we name the iterator with a new variable, **`k`**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"13de6f51-a7bc-482f-b6dd-6a252b6d5236","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"data_bricks_associate_developer_exam_basic6.ipynb","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1564495909092797,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":1564495909092771}},"nbformat":4,"nbformat_minor":0}
