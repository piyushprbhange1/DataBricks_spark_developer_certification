{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a11d49a4-8298-4477-8323-b092f8d4d570","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[5]: DataFrame[value: int]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[5]: DataFrame[value: int]","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["## create dataFrame from list \n","ages_list = [21, 23, 18, 41, 32]\n","type(ages_list)\n","# spark.createDataFrame(ages_list)\n","## this will give error for schema \n","spark.createDataFrame(ages_list, 'int')\n","## error will be for interger \n","from pyspark.sql.types import IntegerType\n","spark.createDataFrame(ages_list, IntegerType())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3e805c89-fd4b-44bc-8ba6-d7df6f3c1984","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[10]: DataFrame[value: string]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[10]: DataFrame[value: string]","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql.types  import  StringType\n","names_list = ['Scott', 'Donald', 'Mickey']\n","##by spark dataType \n","spark.createDataFrame(names_list, StringType())\n","spark.createDataFrame(names_list, 'string')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"03e6b61c-c13f-4859-ad01-aabe4958fce0","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[19]: DataFrame[user_id: int, user_first_name: string]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[19]: DataFrame[user_id: int, user_first_name: string]","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["### creating multi column data frame from spark \n","ages_list = [(21, ), (23, ), (41, ), (32, )]\n","type(ages_list)\n","type(ages_list[2])\n","## type of 2nd element in list \n","df4=spark.createDataFrame(ages_list, 'age int')\n","df4\n","users_list = [(1, 'Scott'), (2, 'Donald'), (3, 'Mickey'), (4, 'Elvis')]\n","df5=spark.createDataFrame(users_list)\n","df5\n","# display(df5)\n","## here df5 being dataType implicitly taken  with column name given _1,_2 for tuple in list\n","\n","df6=spark.createDataFrame(users_list, 'user_id int, user_first_name string')\n","df6"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fbbb96c9-2e4e-4b12-9663-65a8b69b6cf5","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[22]: list"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[22]: list","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["## overview of spark row \n","users_list = [(1, 'Scott'), (2, 'Donald'), (3, 'Mickey'), (4, 'Elvis')]\n","df7 = spark.createDataFrame(users_list, 'user_id int, user_first_name string')\n","df7.collect()\n","# note .collect() on spark dataFrame will give the rows of type list \n","#  row is class in pyspark.sql  row(* arg ,**keywords )\n","type(df7.collect())\n","## df7 is example of row(*args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"96d3c527-76c8-45e4-abf9-89aa357e5634","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[23]: <Row('Alice', 11)>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[23]: <Row('Alice', 11)>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql import Row\n","# help(Row)\n","r = Row(\"Alice\", 11)\n","r\n","## with row (*arg)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6a4c06a2-a833-44c0-a252-00c2893ec127","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[25]: 'Alice'"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[25]: 'Alice'","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql import Row\n","# help(Row)\n","row2 = Row(name=\"Alice\", age=11)\n","row2\n","## advantage is we will able to access the things via name \n","row2.name\n","row2['name']"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3fd1fa97-baee-48e4-a37d-3df6bc62da47","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>user_first_name</th></tr></thead><tbody><tr><td>1</td><td>Scott</td></tr><tr><td>2</td><td>Donald</td></tr><tr><td>3</td><td>Mickey</td></tr><tr><td>4</td><td>Elvis</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,"Scott"],[2,"Donald"],[3,"Mickey"],[4,"Elvis"]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"user_id","type":"\"integer\""},{"metadata":"{}","name":"user_first_name","type":"\"string\""}],"type":"table"}},"output_type":"display_data"}],"source":["## converting list of list to ddataframe using row \n","users_list = [[1, 'Scott'], [2, 'Donald'], [3, 'Mickey'], [4, 'Elvis']]\n","type(users_list[1])\n","## accessing the first element in list\n","df8=spark.createDataFrame(users_list, 'user_id int, user_first_name string')\n","df8\n","display(df8)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6be96ffc-6e7c-4147-8f3a-dba3ce99e733","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["[<Row(1, 'Scott')>, <Row(2, 'Donald')>, <Row(3, 'Mickey')>, <Row(4, 'Elvis')>]\n","<class 'list'>\n","<class 'pyspark.sql.types.Row'>\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"[<Row(1, 'Scott')>, <Row(2, 'Donald')>, <Row(3, 'Mickey')>, <Row(4, 'Elvis')>]\n<class 'list'>\n<class 'pyspark.sql.types.Row'>\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql import Row\n","users_list = [[1, 'Scott'], [2, 'Donald'], [3, 'Mickey'], [4, 'Elvis']]\n","users_rows = [Row(*user) for user in users_list]\n","print(users_rows)\n","\n","## here we have convert list of list to list of rows then created dataFrame * row will create rows \n","print(type(users_list[1]))\n","print(type(users_rows[1]))\n","df9=spark.createDataFrame(users_rows, 'user_id int, user_first_name string')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b523f3ba-8877-4c94-8159-ebe73635db81","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["(1,)\n","1\n","(1, 'Hello')\n","2\n","None\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"(1,)\n1\n(1, 'Hello')\n2\nNone\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["## creating function \n","def dummy(*args):\n","    print(args)\n","    print(len(args))\n","dummy(1)\n","\n","# note * arg will return tuple \n","print(dummy(1, 'Hello'))\n","## created tuple and given length \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c39cb079-5d25-4786-add5-cb21079593c1","showTitle":false,"title":""}},"outputs":[],"source":["## conver tuple into rows \n","users_list = [(1, 'Scott'), (2, 'Donald'), (3, 'Mickey'), (4, 'Elvis')]\n","print(type(users_list[1]))\n","df9=spark.createDataFrame(users_list, 'user_id int, user_first_name string')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0270b80b-20f9-4e22-9595-c011aa23efb8","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["(1, 'Scott')\n","2\n","<class 'pyspark.sql.types.Row'>\n","Out[39]: DataFrame[user_id: int, user_first_name: string]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"(1, 'Scott')\n2\n<class 'pyspark.sql.types.Row'>\nOut[39]: DataFrame[user_id: int, user_first_name: string]","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql import Row\n","def dummy(*arg):\n","  print(arg)\n","  print(len(arg))\n","  \n","user_details = (1, 'Scott')\n","dummy(*user_details)\n","## we are using * so that it is taken only as one tuple \n","users_rows = [Row(*user) for user in users_list]\n","print(type(users_rows[1]))\n","df10 =spark.createDataFrame(users_rows, 'user_id int, user_first_name string')\n","df10\n","\n","\n","## note: basically row is list of tuples "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8482525f-f092-4e10-af41-8995636de4df","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[45]: DataFrame[user_first_name: string, user_id: bigint]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[45]: DataFrame[user_first_name: string, user_id: bigint]","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["## coverting dict to dataframe using row\n","users_list = [\n","   {'user_id': 1, 'user_first_name': 'Scott'},\n","   {'user_id': 2, 'user_first_name': 'Donald'},\n","   {'user_id': 3, 'user_first_name': 'Mickey'},\n","   {'user_id': 4, 'user_first_name': 'Elvis'}]\n","df11 = spark.createDataFrame(users_list)\n","df11\n","## disctionary key is take as columns"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f2e3346f-0ace-4691-8f82-a47080e0afb1","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["{'user_id': 2, 'user_first_name': 'Donald'}\n","[<Row(1, 'Scott')>, <Row(2, 'Donald')>, <Row(3, 'Mickey')>, <Row(4, 'Elvis')>]\n","DataFrame[user_id: bigint, New_user_first_name: string]\n","[Row(user_id=1, user_first_name='Scott'), Row(user_id=2, user_first_name='Donald'), Row(user_id=3, user_first_name='Mickey'), Row(user_id=4, user_first_name='Elvis')]\n","<bound method DataFrame.collect of DataFrame[user_id: bigint, user_first_name: string]>\n","{'user_details': {'user_id': 1, 'user_first_name': 'Scott'}}\n","1\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"{'user_id': 2, 'user_first_name': 'Donald'}\n[<Row(1, 'Scott')>, <Row(2, 'Donald')>, <Row(3, 'Mickey')>, <Row(4, 'Elvis')>]\nDataFrame[user_id: bigint, New_user_first_name: string]\n[Row(user_id=1, user_first_name='Scott'), Row(user_id=2, user_first_name='Donald'), Row(user_id=3, user_first_name='Mickey'), Row(user_id=4, user_first_name='Elvis')]\n<bound method DataFrame.collect of DataFrame[user_id: bigint, user_first_name: string]>\n{'user_details': {'user_id': 1, 'user_first_name': 'Scott'}}\n1\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n","\u001b[0;32m<command-1361638322836564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[1;32m     26\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     27\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf14\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     30\u001b[0m \u001b[0;31m## this will create dictionary where user_details will be key and one element in list of dictionary will be dictionary ie dictionary of dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n","\u001b[1;32m    720\u001b[0m             return super(SparkSession, self).createDataFrame(\n","\u001b[1;32m    721\u001b[0m                 data, schema, samplingRatio, verifySchema)\n","\u001b[0;32m--> 722\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n","\u001b[1;32m    752\u001b[0m                 \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    753\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    756\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-1361638322836564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf14\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m## this will create dictionary where user_details will be key and one element in list of dictionary will be dictionary ie dictionary of dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    720\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    721\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: 'NoneType' object is not iterable","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["from pyspark.sql import Row\n","user_details = users_list[1]\n","## list indexing start with 1\n","print(user_details)\n","## gives first row \n","Row(*user_details.values())\n","\n","users_rows = [Row(*user.values()) for user in users_list]\n","print(users_rows)\n","## they will return the tuples\n","df12=spark.createDataFrame(users_rows, 'user_id bigint, New_user_first_name string')\n","print(df12)\n","## here we have given custom schema to all values in dictionary \n","users_rows = [Row(**user) for user in users_list]\n","print(users_rows)\n","## here ** will parse all the key-value in  list of dictionary\n","df13=spark.createDataFrame(users_rows)\n","## here dont have to give the columns name to dictionary \n","print(df13.collect)\n","## by using custom defination functions \n","\n","## working of  ** kwargs \n","def dummy(**kwargs):\n","  print(kwargs)\n","  print(len(kwargs))\n","\n","user_details = {'user_id': 1, 'user_first_name': 'Scott'}\n","a=dummy(user_details=user_details)\n","a\n","## this will create dictionary where user_details will be key and one element in list of dictionary will be dictionary ie dictionary of dictionaries \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b32f5cb7-f69f-4801-ae42-abbae5b79e03","showTitle":false,"title":""}},"outputs":[],"source":["## overview of basic data types in spark \n","import datetime\n","users = [\n","    {\n","        \"id\": 1,\n","        \"first_name\": \"Corrie\",\n","        \"last_name\": \"Van den Oord\",\n","        \"email\": \"cvandenoord0@etsy.com\",\n","        \"is_customer\": True,\n","        \"amount_paid\": 1000.55,\n","        \"customer_from\": datetime.date(2021, 1, 15),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n","    },\n","    {\n","        \"id\": 2,\n","        \"first_name\": \"Nikolaus\",\n","        \"last_name\": \"Brewitt\",\n","        \"email\": \"nbrewitt1@dailymail.co.uk\",\n","        \"is_customer\": True,\n","        \"amount_paid\": 900.0,\n","        \"customer_from\": datetime.date(2021, 2, 14),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n","    },\n","    {\n","        \"id\": 3,\n","        \"first_name\": \"Orelie\",\n","        \"last_name\": \"Penney\",\n","        \"email\": \"openney2@vistaprint.com\",\n","        \"is_customer\": True,\n","        \"amount_paid\": 850.55,\n","        \"customer_from\": datetime.date(2021, 1, 21),\n","        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n","    },\n","    {\n","        \"id\": 4,\n","        \"first_name\": \"Ashby\",\n","        \"last_name\": \"Maddocks\",\n","        \"email\": \"amaddocks3@home.pl\",\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n","    },\n","    {\n","        \"id\": 5,\n","        \"first_name\": \"Kurt\",\n","        \"last_name\": \"Rome\",\n","        \"email\": \"krome4@shutterfly.com\",\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n","    }\n","]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0837b171-13bd-45e5-90f0-c93531207159","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["root\n"," |-- id: long (nullable = true)\n"," |-- first_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- is_customer: boolean (nullable = true)\n"," |-- amount_paid: double (nullable = true)\n"," |-- customer_from: date (nullable = true)\n"," |-- last_updated_ts: timestamp (nullable = true)\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"root\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql import Row\n","users_df = spark.createDataFrame([Row(**user) for user in users])\n","## creating dataframe from list of dictionaries \n","users_df.printSchema()\n","## spark have following data types \n","## long , string , boolean , double,date .timestamp "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"425fb384-4348-4aee-8b9d-b2ad5e274050","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["['id', 'first_name', 'last_name', 'email', 'is_customer', 'amount_paid', 'customer_from', 'last_updated_ts']\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","\n","None\n","[('id', 'bigint'), ('first_name', 'string'), ('last_name', 'string'), ('email', 'string'), ('is_customer', 'boolean'), ('amount_paid', 'double'), ('customer_from', 'date'), ('last_updated_ts', 'timestamp')]\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"['id', 'first_name', 'last_name', 'email', 'is_customer', 'amount_paid', 'customer_from', 'last_updated_ts']\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\nNone\n[('id', 'bigint'), ('first_name', 'string'), ('last_name', 'string'), ('email', 'string'), ('is_customer', 'boolean'), ('amount_paid', 'double'), ('customer_from', 'date'), ('last_updated_ts', 'timestamp')]\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["print(users_df.columns)\n","## note column is of type instance hence we are not using ()\n","print(users_df.show())\n","print(users_df.dtypes)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"385bcf2d-a518-4a79-9c60-1aab2b150ea7","showTitle":false,"title":""}},"outputs":[],"source":["## spaecifing schema as strings \n","import datetime\n","users = [(1,\n","  'Corrie',\n","  'Van den Oord',\n","  'cvandenoord0@etsy.com',\n","  True,\n","  1000.55,\n","  datetime.date(2021, 1, 15),\n","  datetime.datetime(2021, 2, 10, 1, 15)),\n"," (2,\n","  'Nikolaus',\n","  'Brewitt',\n","  'nbrewitt1@dailymail.co.uk',\n","  True,\n","  900.0,\n","  datetime.date(2021, 2, 14),\n","  datetime.datetime(2021, 2, 18, 3, 33)),\n"," (3,\n","  'Orelie',\n","  'Penney',\n","  'openney2@vistaprint.com',\n","  True,\n","  850.55,\n","  datetime.date(2021, 1, 21),\n","  datetime.datetime(2021, 3, 15, 15, 16, 55)),\n"," (4,\n","  'Ashby',\n","  'Maddocks',\n","  'amaddocks3@home.pl',\n","  False,\n","  None,\n","  None,\n","  datetime.datetime(2021, 4, 10, 17, 45, 30)),\n"," (5,\n","  'Kurt',\n","  'Rome',\n","  'krome4@shutterfly.com',\n","  False,\n","  None,\n","  None,\n","  datetime.datetime(2021, 4, 2, 0, 55, 18))]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"dc08e06e-1f11-443f-afb1-47d66eebcb80","showTitle":false,"title":""}},"outputs":[],"source":["users_schema = '''\n","    id INT,\n","    first_name STRING,\n","    last_name STRING,\n","    email STRING,\n","    is_customer BOOLEAN,\n","    amount_paid FLOAT,\n","    customer_from DATE,\n","    last_updated_ts TIMESTAMP\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"14b368c7-2f05-466f-b08e-a3d5117e8f22","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["DataFrame[id: int, first_name: string, last_name: string, email: string, is_customer: boolean, amount_paid: float, customer_from: date, last_updated_ts: timestamp]\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"DataFrame[id: int, first_name: string, last_name: string, email: string, is_customer: boolean, amount_paid: float, customer_from: date, last_updated_ts: timestamp]\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["df14= spark.createDataFrame(users, schema=users_schema)\n","print(df14)\n","df14=spark.createDataFrame(users, schema=users_schema).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d726d49e-ff6c-43d1-9a93-70d4e3dc92b7","showTitle":false,"title":""}},"outputs":[],"source":["## spark special datatypes are array ,map ,struct \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b953281c-1b51-4a2e-89bc-e6e47e59257d","showTitle":false,"title":""}},"outputs":[],"source":["import datetime\n","users = [(1,\n","  'Corrie',\n","  'Van den Oord',\n","  'cvandenoord0@etsy.com',\n","  True,\n","  1000.55,\n","  datetime.date(2021, 1, 15),\n","  datetime.datetime(2021, 2, 10, 1, 15)),\n"," (2,\n","  'Nikolaus',\n","  'Brewitt',\n","  'nbrewitt1@dailymail.co.uk',\n","  True,\n","  900.0,\n","  datetime.date(2021, 2, 14),\n","  datetime.datetime(2021, 2, 18, 3, 33)),\n"," (3,\n","  'Orelie',\n","  'Penney',\n","  'openney2@vistaprint.com',\n","  True,\n","  850.55,\n","  datetime.date(2021, 1, 21),\n","  datetime.datetime(2021, 3, 15, 15, 16, 55)),\n"," (4,\n","  'Ashby',\n","  'Maddocks',\n","  'amaddocks3@home.pl',\n","  False,\n","  None,\n","  None,\n","  datetime.datetime(2021, 4, 10, 17, 45, 30)),\n"," (5,\n","  'Kurt',\n","  'Rome',\n","  'krome4@shutterfly.com',\n","  False,\n","  None,\n","  None,\n","  datetime.datetime(2021, 4, 2, 0, 55, 18))]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"22345653-0bb9-49e0-a662-27528722a370","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n","|id INT|first_name STRING|last_name STRING|        email STRING|is_customer BOOLEAN|amount_paid FLOAT|customer_from DATE|last_updated_ts TIMESTAMP|\n","+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n","|     1|           Corrie|    Van den Oord|cvandenoord0@etsy...|               true|          1000.55|        2021-01-15|      2021-02-10 01:15:00|\n","|     2|         Nikolaus|         Brewitt|nbrewitt1@dailyma...|               true|            900.0|        2021-02-14|      2021-02-18 03:33:00|\n","|     3|           Orelie|          Penney|openney2@vistapri...|               true|           850.55|        2021-01-21|      2021-03-15 15:16:55|\n","|     4|            Ashby|        Maddocks|  amaddocks3@home.pl|              false|             null|              null|      2021-04-10 17:45:30|\n","|     5|             Kurt|            Rome|krome4@shutterfly...|              false|             null|              null|      2021-04-02 00:55:18|\n","+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n|id INT|first_name STRING|last_name STRING|        email STRING|is_customer BOOLEAN|amount_paid FLOAT|customer_from DATE|last_updated_ts TIMESTAMP|\n+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n|     1|           Corrie|    Van den Oord|cvandenoord0@etsy...|               true|          1000.55|        2021-01-15|      2021-02-10 01:15:00|\n|     2|         Nikolaus|         Brewitt|nbrewitt1@dailyma...|               true|            900.0|        2021-02-14|      2021-02-18 03:33:00|\n|     3|           Orelie|          Penney|openney2@vistapri...|               true|           850.55|        2021-01-21|      2021-03-15 15:16:55|\n|     4|            Ashby|        Maddocks|  amaddocks3@home.pl|              false|             null|              null|      2021-04-10 17:45:30|\n|     5|             Kurt|            Rome|krome4@shutterfly...|              false|             null|              null|      2021-04-02 00:55:18|\n+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["users_schema = [\n","    'id INT',\n","    'first_name STRING',\n","    'last_name STRING',\n","    'email STRING',\n","    'is_customer BOOLEAN',\n","    'amount_paid FLOAT',\n","    'customer_from DATE',\n","    'last_updated_ts TIMESTAMP'\n","]\n","\n","df14 =spark.createDataFrame(users, schema=users_schema)\n","df14.show()\n","## here we just parses the list as schema with explisitly schem defination \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e05f1f10-cc33-4d8d-8232-685dd7695708","showTitle":false,"title":""}},"outputs":[],"source":["## specifing schema with spark types \n","import datetime\n","users = [(1,\n","  'Corrie',\n","  'Van den Oord',\n","  'cvandenoord0@etsy.com',\n","  True,\n","  1000.55,\n","  datetime.date(2021, 1, 15),\n","  datetime.datetime(2021, 2, 10, 1, 15)),\n"," (2,\n","  'Nikolaus',\n","  'Brewitt',\n","  'nbrewitt1@dailymail.co.uk',\n","  True,\n","  900.0,\n","  datetime.date(2021, 2, 14),\n","  datetime.datetime(2021, 2, 18, 3, 33)),\n"," (3,\n","  'Orelie',\n","  'Penney',\n","  'openney2@vistaprint.com',\n","  True,\n","  850.55,\n","  datetime.date(2021, 1, 21),\n","  datetime.datetime(2021, 3, 15, 15, 16, 55)),\n"," (4,\n","  'Ashby',\n","  'Maddocks',\n","  'amaddocks3@home.pl',\n","  False,\n","  None,\n","  None,\n","  datetime.datetime(2021, 4, 10, 17, 45, 30)),\n"," (5,\n","  'Kurt',\n","  'Rome',\n","  'krome4@shutterfly.com',\n","  False,\n","  None,\n","  None,\n","  datetime.datetime(2021, 4, 2, 0, 55, 18))]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9046a837-58fd-40f8-a009-237c66a39582","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["DataFrame[id: int, first_name: string, last_name: string, email: string, is_customer: boolean, amount_paid: float, customer_from: date, last_updated_ts: timestamp]\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","\n","None\n","Out[81]: [Row(id=1, first_name='Corrie', last_name='Van den Oord', email='cvandenoord0@etsy.com', is_customer=True, amount_paid=1000.5499877929688, customer_from=datetime.date(2021, 1, 15), last_updated_ts=datetime.datetime(2021, 2, 10, 1, 15)),\n"," Row(id=2, first_name='Nikolaus', last_name='Brewitt', email='nbrewitt1@dailymail.co.uk', is_customer=True, amount_paid=900.0, customer_from=datetime.date(2021, 2, 14), last_updated_ts=datetime.datetime(2021, 2, 18, 3, 33)),\n"," Row(id=3, first_name='Orelie', last_name='Penney', email='openney2@vistaprint.com', is_customer=True, amount_paid=850.5499877929688, customer_from=datetime.date(2021, 1, 21), last_updated_ts=datetime.datetime(2021, 3, 15, 15, 16, 55)),\n"," Row(id=4, first_name='Ashby', last_name='Maddocks', email='amaddocks3@home.pl', is_customer=False, amount_paid=None, customer_from=None, last_updated_ts=datetime.datetime(2021, 4, 10, 17, 45, 30)),\n"," Row(id=5, first_name='Kurt', last_name='Rome', email='krome4@shutterfly.com', is_customer=False, amount_paid=None, customer_from=None, last_updated_ts=datetime.datetime(2021, 4, 2, 0, 55, 18))]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"DataFrame[id: int, first_name: string, last_name: string, email: string, is_customer: boolean, amount_paid: float, customer_from: date, last_updated_ts: timestamp]\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\nNone\nOut[81]: [Row(id=1, first_name='Corrie', last_name='Van den Oord', email='cvandenoord0@etsy.com', is_customer=True, amount_paid=1000.5499877929688, customer_from=datetime.date(2021, 1, 15), last_updated_ts=datetime.datetime(2021, 2, 10, 1, 15)),\n Row(id=2, first_name='Nikolaus', last_name='Brewitt', email='nbrewitt1@dailymail.co.uk', is_customer=True, amount_paid=900.0, customer_from=datetime.date(2021, 2, 14), last_updated_ts=datetime.datetime(2021, 2, 18, 3, 33)),\n Row(id=3, first_name='Orelie', last_name='Penney', email='openney2@vistaprint.com', is_customer=True, amount_paid=850.5499877929688, customer_from=datetime.date(2021, 1, 21), last_updated_ts=datetime.datetime(2021, 3, 15, 15, 16, 55)),\n Row(id=4, first_name='Ashby', last_name='Maddocks', email='amaddocks3@home.pl', is_customer=False, amount_paid=None, customer_from=None, last_updated_ts=datetime.datetime(2021, 4, 10, 17, 45, 30)),\n Row(id=5, first_name='Kurt', last_name='Rome', email='krome4@shutterfly.com', is_customer=False, amount_paid=None, customer_from=None, last_updated_ts=datetime.datetime(2021, 4, 2, 0, 55, 18))]","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql.types import *\n","## importing the data types \n","users_schema = StructType([\n","    StructField('id', IntegerType()),\n","    StructField('first_name', StringType()),\n","    StructField('last_name', StringType()),\n","    StructField('email', StringType()),\n","    StructField('is_customer', BooleanType()),\n","    StructField('amount_paid', FloatType()),\n","    StructField('customer_from', DateType()),\n","    StructField('last_updated_ts', TimestampType())\n","])\n","\n","type(users_schema)\n","\n","df15=spark.createDataFrame(users, schema=users_schema)\n","print(df15)\n","print(df15.show())\n","### converting dataframes to rdd and showing it \n","df15.rdd.collect()\n","## rdd are shown as rows "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"807c1b1f-4c3d-4a19-bf53-c7c658322dd7","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["root\n"," |-- id: long (nullable = true)\n"," |-- first_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- is_customer: boolean (nullable = true)\n"," |-- amount_paid: double (nullable = true)\n"," |-- customer_from: date (nullable = true)\n"," |-- last_updated_ts: timestamp (nullable = true)\n","\n","None\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"root\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\nNone\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n","\u001b[0;32m<command-1361638322836576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[1;32m     52\u001b[0m \u001b[0musers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m---> 54\u001b[0;31m \u001b[0musers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n","\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    504\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n","\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n","\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n","\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    119\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n","\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n","\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2986.showString.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 43.0 failed 1 times, most recent failure: Lost task 1.0 in stage 43.0 (TID 142) (ip-10-172-216-22.us-west-2.compute.internal executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 8 fields are required while 6 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:187)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:212)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:181)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:943)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n","\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n","\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n","\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n","\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n","\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n","\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n","\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n","\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:573)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:582)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:528)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:527)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:424)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:403)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:424)\n","\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3153)\n","\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3959)\n","\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2877)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n","\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2877)\n","\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n","\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:293)\n","\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:332)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n","\tat py4j.Gateway.invoke(Gateway.java:295)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 8 fields are required while 6 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:187)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:212)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:181)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:943)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n","\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\t... 1 more\n"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m<command-1361638322836576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0musers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0musers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2986.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 43.0 failed 1 times, most recent failure: Lost task 1.0 in stage 43.0 (TID 142) (ip-10-172-216-22.us-west-2.compute.internal executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 8 fields are required while 6 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:187)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:212)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:181)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:943)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:573)\n\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:582)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:528)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:527)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:424)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:403)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3153)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3959)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2877)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2877)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:293)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:332)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 8 fields are required while 6 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:187)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:212)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:181)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:943)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 43.0 failed 1 times, most recent failure: Lost task 1.0 in stage 43.0 (TID 142) (ip-10-172-216-22.us-west-2.compute.internal executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 8 fields are required while 6 values are provided.","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["## spark dataframe using pandas dataframe \n","import datetime\n","users = [\n","    {\n","        \"id\": 1,\n","        \"first_name\": \"Corrie\",\n","        \"last_name\": \"Van den Oord\",\n","        \"email\": \"cvandenoord0@etsy.com\",\n","        \"is_customer\": True,\n","        \"amount_paid\": 1000.55,\n","        \"customer_from\": datetime.date(2021, 1, 15),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n","    },\n","    {\n","        \"id\": 2,\n","        \"first_name\": \"Nikolaus\",\n","        \"last_name\": \"Brewitt\",\n","        \"email\": \"nbrewitt1@dailymail.co.uk\",\n","        \"is_customer\": True,\n","        \"amount_paid\": 900.0,\n","        \"customer_from\": datetime.date(2021, 2, 14),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n","    },\n","    {\n","        \"id\": 3,\n","        \"first_name\": \"Orelie\",\n","        \"last_name\": \"Penney\",\n","        \"email\": \"openney2@vistaprint.com\",\n","        \"is_customer\": True,\n","        \"amount_paid\": 850.55,\n","        \"customer_from\": datetime.date(2021, 1, 21),\n","        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n","    },\n","    {\n","        \"id\": 4,\n","        \"first_name\": \"Ashby\",\n","        \"last_name\": \"Maddocks\",\n","        \"email\": \"amaddocks3@home.pl\",\n","        \"is_customer\": False,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n","    },\n","    {\n","        \"id\": 5,\n","        \"first_name\": \"Kurt\",\n","        \"last_name\": \"Rome\",\n","        \"email\": \"krome4@shutterfly.com\",\n","        \"is_customer\": False,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n","    }\n","]\n","from pyspark.sql import Row\n","users_df = spark.createDataFrame([Row(**user) for user in users])\n","print(users_df.printSchema())\n","users_df.show()\n","## here we are geting error  because first element in list of dictionary used to create schema for dataframe and other element dont have that elements present \n","## note advantages of creating dataframe from pandas this values are printed as null 9"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0745e280-6030-44e9-a323-eee9516207bf","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["   id first_name     last_name                      email  is_customer  \\\n","0   1     Corrie  Van den Oord      cvandenoord0@etsy.com         True   \n","1   2   Nikolaus       Brewitt  nbrewitt1@dailymail.co.uk         True   \n","2   3     Orelie        Penney    openney2@vistaprint.com         True   \n","3   4      Ashby      Maddocks         amaddocks3@home.pl        False   \n","4   5       Kurt          Rome      krome4@shutterfly.com        False   \n","\n","   amount_paid customer_from     last_updated_ts  \n","0      1000.55    2021-01-15 2021-02-10 01:15:00  \n","1       900.00    2021-02-14 2021-02-18 03:33:00  \n","2       850.55    2021-01-21 2021-03-15 15:16:55  \n","3          NaN           NaN 2021-04-10 17:45:30  \n","4          NaN           NaN 2021-04-02 00:55:18  \n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n","\n","None\n","root\n"," |-- id: long (nullable = true)\n"," |-- first_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- is_customer: boolean (nullable = true)\n"," |-- amount_paid: double (nullable = true)\n"," |-- customer_from: date (nullable = true)\n"," |-- last_updated_ts: timestamp (nullable = true)\n","\n","None\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"   id first_name     last_name                      email  is_customer  \\\n0   1     Corrie  Van den Oord      cvandenoord0@etsy.com         True   \n1   2   Nikolaus       Brewitt  nbrewitt1@dailymail.co.uk         True   \n2   3     Orelie        Penney    openney2@vistaprint.com         True   \n3   4      Ashby      Maddocks         amaddocks3@home.pl        False   \n4   5       Kurt          Rome      krome4@shutterfly.com        False   \n\n   amount_paid customer_from     last_updated_ts  \n0      1000.55    2021-01-15 2021-02-10 01:15:00  \n1       900.00    2021-02-14 2021-02-18 03:33:00  \n2       850.55    2021-01-21 2021-03-15 15:16:55  \n3          NaN           NaN 2021-04-10 17:45:30  \n4          NaN           NaN 2021-04-02 00:55:18  \n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\nNone\nroot\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\nNone\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["## note advantages of creating dataframe from pandas this values are printed as null \n","import pandas as pd \n","df17 =pd.DataFrame(users)\n","print(df17)\n","users_df_pandas=spark.createDataFrame(df17)\n","print(users_df_pandas.show())\n","print(users_df_pandas.printSchema())"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9c5f6c5f-3c2d-452a-baeb-a95bc802758b","showTitle":false,"title":""}},"source":["* Here are the special types that are supported by Spark.\n","  * ARRAY\n","  * STRUCT\n","  * MAP\n","* Python structures such as list and dict can be implicitly converted to Spark ARRAY and MAP respectively.\n","* We need to use few Spark related APIs to convert Python data structures to STRUCT type."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"92736ad2-fa16-4947-afed-56c839dfdfd2","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["root\n"," |-- id: long (nullable = true)\n"," |-- first_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- phone_numbers: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- is_customer: boolean (nullable = true)\n"," |-- amount_paid: double (nullable = true)\n"," |-- customer_from: date (nullable = true)\n"," |-- last_updated_ts: timestamp (nullable = true)\n","\n","None\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|[+1 234 567 8901,...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|[+1 234 567 8923,...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n","|  3|    Orelie|      Penney|openney2@vistapri...|[+1 714 512 9752,...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|                null|      false|       null|         null|2021-04-10 17:45:30|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|   [+1 817 934 7142]|      false|       null|         null|2021-04-02 00:55:18|\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","\n","None\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"root\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\nNone\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|[+1 234 567 8901,...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|[+1 234 567 8923,...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|[+1 714 512 9752,...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|                null|      false|       null|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|   [+1 817 934 7142]|      false|       null|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n\nNone\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["import datetime\n","users = [\n","    {\n","        \"id\": 1,\n","        \"first_name\": \"Corrie\",\n","        \"last_name\": \"Van den Oord\",\n","        \"email\": \"cvandenoord0@etsy.com\",\n","        \"phone_numbers\": [\"+1 234 567 8901\", \"+1 234 567 8911\"],\n","        \"is_customer\": True,\n","        \"amount_paid\": 1000.55,\n","        \"customer_from\": datetime.date(2021, 1, 15),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n","    },\n","    {\n","        \"id\": 2,\n","        \"first_name\": \"Nikolaus\",\n","        \"last_name\": \"Brewitt\",\n","        \"email\": \"nbrewitt1@dailymail.co.uk\",\n","        \"phone_numbers\": [\"+1 234 567 8923\", \"+1 234 567 8934\"],\n","        \"is_customer\": True,\n","        \"amount_paid\": 900.0,\n","        \"customer_from\": datetime.date(2021, 2, 14),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n","    },\n","    {\n","        \"id\": 3,\n","        \"first_name\": \"Orelie\",\n","        \"last_name\": \"Penney\",\n","        \"email\": \"openney2@vistaprint.com\",\n","        \"phone_numbers\": [\"+1 714 512 9752\", \"+1 714 512 6601\"],\n","        \"is_customer\": True,\n","        \"amount_paid\": 850.55,\n","        \"customer_from\": datetime.date(2021, 1, 21),\n","        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n","    },\n","    {\n","        \"id\": 4,\n","        \"first_name\": \"Ashby\",\n","        \"last_name\": \"Maddocks\",\n","        \"email\": \"amaddocks3@home.pl\",\n","        \"phone_numbers\": None,\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n","    },\n","    {\n","        \"id\": 5,\n","        \"first_name\": \"Kurt\",\n","        \"last_name\": \"Rome\",\n","        \"email\": \"krome4@shutterfly.com\",\n","        \"phone_numbers\": [\"+1 817 934 7142\"],\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n","    }\n","]\n","\n","from pyspark.sql import Row\n","users_df = spark.createDataFrame([Row(**user) for user in users])\n","print(users_df.printSchema())\n","## here phone no is list  so it got converted to array data type \n","## python datetime.datetime(yyyy,mm,dd,min,sec,milsec) convert to timestamp \n","print(users_df.show())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fab7630d-4c7b-433c-9d67-22e3f1acdf43","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n","| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|   phone_number|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8901|\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8911|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8923|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8934|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 9752|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 6601|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|+1 817 934 7142|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|   phone_number|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8901|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8911|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8923|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8934|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 9752|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 6601|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|+1 817 934 7142|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql.functions import explode\n","users_df. \\\n","    withColumn('phone_number', explode('phone_numbers')). \\\n","    drop('phone_numbers'). \\\n","    show()\n","\n","## note: explode convert array to cells "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"24966c24-866d-4a20-8b04-ac96bd9a5eac","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+---+---------------+---------------+\n","| id|         mobile|           home|\n","+---+---------------+---------------+\n","|  1|+1 234 567 8901|+1 234 567 8911|\n","|  2|+1 234 567 8923|+1 234 567 8934|\n","|  3|+1 714 512 9752|+1 714 512 6601|\n","|  4|           null|           null|\n","|  5|+1 817 934 7142|           null|\n","+---+---------------+---------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+---+---------------+---------------+\n| id|         mobile|           home|\n+---+---------------+---------------+\n|  1|+1 234 567 8901|+1 234 567 8911|\n|  2|+1 234 567 8923|+1 234 567 8934|\n|  3|+1 714 512 9752|+1 714 512 6601|\n|  4|           null|           null|\n|  5|+1 817 934 7142|           null|\n+---+---------------+---------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql.functions import col\n","users_df. \\\n","    select('id', col('phone_numbers')[0].alias('mobile'), col('phone_numbers')[1].alias('home')). \\\n","    show()\n","\n","## here we are seperating phone no according to array index\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1a094360-1fa9-4414-8a46-b6ff88efb793","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n","| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|   phone_number|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8901|\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8911|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8923|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8934|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 9752|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 6601|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|           null|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|+1 817 934 7142|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|   phone_number|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8901|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8911|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8923|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8934|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 9752|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 6601|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|           null|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|+1 817 934 7142|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql.functions import explode_outer\n","users_df. \\\n","    withColumn('phone_number', explode_outer('phone_numbers')). \\\n","    drop('phone_numbers'). \\\n","    show()\n","\n","# # note explode outer will explode array is same as explode except it will also give/ return the value where originally phone_number was null ie nothing to explode , ie no droping of data "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7be82d22-e98e-466b-9a12-c051edb36b61","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["root\n"," |-- id: long (nullable = true)\n"," |-- first_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- phone_numbers: map (nullable = true)\n"," |    |-- key: string\n"," |    |-- value: string (valueContainsNull = true)\n"," |-- is_customer: boolean (nullable = true)\n"," |-- amount_paid: double (nullable = true)\n"," |-- customer_from: date (nullable = true)\n"," |-- last_updated_ts: timestamp (nullable = true)\n","\n","None\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{mobile -> +1 234...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{mobile -> +1 234...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n","|  3|    Orelie|      Penney|openney2@vistapri...|{mobile -> +1 714...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|                null|      false|       null|         null|2021-04-10 17:45:30|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|{mobile -> +1 817...|      false|       null|         null|2021-04-02 00:55:18|\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","\n","None\n","+---+----------------------------------------------------+\n","|id |phone_numbers                                       |\n","+---+----------------------------------------------------+\n","|1  |{mobile -> +1 234 567 8901, home -> +1 234 567 8911}|\n","|2  |{mobile -> +1 234 567 8923, home -> +1 234 567 8934}|\n","|3  |{mobile -> +1 714 512 9752, home -> +1 714 512 6601}|\n","|4  |null                                                |\n","|5  |{mobile -> +1 817 934 7142}                         |\n","+---+----------------------------------------------------+\n","\n","+---+------+---------------+\n","|id |key   |value          |\n","+---+------+---------------+\n","|1  |mobile|+1 234 567 8901|\n","|1  |home  |+1 234 567 8911|\n","|2  |mobile|+1 234 567 8923|\n","|2  |home  |+1 234 567 8934|\n","|3  |mobile|+1 714 512 9752|\n","|3  |home  |+1 714 512 6601|\n","|5  |mobile|+1 817 934 7142|\n","+---+------+---------------+\n","\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n","| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|phone_type|   phone_number|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|    mobile|+1 234 567 8901|\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|      home|+1 234 567 8911|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|    mobile|+1 234 567 8923|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|      home|+1 234 567 8934|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|    mobile|+1 714 512 9752|\n","|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|      home|+1 714 512 6601|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|    mobile|+1 817 934 7142|\n","+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"root\n |-- id: long (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_numbers: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- is_customer: boolean (nullable = true)\n |-- amount_paid: double (nullable = true)\n |-- customer_from: date (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n\nNone\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{mobile -> +1 234...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{mobile -> +1 234...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{mobile -> +1 714...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|                null|      false|       null|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{mobile -> +1 817...|      false|       null|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n\nNone\n+---+----------------------------------------------------+\n|id |phone_numbers                                       |\n+---+----------------------------------------------------+\n|1  |{mobile -> +1 234 567 8901, home -> +1 234 567 8911}|\n|2  |{mobile -> +1 234 567 8923, home -> +1 234 567 8934}|\n|3  |{mobile -> +1 714 512 9752, home -> +1 714 512 6601}|\n|4  |null                                                |\n|5  |{mobile -> +1 817 934 7142}                         |\n+---+----------------------------------------------------+\n\n+---+------+---------------+\n|id |key   |value          |\n+---+------+---------------+\n|1  |mobile|+1 234 567 8901|\n|1  |home  |+1 234 567 8911|\n|2  |mobile|+1 234 567 8923|\n|2  |home  |+1 234 567 8934|\n|3  |mobile|+1 714 512 9752|\n|3  |home  |+1 714 512 6601|\n|5  |mobile|+1 817 934 7142|\n+---+------+---------------+\n\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|phone_type|   phone_number|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|    mobile|+1 234 567 8901|\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|      home|+1 234 567 8911|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|    mobile|+1 234 567 8923|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|      home|+1 234 567 8934|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|    mobile|+1 714 512 9752|\n|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|      home|+1 714 512 6601|\n|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|    mobile|+1 817 934 7142|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["###  map type data \n","import datetime\n","users = [\n","    {\n","        \"id\": 1,\n","        \"first_name\": \"Corrie\",\n","        \"last_name\": \"Van den Oord\",\n","        \"email\": \"cvandenoord0@etsy.com\",\n","        \"phone_numbers\": {\"mobile\": \"+1 234 567 8901\", \"home\": \"+1 234 567 8911\"},\n","        \"is_customer\": True,\n","        \"amount_paid\": 1000.55,\n","        \"customer_from\": datetime.date(2021, 1, 15),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n","    },\n","    {\n","        \"id\": 2,\n","        \"first_name\": \"Nikolaus\",\n","        \"last_name\": \"Brewitt\",\n","        \"email\": \"nbrewitt1@dailymail.co.uk\",\n","        \"phone_numbers\": {\"mobile\": \"+1 234 567 8923\", \"home\": \"+1 234 567 8934\"},\n","        \"is_customer\": True,\n","        \"amount_paid\": 900.0,\n","        \"customer_from\": datetime.date(2021, 2, 14),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n","    },\n","    {\n","        \"id\": 3,\n","        \"first_name\": \"Orelie\",\n","        \"last_name\": \"Penney\",\n","        \"email\": \"openney2@vistaprint.com\",\n","        \"phone_numbers\": {\"mobile\": \"+1 714 512 9752\", \"home\": \"+1 714 512 6601\"},\n","        \"is_customer\": True,\n","        \"amount_paid\": 850.55,\n","        \"customer_from\": datetime.date(2021, 1, 21),\n","        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n","    },\n","    {\n","        \"id\": 4,\n","        \"first_name\": \"Ashby\",\n","        \"last_name\": \"Maddocks\",\n","        \"email\": \"amaddocks3@home.pl\",\n","        \"phone_numbers\": None,\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n","    },\n","    {\n","        \"id\": 5,\n","        \"first_name\": \"Kurt\",\n","        \"last_name\": \"Rome\",\n","        \"email\": \"krome4@shutterfly.com\",\n","        \"phone_numbers\": {\"mobile\": \"+1 817 934 7142\"},\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n","    }\n","]\n","\n","from pyspark.sql import Row\n","users_df = spark.createDataFrame([Row(**user) for user in users])\n","print(users_df.printSchema())\n","\n","## note we are usingh row so we can get proper shema for out data frame \n","print(users_df.show())\n","## phone no is of type map \n","users_df.select('id', 'phone_numbers').show(truncate=False)\n","users_df.select('id', explode('phone_numbers')).show(truncate=False)\n","\n","## note : when map get exxplode it create key , value , map is scala/java collection \n","## adding seperate columns \n","users_df.select('*', explode('phone_numbers')). \\\n","    withColumnRenamed('key', 'phone_type'). \\\n","    withColumnRenamed('value', 'phone_number'). \\\n","    drop('phone_numbers'). \\\n","    show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"951dc21a-1a38-442a-9606-856e6da67e7c","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n","|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n","|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n","|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|      false|       null|         null|2021-04-10 17:45:30|\n","|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|      false|       null|         null|2021-04-02 00:55:18|\n","+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|      false|       null|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|      false|       null|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["## creating dataframe from struct data Type \n","from pyspark.sql import Row\n","import datetime\n","users = [\n","    {\n","        \"id\": 1,\n","        \"first_name\": \"Corrie\",\n","        \"last_name\": \"Van den Oord\",\n","        \"email\": \"cvandenoord0@etsy.com\",\n","        \"phone_numbers\": Row(mobile=\"+1 234 567 8901\", home=\"+1 234 567 8911\"),\n","        \"is_customer\": True,\n","        \"amount_paid\": 1000.55,\n","        \"customer_from\": datetime.date(2021, 1, 15),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n","    },\n","    {\n","        \"id\": 2,\n","        \"first_name\": \"Nikolaus\",\n","        \"last_name\": \"Brewitt\",\n","        \"email\": \"nbrewitt1@dailymail.co.uk\",\n","        \"phone_numbers\":  Row(mobile=\"+1 234 567 8923\", home=\"1 234 567 8934\"),\n","        \"is_customer\": True,\n","        \"amount_paid\": 900.0,\n","        \"customer_from\": datetime.date(2021, 2, 14),\n","        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n","    },\n","    {\n","        \"id\": 3,\n","        \"first_name\": \"Orelie\",\n","        \"last_name\": \"Penney\",\n","        \"email\": \"openney2@vistaprint.com\",\n","        \"phone_numbers\": Row(mobile=\"+1 714 512 9752\", home=\"+1 714 512 6601\"),\n","        \"is_customer\": True,\n","        \"amount_paid\": 850.55,\n","        \"customer_from\": datetime.date(2021, 1, 21),\n","        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n","    },\n","    {\n","        \"id\": 4,\n","        \"first_name\": \"Ashby\",\n","        \"last_name\": \"Maddocks\",\n","        \"email\": \"amaddocks3@home.pl\",\n","        \"phone_numbers\": Row(mobile=None, home=None),\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n","    },\n","    {\n","        \"id\": 5,\n","        \"first_name\": \"Kurt\",\n","        \"last_name\": \"Rome\",\n","        \"email\": \"krome4@shutterfly.com\",\n","        \"phone_numbers\": Row(mobile=\"+1 817 934 7142\", home=None),\n","        \"is_customer\": False,\n","        \"amount_paid\": None,\n","        \"customer_from\": None,\n","        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n","    }\n","]\n","\n","users_df = spark.createDataFrame([Row(**user) for user in users])\n","users_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4b6bb18d-a605-418c-a312-2316bc5be0cb","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+---+----------------------------------+\n","|id |phone_numbers                     |\n","+---+----------------------------------+\n","|1  |{+1 234 567 8901, +1 234 567 8911}|\n","|2  |{+1 234 567 8923, 1 234 567 8934} |\n","|3  |{+1 714 512 9752, +1 714 512 6601}|\n","|4  |{null, null}                      |\n","|5  |{+1 817 934 7142, null}           |\n","+---+----------------------------------+\n","\n","None\n","+---+---------------+---------------+\n","| id|         mobile|           home|\n","+---+---------------+---------------+\n","|  1|+1 234 567 8901|+1 234 567 8911|\n","|  2|+1 234 567 8923| 1 234 567 8934|\n","|  3|+1 714 512 9752|+1 714 512 6601|\n","|  4|           null|           null|\n","|  5|+1 817 934 7142|           null|\n","+---+---------------+---------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+---+----------------------------------+\n|id |phone_numbers                     |\n+---+----------------------------------+\n|1  |{+1 234 567 8901, +1 234 567 8911}|\n|2  |{+1 234 567 8923, 1 234 567 8934} |\n|3  |{+1 714 512 9752, +1 714 512 6601}|\n|4  |{null, null}                      |\n|5  |{+1 817 934 7142, null}           |\n+---+----------------------------------+\n\nNone\n+---+---------------+---------------+\n| id|         mobile|           home|\n+---+---------------+---------------+\n|  1|+1 234 567 8901|+1 234 567 8911|\n|  2|+1 234 567 8923| 1 234 567 8934|\n|  3|+1 714 512 9752|+1 714 512 6601|\n|  4|           null|           null|\n|  5|+1 817 934 7142|           null|\n+---+---------------+---------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["print(users_df.select('id', 'phone_numbers').show(truncate=False))\n","\n","users_df. \\\n","    select('id', 'phone_numbers.mobile', 'phone_numbers.home'). \\\n","    show()\n","\n","## note: struct is basically  row type ie () with key,value mensions eg  Row(mobile=\"+1 714 512 9752\", home=\"+1 714 512 6601\") and not list of dictionary \n","## it will have column under column in lay man term \n","## difference between map and struct is that column need to have predefine structure \n","## struct can be access by giving columnName.key "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f0e3fb1d-c2c5-484f-91cd-477fdfd099aa","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+---+---------------+---------------+\n","| id|         mobile|           home|\n","+---+---------------+---------------+\n","|  1|+1 234 567 8901|+1 234 567 8911|\n","|  2|+1 234 567 8923| 1 234 567 8934|\n","|  3|+1 714 512 9752|+1 714 512 6601|\n","|  4|           null|           null|\n","|  5|+1 817 934 7142|           null|\n","+---+---------------+---------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+---+---------------+---------------+\n| id|         mobile|           home|\n+---+---------------+---------------+\n|  1|+1 234 567 8901|+1 234 567 8911|\n|  2|+1 234 567 8923| 1 234 567 8934|\n|  3|+1 714 512 9752|+1 714 512 6601|\n|  4|           null|           null|\n|  5|+1 817 934 7142|           null|\n+---+---------------+---------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["users_df. \\\n","    select('id', 'phone_numbers.*'). \\\n","    show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a22d2d84-64b5-4f96-991b-35e7082b5540","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["+---+--------------------+------------------+\n","| id|phone_numbers.mobile|phone_numbers.home|\n","+---+--------------------+------------------+\n","|  1|     +1 234 567 8901|   +1 234 567 8911|\n","|  2|     +1 234 567 8923|    1 234 567 8934|\n","|  3|     +1 714 512 9752|   +1 714 512 6601|\n","|  4|                null|              null|\n","|  5|     +1 817 934 7142|              null|\n","+---+--------------------+------------------+\n","\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"+---+--------------------+------------------+\n| id|phone_numbers.mobile|phone_numbers.home|\n+---+--------------------+------------------+\n|  1|     +1 234 567 8901|   +1 234 567 8911|\n|  2|     +1 234 567 8923|    1 234 567 8934|\n|  3|     +1 714 512 9752|   +1 714 512 6601|\n|  4|                null|              null|\n|  5|     +1 817 934 7142|              null|\n+---+--------------------+------------------+\n\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from pyspark.sql.functions import col\n","users_df. \\\n","    select('id', col('phone_numbers')['mobile'], col('phone_numbers')['home']). \\\n","    show()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"data_bricks_associate_developer_exam_basic1","notebookOrigID":3047559665068201,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
